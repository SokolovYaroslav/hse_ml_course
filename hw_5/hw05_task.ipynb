{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "hw05_task.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "5a7bd829ccdd44449266badd69bd0262": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_ad76684efa9e4d9a80d46ec35c62a757",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_3023475c672047f1937e78184214f2ca",
       "IPY_MODEL_ac41fc0e882e424fb22466eed075fc60"
      ]
     }
    },
    "ad76684efa9e4d9a80d46ec35c62a757": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "3023475c672047f1937e78184214f2ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_964b11cd843f4d7dbb8d00d1005fd5cc",
      "_dom_classes": [],
      "description": "",
      "_model_name": "FloatProgressModel",
      "bar_style": "info",
      "max": 1,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 1,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_167cc9e8396f480f90cf141a22019ff0"
     }
    },
    "ac41fc0e882e424fb22466eed075fc60": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_15db84e2b3844384b872e907ae4a6a57",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 170500096/? [00:20&lt;00:00, 96880037.70it/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_2668cebcf3e34cbd9454be153897e192"
     }
    },
    "964b11cd843f4d7dbb8d00d1005fd5cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "167cc9e8396f480f90cf141a22019ff0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "15db84e2b3844384b872e907ae4a6a57": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "2668cebcf3e34cbd9454be153897e192": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Нейронные сети\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "__Решение отправлять на `ml.course.practice@gmail.com`__\n",
    "\n",
    "__Тема письма: `[ML][HW05] <ФИ>`, где вместо `<ФИ>` указаны фамилия и имя__\n",
    "\n",
    "Для начала вам предстоит реализовать свой собственный backpropagation и протестировать его на реальных данных, а затем научиться обучать нейронные сети при помощи библиотеки `PyTorch` и использовать это умение для классификации классического набора данных CIFAR10."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from typing import List, NoReturn, Iterable, Tuple\n",
    "import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Задание 1 (3 балла)\n",
    "Нейронные сети состоят из слоев, поэтому для начала понадобится реализовать их. Пока нам понадобятся только три:\n",
    "\n",
    "`Linear` - полносвязный слой, в котором `y = Wx + b`, где `y` - выход, `x` - вход, `W` - матрица весов, а `b` - смещение. \n",
    "\n",
    "`ReLU` - слой, соответствующий функции активации `y = max(0, x)`.\n",
    "\n",
    "`Softmax` - слой, соответствующий функции активации [softmax](https://ru.wikipedia.org/wiki/Softmax)\n",
    "\n",
    "\n",
    "#### Методы\n",
    "`forward(X)` - возвращает предсказанные для `X`. `X` может быть как вектором, так и батчем\n",
    "\n",
    "`backward(d)` - считает градиент при помощи обратного распространения ошибки. Возвращает новое значение `d`\n",
    "\n",
    "`update(alpha)` - обновляет веса (если необходимо) с заданой скоростью обучения"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RWFLlHqaYbgC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1610650422610,
     "user_tz": -180,
     "elapsed": 580,
     "user": {
      "displayName": "Iaroslav Sokolov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHt_w9-U1o8Sy_kutVo5pzmBrbbw0dWtxW9hnv=s64",
      "userId": "04885912710307977692"
     }
    }
   },
   "source": [
    "class Module:\n",
    "    \"\"\"\n",
    "    Абстрактный класс. Его менять не нужно.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def backward(self, d):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def update(self, alpha):\n",
    "        pass"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aYS2gE4PYepZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1610650422955,
     "user_tz": -180,
     "elapsed": 413,
     "user": {
      "displayName": "Iaroslav Sokolov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHt_w9-U1o8Sy_kutVo5pzmBrbbw0dWtxW9hnv=s64",
      "userId": "04885912710307977692"
     }
    }
   },
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    Линейный полносвязный слой.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features : int\n",
    "            Размер входа.\n",
    "        out_features : int \n",
    "            Размер выхода.\n",
    "    \n",
    "        Notes\n",
    "        -----\n",
    "        W и b инициализируются случайно.\n",
    "        \"\"\"\n",
    "        self._w = np.random.randn(in_features, out_features)\n",
    "        self._b = np.random.randn(out_features)\n",
    "\n",
    "        self._in = None\n",
    "        self._grad_w = None\n",
    "        self._grad_b = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Возвращает y = Wx + b.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            Входной вектор или батч.\n",
    "            То есть, либо x вектор с in_features элементов,\n",
    "            либо матрица размерности (batch_size, in_features).\n",
    "    \n",
    "        Return\n",
    "        ------\n",
    "        y : np.ndarray\n",
    "            Выход после слоя.\n",
    "            Либо вектор с out_features элементами,\n",
    "            либо матрица размерности (batch_size, out_features)\n",
    "\n",
    "        \"\"\"\n",
    "        self._in = x\n",
    "        return x @ self._w + self._b\n",
    "    \n",
    "    def backward(self, d: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Cчитает градиент при помощи обратного распространения ошибки.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        d : np.ndarray\n",
    "            Градиент.\n",
    "        Return\n",
    "        ------\n",
    "        np.ndarray\n",
    "            Новое значение градиента.\n",
    "        \"\"\"\n",
    "        self._grad_w = self._in.T @ d\n",
    "        self._grad_b = np.sum(d, axis=0)\n",
    "        return d @ self._w.T\n",
    "        \n",
    "    def update(self, alpha: float) -> NoReturn:\n",
    "        \"\"\"\n",
    "        Обновляет W и b с заданной скоростью обучения.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha : float\n",
    "            Скорость обучения.\n",
    "        \"\"\"\n",
    "        self._w -= alpha * self._grad_w\n",
    "        self._b -= alpha * self._grad_b"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "94hkbnD1QuvG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1610650423601,
     "user_tz": -180,
     "elapsed": 575,
     "user": {
      "displayName": "Iaroslav Sokolov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHt_w9-U1o8Sy_kutVo5pzmBrbbw0dWtxW9hnv=s64",
      "userId": "04885912710307977692"
     }
    }
   },
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"\n",
    "    Слой, соответствующий функции активации ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._in = None\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Возвращает y = max(0, x).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            Входной вектор или батч.\n",
    "    \n",
    "        Return\n",
    "        ------\n",
    "        y : np.ndarray\n",
    "            Выход после слоя (той же размерности, что и вход).\n",
    "\n",
    "        \"\"\"\n",
    "        self._in = x\n",
    "        return np.maximum(0, x)\n",
    "        \n",
    "    def backward(self, d) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Cчитает градиент при помощи обратного распространения ошибки.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        d : np.ndarray\n",
    "            Градиент.\n",
    "        Return\n",
    "        ------\n",
    "        np.ndarray\n",
    "            Новое значение градиента.\n",
    "        \"\"\"\n",
    "        return d * (self._in >= 0)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Nm8VMc3gPUKb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1610650424023,
     "user_tz": -180,
     "elapsed": 486,
     "user": {
      "displayName": "Iaroslav Sokolov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHt_w9-U1o8Sy_kutVo5pzmBrbbw0dWtxW9hnv=s64",
      "userId": "04885912710307977692"
     }
    }
   },
   "source": [
    "def softmax(x: np.ndarray):\n",
    "    x -= np.max(x, axis=-1, keepdims=True)\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "\n",
    "        \n",
    "class SoftmaxCrossEntropy:\n",
    "    \"\"\"\n",
    "    Слой, соответствующий функции активации Softmax с последующим CrossEntropy\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._labels = None\n",
    "        self._probs = None\n",
    "\n",
    "    def forward(self, x: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Возвращает L = CrossEntropy(Softmax(x)).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            Входной вектор или батч.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        y : np.ndarray\n",
    "            Выход после слоя (одно число).\n",
    "\n",
    "        \"\"\"\n",
    "        self._labels = labels\n",
    "\n",
    "        self._probs = softmax(x)\n",
    "\n",
    "        selected_probs = self._probs[range(self._probs.shape[0]), labels]\n",
    "        selected_log_probs = np.log(selected_probs + np.finfo(x.dtype).eps)\n",
    "        log_loss = -np.mean(selected_log_probs)\n",
    "\n",
    "        return log_loss\n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Cчитает градиент при помощи обратного распространения ошибки.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        d : np.ndarray\n",
    "            Нисходящий градиент\n",
    "        Return\n",
    "        ------\n",
    "        np.ndarray\n",
    "            Новое значение градиента.\n",
    "        \"\"\"\n",
    "        grad = self._probs\n",
    "        grad[range(grad.shape[0]), self._labels] -= 1\n",
    "        return grad / self._probs.shape[0]"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rb_ip_h8QuvJ"
   },
   "source": [
    "### Задание 2 (2 балла)\n",
    "Теперь сделаем саму нейронную сеть.\n",
    "\n",
    "#### Методы\n",
    "`fit(X, y)` - обучает нейронную сеть заданное число эпох. В каждой эпохе необходимо использовать [cross-entropy loss](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy) для обучения, а так же производить обновления не по одному элементу, а используя батчи.\n",
    "\n",
    "`predict_proba(X)` - предсказывает вероятности классов для элементов `X`\n",
    "\n",
    "#### Параметры конструктора\n",
    "`modules` - список, состоящий из ранее реализованных модулей и описывающий слои нейронной сети. В конец необходимо добавить `Softmax`\n",
    "\n",
    "`epochs` - количество эпох обучения\n",
    "\n",
    "`alpha` - скорость обучения"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q_JFCizKQuvK",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1610650425262,
     "user_tz": -180,
     "elapsed": 681,
     "user": {
      "displayName": "Iaroslav Sokolov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHt_w9-U1o8Sy_kutVo5pzmBrbbw0dWtxW9hnv=s64",
      "userId": "04885912710307977692"
     }
    }
   },
   "source": [
    "class MLPClassifier:\n",
    "    def __init__(self, modules: List[Module], epochs: int = 40, alpha: float = 0.01):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        modules : List[Module]\n",
    "            Cписок, состоящий из ранее реализованных модулей и \n",
    "            описывающий слои нейронной сети. \n",
    "            В конец необходимо добавить Softmax.\n",
    "        epochs : int\n",
    "            Количество эпох обученияю\n",
    "        alpha : float\n",
    "            Cкорость обучения.\n",
    "        \"\"\"\n",
    "        self._modules = copy.copy(modules)\n",
    "        self._epochs = epochs\n",
    "        self._alpha = alpha\n",
    "\n",
    "        self._softmax_loss = SoftmaxCrossEntropy()\n",
    "            \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, batch_size=32) -> NoReturn:\n",
    "        \"\"\"\n",
    "        Обучает нейронную сеть заданное число эпох. \n",
    "        В каждой эпохе необходимо использовать cross-entropy loss для обучения, \n",
    "        а так же производить обновления не по одному элементу, а используя батчи.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Данные для обучения.\n",
    "        y : np.ndarray\n",
    "            Вектор меток классов для данных.\n",
    "        batch_size : int\n",
    "            Размер батча.\n",
    "        \"\"\"\n",
    "        for epoch_num in tqdm.trange(self._epochs):\n",
    "            losses = []\n",
    "            for x_batch, y_batch in MLPClassifier._batchify(X, y, batch_size):\n",
    "                # forward pass\n",
    "                out = x_batch\n",
    "                for module in self._modules:\n",
    "                    out = module.forward(out)\n",
    "                # loss calc\n",
    "                loss = self._softmax_loss.forward(out, y_batch)\n",
    "                losses.append(loss)\n",
    "                # backward pass and params update\n",
    "                grad = self._softmax_loss.backward()\n",
    "                for module in reversed(self._modules):\n",
    "                    grad = module.backward(grad)\n",
    "                    module.update(self._alpha)\n",
    "            print(f\"Last loss: {loss}, Avg loss: {sum(losses) / len(losses)}\")\n",
    "\n",
    "        \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Предсказывает вероятности классов для элементов X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Данные для предсказания.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        np.ndarray\n",
    "            Предсказанные вероятности классов для всех элементов X.\n",
    "            Размерность (X.shape[0], n_classes)\n",
    "        \n",
    "        \"\"\"\n",
    "        out = X\n",
    "        for module in self._modules:\n",
    "            out = module.forward(out)\n",
    "        return softmax(out)\n",
    "        \n",
    "    def predict(self, X) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Предсказывает метки классов для элементов X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Данные для предсказания.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        np.ndarray\n",
    "            Вектор предсказанных классов\n",
    "        \n",
    "        \"\"\"\n",
    "        p = self.predict_proba(X)\n",
    "        return np.argmax(p, axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _batchify(x: np.ndarray, y: np.ndarray, batch_size: int) -> Iterable[Tuple[np.ndarray, np.ndarray]]:\n",
    "        mask = np.random.permutation(x.shape[0])\n",
    "        x_shuffled, y_shuffled = x[mask], y[mask]\n",
    "        xs = np.split(x_shuffled, range(batch_size, x.shape[0], batch_size))\n",
    "        ys = np.split(y_shuffled, range(batch_size, y.shape[0], batch_size))\n",
    "        return ((x_batch, y_batch) for x_batch, y_batch in zip(xs, ys))"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "onDymYQXQuvN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1610650426136,
     "user_tz": -180,
     "elapsed": 598,
     "user": {
      "displayName": "Iaroslav Sokolov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHt_w9-U1o8Sy_kutVo5pzmBrbbw0dWtxW9hnv=s64",
      "userId": "04885912710307977692"
     }
    },
    "outputId": "09a119c5-be14-4ab8-8395-d9c135c490ae"
   },
   "source": [
    "p = MLPClassifier([\n",
    "    Linear(4, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 2)\n",
    "])\n",
    "\n",
    "X = np.random.randn(50, 4)\n",
    "y = np.array([(0 if x[0] > x[2]**2 or x[3]**3 > 0.5 else 1) for x in X])\n",
    "p.fit(X, y)"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 924.56it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 8.806795493024202, Avg loss: 14.047906304429699\n",
      "Last loss: 4.936174245524963, Avg loss: 5.950837271261423\n",
      "Last loss: 3.470360225816174, Avg loss: 3.372486682391254\n",
      "Last loss: 2.2536578682328536, Avg loss: 2.881895991707745\n",
      "Last loss: 1.913650276049026, Avg loss: 1.9585653007600783\n",
      "Last loss: 1.0860868398015595, Avg loss: 1.693923242689793\n",
      "Last loss: 0.0005451575455588356, Avg loss: 1.593419768230646\n",
      "Last loss: 2.8436921414769167, Avg loss: 1.9938482574563952\n",
      "Last loss: 2.388721699769063, Avg loss: 1.6548170182360913\n",
      "Last loss: 2.0185822050847033, Avg loss: 1.3459696171533428\n",
      "Last loss: 0.6184383710492584, Avg loss: 0.798524623221702\n",
      "Last loss: 7.705675541716768, Avg loss: 5.491117611607668\n",
      "Last loss: 0.48775504671559783, Avg loss: 1.9531764523119064\n",
      "Last loss: 1.7552380514980277, Avg loss: 1.0014425923223758\n",
      "Last loss: 0.27426785798093667, Avg loss: 0.6861283120947554\n",
      "Last loss: 1.3006898770902184, Avg loss: 1.1114094900268698\n",
      "Last loss: 0.0014390303529037212, Avg loss: 0.546640804570331\n",
      "Last loss: 1.0224021225862385, Avg loss: 0.578657827230915\n",
      "Last loss: 0.28669588660427875, Avg loss: 0.31577211373669245\n",
      "Last loss: 0.07313522444323878, Avg loss: 0.18964543426577757\n",
      "Last loss: 0.039806820852471156, Avg loss: 0.1870787291136343\n",
      "Last loss: 0.05977699846979297, Avg loss: 0.15906845773688424\n",
      "Last loss: 0.05676352387945923, Avg loss: 0.11864799154137362\n",
      "Last loss: 0.16696382510661156, Avg loss: 0.0875165221557141\n",
      "Last loss: 0.46824181500920997, Avg loss: 0.2983924973909471\n",
      "Last loss: 0.04384071725908712, Avg loss: 0.08865513538241727\n",
      "Last loss: 0.03329584004163995, Avg loss: 0.028151911818792286\n",
      "Last loss: 0.01053809760880888, Avg loss: 0.023253765953465722\n",
      "Last loss: 0.006822339750198968, Avg loss: 0.01084335032429261\n",
      "Last loss: 0.0006236370516998919, Avg loss: 0.0065097147373919285\n",
      "Last loss: 0.014339182374741096, Avg loss: 0.009902000869389208\n",
      "Last loss: 0.01129908335059368, Avg loss: 0.008105559979742376\n",
      "Last loss: 0.006209576957030609, Avg loss: 0.0057113390241240545\n",
      "Last loss: 0.010459572302975735, Avg loss: 0.0062543307559341154\n",
      "Last loss: 9.736316775028795e-05, Avg loss: 0.0035584371398874972\n",
      "Last loss: 0.006117602362323334, Avg loss: 0.00489647009323186\n",
      "Last loss: 0.004200762929317719, Avg loss: 0.00426241412331172\n",
      "Last loss: 0.006079651733040755, Avg loss: 0.004311006485503782\n",
      "Last loss: 0.005775682154439922, Avg loss: 0.004059150982684381\n",
      "Last loss: 0.0029173337704286708, Avg loss: 0.0033512930661239704\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C1EIsDqQuvQ"
   },
   "source": [
    "### Задание 3 (2 балла)\n",
    "Протестируем наше решение на синтетических данных. Необходимо подобрать гиперпараметры, при которых качество полученных классификаторов будет достаточным.\n",
    "\n",
    "#### Оценка\n",
    "Accuracy на первом датасете больше 0.85 - +1 балл\n",
    "\n",
    "Accuracy на втором датасете больше 0.85 - +1 балл"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d5UAgXTcQuvQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1610650437010,
     "user_tz": -180,
     "elapsed": 3721,
     "user": {
      "displayName": "Iaroslav Sokolov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHt_w9-U1o8Sy_kutVo5pzmBrbbw0dWtxW9hnv=s64",
      "userId": "04885912710307977692"
     }
    },
    "outputId": "e1300bbb-3fc8-458a-f5aa-8f1ded043156"
   },
   "source": [
    "X, y = make_moons(400, noise=0.075)\n",
    "X_test, y_test = make_moons(400, noise=0.075)\n",
    "\n",
    "best_acc = 0\n",
    "for _ in range(25):\n",
    "    p = MLPClassifier([\n",
    "        Linear(2, 16),\n",
    "        ReLU(),\n",
    "        Linear(16, 16),\n",
    "        ReLU(),\n",
    "        Linear(16, 2)\n",
    "    ])\n",
    "\n",
    "    p.fit(X, y)\n",
    "    best_acc = max(np.mean(p.predict(X_test) == y_test), best_acc)\n",
    "print(\"Accuracy\", best_acc)"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 345.32it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.37027542203639874, Avg loss: 3.7648303242522134\n",
      "Last loss: 0.49029238300906064, Avg loss: 0.49677224354879546\n",
      "Last loss: 0.3561586895083439, Avg loss: 0.3649178437191886\n",
      "Last loss: 0.3163552997991038, Avg loss: 0.34664847088680434\n",
      "Last loss: 0.3558051387800101, Avg loss: 0.335980726204351\n",
      "Last loss: 0.7301147472883238, Avg loss: 0.33148810240969834\n",
      "Last loss: 0.5813229149984508, Avg loss: 0.34300390159566807\n",
      "Last loss: 0.18295475951638673, Avg loss: 0.29994207004742174\n",
      "Last loss: 0.20931987890716494, Avg loss: 0.29318179052784943\n",
      "Last loss: 0.3395769531366661, Avg loss: 0.2924102924888096\n",
      "Last loss: 0.09009776920369222, Avg loss: 0.27953842622872993\n",
      "Last loss: 0.349837475474524, Avg loss: 0.2798470899386874\n",
      "Last loss: 0.2884120281704303, Avg loss: 0.2643031583839549\n",
      "Last loss: 0.377932425448989, Avg loss: 0.27787508771714947\n",
      "Last loss: 0.2966429221410205, Avg loss: 0.2654398637073856\n",
      "Last loss: 0.18659283961500817, Avg loss: 0.26222814518025606\n",
      "Last loss: 0.2836166573678436, Avg loss: 0.25949190546733747\n",
      "Last loss: 0.1997740201240495, Avg loss: 0.2442438231801803\n",
      "Last loss: 0.23830867831174174, Avg loss: 0.24361222540629576\n",
      "Last loss: 0.11726562498305912, Avg loss: 0.23488586531293532\n",
      "Last loss: 0.029869105476839346, Avg loss: 0.22783175615612855\n",
      "Last loss: 0.30516497561224454, Avg loss: 0.23996892201941478\n",
      "Last loss: 0.16624440799005627, Avg loss: 0.22083115998011565\n",
      "Last loss: 0.4177870196470339, Avg loss: 0.23976048614666345\n",
      "Last loss: 0.20056850112576136, Avg loss: 0.2229977210836853\n",
      "Last loss: 0.3239135718924083, Avg loss: 0.22104740685801205\n",
      "Last loss: 0.30266709520216106, Avg loss: 0.2212677189544852\n",
      "Last loss: 0.21460709158710933, Avg loss: 0.21554598727302324\n",
      "Last loss: 0.14733059308943, Avg loss: 0.20789855993817388\n",
      "Last loss: 0.3863568258393064, Avg loss: 0.2097136952521147\n",
      "Last loss: 0.04215008624487852, Avg loss: 0.20137411659251064\n",
      "Last loss: 0.05756923481717305, Avg loss: 0.19284738617692312\n",
      "Last loss: 0.41014589472823415, Avg loss: 0.20771816316688962\n",
      "Last loss: 0.1810428319069003, Avg loss: 0.19720875862851156\n",
      "Last loss: 0.1760675104820592, Avg loss: 0.19247096383698553\n",
      "Last loss: 0.37065350263513375, Avg loss: 0.20386697474908486\n",
      "Last loss: 0.10344359432635084, Avg loss: 0.19400071211763292\n",
      "Last loss: 0.13499812699867206, Avg loss: 0.17936344430369833\n",
      "Last loss: 0.09408513915047076, Avg loss: 0.19134585200020962\n",
      "Last loss: 0.14750295649327164, Avg loss: 0.18637996356608993\n",
      "Last loss: 5.050589508835136, Avg loss: 7.167873988317941\n",
      "Last loss: 1.6457849634906492, Avg loss: 2.998262268839597\n",
      "Last loss: 1.2579604402479148, Avg loss: 1.556388727471594\n",
      "Last loss: 0.44520388857046506, Avg loss: 0.810098755900312\n",
      "Last loss: 0.6183108225609977, Avg loss: 0.536099052774335\n",
      "Last loss: 0.5433802870267135, Avg loss: 0.4029764508088427\n",
      "Last loss: 0.19397277584952888, Avg loss: 0.3113249976292756\n",
      "Last loss: 0.3195714135521183, Avg loss: 0.27019526985898845\n",
      "Last loss: 0.171949026898791, Avg loss: 0.23047028172020936\n",
      "Last loss: 0.24085941470034972, Avg loss: 0.2081133241286965\n",
      "Last loss: 0.16114634460048502, Avg loss: 0.18576230160493165\n",
      "Last loss: 0.22122001990825624, Avg loss: 0.1738013226372293\n",
      "Last loss: 0.1889504941239733, Avg loss: 0.16003557402879595\n",
      "Last loss: 0.099876770910387, Avg loss: 0.1474065199814531\n",
      "Last loss: 0.15534939128847602, Avg loss: 0.14096332806197048\n",
      "Last loss: 0.17828972174224272, Avg loss: 0.13559249411280716\n",
      "Last loss: 0.18885324772268228, Avg loss: 0.13024761761878245\n",
      "Last loss: 0.12084041223118513, Avg loss: 0.12166053421395812\n",
      "Last loss: 0.04484121089990336, Avg loss: 0.11234291426804055\n",
      "Last loss: 0.08501756834654241, Avg loss: 0.10824173762874374\n",
      "Last loss: 0.08390412608008994, Avg loss: 0.10398137149458195\n",
      "Last loss: 0.06385501549595851, Avg loss: 0.09841211398583588\n",
      "Last loss: 0.08649514678954237, Avg loss: 0.09560092016173989\n",
      "Last loss: 0.06747557185550916, Avg loss: 0.09193021428471543\n",
      "Last loss: 0.03798831899853985, Avg loss: 0.0877872837040494\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 285.68it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 324.27it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.05779569715777644, Avg loss: 0.08651108093443813\n",
      "Last loss: 0.08118612974834187, Avg loss: 0.08468812877887945\n",
      "Last loss: 0.13877687047471765, Avg loss: 0.0851763998114797\n",
      "Last loss: 0.08417148693948245, Avg loss: 0.08031209561906194\n",
      "Last loss: 0.018556938474398734, Avg loss: 0.07591232255413374\n",
      "Last loss: 0.18569894724680913, Avg loss: 0.08047773352347942\n",
      "Last loss: 0.1317514621571452, Avg loss: 0.07728159026918796\n",
      "Last loss: 0.05608841416965431, Avg loss: 0.07224608137463119\n",
      "Last loss: 0.07020903470361035, Avg loss: 0.0712152069048632\n",
      "Last loss: 0.09465216412764929, Avg loss: 0.07068703603521012\n",
      "Last loss: 0.049322129782594565, Avg loss: 0.06722852557637247\n",
      "Last loss: 0.06719576753327702, Avg loss: 0.0658202128686324\n",
      "Last loss: 0.03374379435136335, Avg loss: 0.06297039178142837\n",
      "Last loss: 0.015330462556369039, Avg loss: 0.060779681270069164\n",
      "Last loss: 0.04677828370979772, Avg loss: 0.060096571334207954\n",
      "Last loss: 3.1527385731986772, Avg loss: 7.565579514523833\n",
      "Last loss: 0.03546928234340191, Avg loss: 0.8368731782335987\n",
      "Last loss: 0.05446993626402691, Avg loss: 0.1656259948513027\n",
      "Last loss: 0.07096437757531206, Avg loss: 0.14281167264221192\n",
      "Last loss: 0.09025859195959643, Avg loss: 0.1332502643263172\n",
      "Last loss: 0.0930167452210764, Avg loss: 0.12555375219128556\n",
      "Last loss: 0.0473853054690844, Avg loss: 0.11825239316706339\n",
      "Last loss: 0.04065202985097693, Avg loss: 0.11366595058037264\n",
      "Last loss: 0.03389260928855707, Avg loss: 0.11078224961579725\n",
      "Last loss: 0.11429968187430854, Avg loss: 0.11017538664645019\n",
      "Last loss: 0.04376229027275755, Avg loss: 0.10365859805890482\n",
      "Last loss: 0.05214830082315768, Avg loss: 0.10190775128819424\n",
      "Last loss: 0.1320600156641443, Avg loss: 0.10223121630560233\n",
      "Last loss: 0.07559684632648873, Avg loss: 0.09694242563281054\n",
      "Last loss: 0.06567092633525676, Avg loss: 0.09519321830471222\n",
      "Last loss: 0.1643133138350622, Avg loss: 0.0962598809010763\n",
      "Last loss: 0.12358378208442969, Avg loss: 0.09233123905993275\n",
      "Last loss: 0.1380328618692791, Avg loss: 0.09081894226800198\n",
      "Last loss: 0.18370801736962536, Avg loss: 0.09065780183241881\n",
      "Last loss: 0.09709043485361075, Avg loss: 0.08540080402276702\n",
      "Last loss: 0.09967458433782943, Avg loss: 0.08381157697917214\n",
      "Last loss: 0.04467357088242196, Avg loss: 0.07985133188438234\n",
      "Last loss: 0.06163043045025157, Avg loss: 0.07951752871525818\n",
      "Last loss: 0.0428974421968112, Avg loss: 0.07757254540231356\n",
      "Last loss: 0.04191612005838138, Avg loss: 0.07589601017730857\n",
      "Last loss: 0.04886552647397865, Avg loss: 0.07432089445316793\n",
      "Last loss: 0.050536003349585336, Avg loss: 0.07353848830194813\n",
      "Last loss: 0.03591958320069599, Avg loss: 0.07171972008084014\n",
      "Last loss: 0.04113171618114761, Avg loss: 0.07024058100084443\n",
      "Last loss: 0.06933998422308058, Avg loss: 0.0702050121062813\n",
      "Last loss: 0.22897845801573558, Avg loss: 0.074554588385779\n",
      "Last loss: 0.02897577603188256, Avg loss: 0.06597021742694603\n",
      "Last loss: 0.07157022873642736, Avg loss: 0.06636506811804156\n",
      "Last loss: 0.05039751754447585, Avg loss: 0.06393947132382952\n",
      "Last loss: 0.17459517489152715, Avg loss: 0.06801641301735048\n",
      "Last loss: 0.12375816690200347, Avg loss: 0.06518053248728722\n",
      "Last loss: 0.08119128058879997, Avg loss: 0.06159531744449693\n",
      "Last loss: 0.09303618129373231, Avg loss: 0.06148683219994422\n",
      "Last loss: 0.08700297092007463, Avg loss: 0.06007557332202763\n",
      "Last loss: 0.016171638638194975, Avg loss: 0.05567912560105696\n",
      "Last loss: 0.25328463916507243, Avg loss: 0.5308488215897786\n",
      "Last loss: 0.29461948097847224, Avg loss: 0.24339273596011926\n",
      "Last loss: 0.11895310632020084, Avg loss: 0.2287959462045555\n",
      "Last loss: 0.22961079119929237, Avg loss: 0.22744205205375842\n",
      "Last loss: 0.23661653011232897, Avg loss: 0.22641957890062653\n",
      "Last loss: 0.10385823767452718, Avg loss: 0.21641098202090964\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 318.30it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.2390746760072089, Avg loss: 0.21430318388509143\n",
      "Last loss: 0.24777275906632784, Avg loss: 0.21391428898796666\n",
      "Last loss: 0.19637119292518293, Avg loss: 0.20856553890108856\n",
      "Last loss: 0.19201088004859512, Avg loss: 0.2061696949027249\n",
      "Last loss: 0.14020150719868688, Avg loss: 0.19731205953330153\n",
      "Last loss: 0.11370267551759727, Avg loss: 0.1913280015135751\n",
      "Last loss: 0.07125246366862995, Avg loss: 0.19455066203167343\n",
      "Last loss: 0.20194483324361664, Avg loss: 0.1881621389435487\n",
      "Last loss: 0.16869443475396093, Avg loss: 0.18635614758118202\n",
      "Last loss: 0.33973988664932553, Avg loss: 0.19010959685725407\n",
      "Last loss: 0.22240205751668768, Avg loss: 0.18279041145814054\n",
      "Last loss: 0.2622436445638248, Avg loss: 0.18191829302557222\n",
      "Last loss: 0.24335118516310333, Avg loss: 0.18020973421905942\n",
      "Last loss: 0.11951961684759306, Avg loss: 0.17203118638961196\n",
      "Last loss: 0.10638160734314524, Avg loss: 0.17133646167651215\n",
      "Last loss: 0.0905963123588241, Avg loss: 0.17078583045833684\n",
      "Last loss: 0.15770529452424054, Avg loss: 0.16783503255520799\n",
      "Last loss: 0.08256487429223228, Avg loss: 0.16357938646419024\n",
      "Last loss: 0.08926574831525894, Avg loss: 0.16383569414932858\n",
      "Last loss: 0.10933796662762442, Avg loss: 0.16628582459907373\n",
      "Last loss: 0.2700430336278463, Avg loss: 0.16802703112490017\n",
      "Last loss: 0.08181798715853184, Avg loss: 0.15902026054522095\n",
      "Last loss: 0.15712857153679666, Avg loss: 0.15634277075368763\n",
      "Last loss: 0.14915331093740944, Avg loss: 0.15672866150537182\n",
      "Last loss: 0.1787932254570358, Avg loss: 0.1546620785768593\n",
      "Last loss: 0.10785331936683457, Avg loss: 0.15562874396396312\n",
      "Last loss: 0.29901556441990984, Avg loss: 0.15958183560975853\n",
      "Last loss: 0.1253055588593673, Avg loss: 0.14945578229515535\n",
      "Last loss: 0.11383783739166685, Avg loss: 0.14127849874332796\n",
      "Last loss: 0.22085341231861516, Avg loss: 0.15397107594789466\n",
      "Last loss: 0.1435393660271103, Avg loss: 0.14779308617857792\n",
      "Last loss: 0.19038051943165707, Avg loss: 0.14994688127720124\n",
      "Last loss: 0.12155565957601933, Avg loss: 0.1458471008908435\n",
      "Last loss: 0.07971144230493164, Avg loss: 0.13993170373050995\n",
      "Last loss: 1.5167111910678428, Avg loss: 1.5934059675158079\n",
      "Last loss: 0.8532567840217286, Avg loss: 0.8726947558375807\n",
      "Last loss: 0.2783837658717939, Avg loss: 0.687451424332841\n",
      "Last loss: 0.347677623834581, Avg loss: 0.597804685707326\n",
      "Last loss: 0.2347193287579931, Avg loss: 0.5307629658288919\n",
      "Last loss: 0.355815730740565, Avg loss: 0.4901120804150642\n",
      "Last loss: 0.3288440413616741, Avg loss: 0.4355311506253817\n",
      "Last loss: 0.289660258561692, Avg loss: 0.4054993230636831\n",
      "Last loss: 0.30043173353352837, Avg loss: 0.370838785716854\n",
      "Last loss: 0.6626848237898403, Avg loss: 0.3594751125119669\n",
      "Last loss: 0.21323011207375847, Avg loss: 0.3136798668113077\n",
      "Last loss: 0.19943946448021496, Avg loss: 0.3073667638459504\n",
      "Last loss: 0.12830526865412942, Avg loss: 0.27915601032849297\n",
      "Last loss: 0.14675498971147588, Avg loss: 0.2578106530551123\n",
      "Last loss: 0.20349585919017715, Avg loss: 0.2496004752227331\n",
      "Last loss: 0.23927794238383274, Avg loss: 0.23450432634351698\n",
      "Last loss: 0.22939092038006131, Avg loss: 0.227954580576629\n",
      "Last loss: 0.28197780791014115, Avg loss: 0.21270460493940288\n",
      "Last loss: 0.10180836983075064, Avg loss: 0.19490527303365823\n",
      "Last loss: 0.17236621326426427, Avg loss: 0.1926387687663338\n",
      "Last loss: 0.23730327629384657, Avg loss: 0.18155277312151188\n",
      "Last loss: 0.07591419764798188, Avg loss: 0.16811332668433526\n",
      "Last loss: 0.06908269810737752, Avg loss: 0.1627525934343486\n",
      "Last loss: 0.09398364019326363, Avg loss: 0.15701040887879747\n",
      "Last loss: 0.10168684097676928, Avg loss: 0.14784765638458347\n",
      "Last loss: 0.16331496775186602, Avg loss: 0.1464680127389258\n",
      "Last loss: 0.14669124293346297, Avg loss: 0.14060800064413034\n",
      "Last loss: 0.09809009782703049, Avg loss: 0.13316254861879676\n",
      "Last loss: 0.32525214510188516, Avg loss: 0.13857264429729985\n",
      "Last loss: 0.0645880295369049, Avg loss: 0.12323947577166587\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 316.70it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 340.61it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.1108453274141589, Avg loss: 0.11896352640039608\n",
      "Last loss: 0.17879305986335836, Avg loss: 0.11584874472806547\n",
      "Last loss: 0.06615517354658412, Avg loss: 0.10984423325132053\n",
      "Last loss: 0.07886130461261961, Avg loss: 0.10563317916353926\n",
      "Last loss: 0.1594868060767057, Avg loss: 0.10439260996001999\n",
      "Last loss: 0.09151788600639707, Avg loss: 0.10046509939001678\n",
      "Last loss: 0.07188101205301069, Avg loss: 0.09429592569436249\n",
      "Last loss: 0.11346483774035271, Avg loss: 0.09356403770548158\n",
      "Last loss: 0.05854407537730296, Avg loss: 0.08827495420126656\n",
      "Last loss: 0.07089170460743896, Avg loss: 0.0859264300192055\n",
      "Last loss: 2.460941370249473, Avg loss: 3.142376416473159\n",
      "Last loss: 1.2595508340217558, Avg loss: 0.8591140370646806\n",
      "Last loss: 0.5387621326334431, Avg loss: 0.5547661516577223\n",
      "Last loss: 0.391622424293939, Avg loss: 0.38455468176145813\n",
      "Last loss: 0.2536706075695072, Avg loss: 0.29378104049613574\n",
      "Last loss: 0.12420948394190796, Avg loss: 0.23842884437208248\n",
      "Last loss: 0.17971376824893476, Avg loss: 0.21342796946997686\n",
      "Last loss: 0.18568457043128586, Avg loss: 0.19774500673121326\n",
      "Last loss: 0.12591413569432755, Avg loss: 0.1827043082910905\n",
      "Last loss: 0.21782215598614313, Avg loss: 0.17397379261358414\n",
      "Last loss: 0.08120385668934048, Avg loss: 0.15939927233237572\n",
      "Last loss: 0.11904467309066102, Avg loss: 0.15488853203964625\n",
      "Last loss: 0.13087451693667046, Avg loss: 0.14967832516696597\n",
      "Last loss: 0.151549926480296, Avg loss: 0.1471154465702794\n",
      "Last loss: 0.15167824063599167, Avg loss: 0.1404767333851078\n",
      "Last loss: 0.23166842367480486, Avg loss: 0.14027795543562002\n",
      "Last loss: 0.11222632207839237, Avg loss: 0.13097607756324445\n",
      "Last loss: 0.10735404302834468, Avg loss: 0.12758935887940992\n",
      "Last loss: 0.1253931652615224, Avg loss: 0.12506384784921806\n",
      "Last loss: 0.23553402972948248, Avg loss: 0.12606461351959966\n",
      "Last loss: 0.07582275879906712, Avg loss: 0.11853278742022212\n",
      "Last loss: 0.11258537893797207, Avg loss: 0.11548179736141563\n",
      "Last loss: 0.09254896768683481, Avg loss: 0.11159865411554583\n",
      "Last loss: 0.09426922270725935, Avg loss: 0.10913781866797359\n",
      "Last loss: 0.13113219234168633, Avg loss: 0.10854518770183232\n",
      "Last loss: 0.1277680372200697, Avg loss: 0.10589548226508352\n",
      "Last loss: 0.06569800845114554, Avg loss: 0.10160401606426149\n",
      "Last loss: 0.11208491083737154, Avg loss: 0.1011693136513875\n",
      "Last loss: 0.06332462571375239, Avg loss: 0.09742641544272111\n",
      "Last loss: 0.1610003256522506, Avg loss: 0.09968115801643164\n",
      "Last loss: 0.06765470587479065, Avg loss: 0.09369205860359188\n",
      "Last loss: 0.11279096047888498, Avg loss: 0.09251533740218941\n",
      "Last loss: 0.09155672720668255, Avg loss: 0.09183100865767924\n",
      "Last loss: 0.07697728525435395, Avg loss: 0.08947672334489513\n",
      "Last loss: 0.05481874753452655, Avg loss: 0.08710760340686526\n",
      "Last loss: 0.053961152256274264, Avg loss: 0.08530309989089438\n",
      "Last loss: 0.12029328380260668, Avg loss: 0.08625303265297005\n",
      "Last loss: 0.05050237573990035, Avg loss: 0.08365388516409186\n",
      "Last loss: 0.11096567043990019, Avg loss: 0.08387155464492062\n",
      "Last loss: 0.06252785788978718, Avg loss: 0.08043858682676326\n",
      "Last loss: 2.0703420218427904, Avg loss: 4.733821358491729\n",
      "Last loss: 0.7807987799571983, Avg loss: 0.886574962183476\n",
      "Last loss: 0.2550858729832265, Avg loss: 0.5027288383229014\n",
      "Last loss: 0.7710291042639218, Avg loss: 0.4067421159289744\n",
      "Last loss: 0.35967206354501297, Avg loss: 0.33475450614821417\n",
      "Last loss: 0.3502242311625602, Avg loss: 0.3048569923196704\n",
      "Last loss: 0.12752468290190444, Avg loss: 0.2630389313115792\n",
      "Last loss: 0.3078257167051339, Avg loss: 0.2456529005777081\n",
      "Last loss: 0.3416979434583691, Avg loss: 0.23121968851489402\n",
      "Last loss: 0.40020443267538863, Avg loss: 0.2122691734051519\n",
      "Last loss: 0.3613795538992782, Avg loss: 0.1980932614730745\n",
      "Last loss: 0.051415036252613955, Avg loss: 0.16494006499863367\n",
      "Last loss: 0.2707258462428073, Avg loss: 0.17941771149889335\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 310.13it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 348.38it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.08524616664081547, Avg loss: 0.15789191850737627\n",
      "Last loss: 0.19196160392115594, Avg loss: 0.1516738499508486\n",
      "Last loss: 0.07773100585556716, Avg loss: 0.14303766134275725\n",
      "Last loss: 0.2249977285530011, Avg loss: 0.14160290562359626\n",
      "Last loss: 0.027289543894022134, Avg loss: 0.12623930158199279\n",
      "Last loss: 0.2057918694154524, Avg loss: 0.12738194741893394\n",
      "Last loss: 0.0996699622629657, Avg loss: 0.12470546913630728\n",
      "Last loss: 0.027287938670909315, Avg loss: 0.12125209198747526\n",
      "Last loss: 0.05728304840373893, Avg loss: 0.11138119899384037\n",
      "Last loss: 0.1552888460136719, Avg loss: 0.11155439964030107\n",
      "Last loss: 0.01984394846272985, Avg loss: 0.10050203494147941\n",
      "Last loss: 0.12356267468095662, Avg loss: 0.10031463512176843\n",
      "Last loss: 0.06077805260284713, Avg loss: 0.10086293064901916\n",
      "Last loss: 0.1086811299602317, Avg loss: 0.09445455602492094\n",
      "Last loss: 0.059666659524106894, Avg loss: 0.09251008732690964\n",
      "Last loss: 0.10079625390029491, Avg loss: 0.08836795419563746\n",
      "Last loss: 0.09310995263179475, Avg loss: 0.08717875897615361\n",
      "Last loss: 0.06078709185705618, Avg loss: 0.08252309708610207\n",
      "Last loss: 0.10550817857619049, Avg loss: 0.0806815203581479\n",
      "Last loss: 0.022355122561435938, Avg loss: 0.07683007785254434\n",
      "Last loss: 0.08138270670204631, Avg loss: 0.07613642546665095\n",
      "Last loss: 0.027323942139615805, Avg loss: 0.07236640672003382\n",
      "Last loss: 0.06593869885156832, Avg loss: 0.07123752501623906\n",
      "Last loss: 0.0595563795141754, Avg loss: 0.07121303682404602\n",
      "Last loss: 0.046033016506051724, Avg loss: 0.06861022830404269\n",
      "Last loss: 0.021018977995239753, Avg loss: 0.06731211734089655\n",
      "Last loss: 0.04241950556002601, Avg loss: 0.06771493676980735\n",
      "Last loss: 0.06325072331476894, Avg loss: 1.0485919495239133\n",
      "Last loss: 0.37429521406579314, Avg loss: 0.2608384412696025\n",
      "Last loss: 0.2487902399707845, Avg loss: 0.23950448310438194\n",
      "Last loss: 0.2821467837143826, Avg loss: 0.24608137261322008\n",
      "Last loss: 0.07076860258227621, Avg loss: 0.2254128827011375\n",
      "Last loss: 0.314370982833868, Avg loss: 0.23517170523618391\n",
      "Last loss: 0.10762245279683687, Avg loss: 0.21963637922438\n",
      "Last loss: 0.48068775640950107, Avg loss: 0.2331444703446657\n",
      "Last loss: 0.20526123668498775, Avg loss: 0.22239823739615233\n",
      "Last loss: 0.10738549414490943, Avg loss: 0.21692404581484123\n",
      "Last loss: 0.15680952243627128, Avg loss: 0.21980353425455795\n",
      "Last loss: 0.0809468798237449, Avg loss: 0.21199085158606643\n",
      "Last loss: 0.4296273967520865, Avg loss: 0.22419278929076297\n",
      "Last loss: 0.2049730866755469, Avg loss: 0.21774180151263284\n",
      "Last loss: 0.34302210225207824, Avg loss: 0.22056316957774397\n",
      "Last loss: 0.12444193728036801, Avg loss: 0.2097137430078414\n",
      "Last loss: 0.21819298853149616, Avg loss: 0.21134541633068432\n",
      "Last loss: 0.2032416865418744, Avg loss: 0.21202843465353036\n",
      "Last loss: 0.13901154381960368, Avg loss: 0.2056510322860641\n",
      "Last loss: 0.20811483250293605, Avg loss: 0.21075965252696408\n",
      "Last loss: 0.24946928453271844, Avg loss: 0.2099551344695286\n",
      "Last loss: 0.2444828256827311, Avg loss: 0.20638535762361349\n",
      "Last loss: 0.19713264327938249, Avg loss: 0.21142774988842927\n",
      "Last loss: 0.2513531677701497, Avg loss: 0.20673833432895117\n",
      "Last loss: 0.1785110266262545, Avg loss: 0.20328274027330775\n",
      "Last loss: 0.333859844648499, Avg loss: 0.20762086663948642\n",
      "Last loss: 0.16123756543797907, Avg loss: 0.20156629330786538\n",
      "Last loss: 0.3305403822282934, Avg loss: 0.2063423749902638\n",
      "Last loss: 0.0788612442956339, Avg loss: 0.19596071726715936\n",
      "Last loss: 0.11630996867407424, Avg loss: 0.19653238263183026\n",
      "Last loss: 0.07531229568403497, Avg loss: 0.1932303580843296\n",
      "Last loss: 0.0709868291560176, Avg loss: 0.1923478385724688\n",
      "Last loss: 0.034968668798717115, Avg loss: 0.19202068676922449\n",
      "Last loss: 0.15068611772220958, Avg loss: 0.1948232402407899\n",
      "Last loss: 0.19268018260716624, Avg loss: 0.19790826421631846\n",
      "Last loss: 0.03253465155368817, Avg loss: 0.18812580504031995\n",
      "Last loss: 0.1594241487574801, Avg loss: 0.1934762222481772\n",
      "Last loss: 0.2543069456595914, Avg loss: 0.19847444879950504\n",
      "Last loss: 0.18378512972321762, Avg loss: 0.19216323419201173\n",
      "Last loss: 0.08076470487526356, Avg loss: 0.18532633686038077\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 324.88it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 1.104025683792047, Avg loss: 0.8764350267520766\n",
      "Last loss: 0.3105648131771648, Avg loss: 0.3946040275420375\n",
      "Last loss: 0.11464557353643087, Avg loss: 0.2567991677929791\n",
      "Last loss: 0.5397280758910461, Avg loss: 0.2073696720544987\n",
      "Last loss: 0.06323120344289443, Avg loss: 0.15087753392842157\n",
      "Last loss: 0.16659243962147413, Avg loss: 0.1339084487114045\n",
      "Last loss: 0.26974499384104245, Avg loss: 0.1212297053958269\n",
      "Last loss: 0.05028898982156518, Avg loss: 0.09990892266786947\n",
      "Last loss: 0.1273451354898575, Avg loss: 0.09153831331460648\n",
      "Last loss: 0.04655641250495584, Avg loss: 0.08052858623629036\n",
      "Last loss: 0.12091230468287265, Avg loss: 0.0767283665671906\n",
      "Last loss: 0.055319696462595894, Avg loss: 0.06753646872357556\n",
      "Last loss: 0.030626612760166313, Avg loss: 0.06217515163914559\n",
      "Last loss: 0.07200831547368113, Avg loss: 0.05899611380814757\n",
      "Last loss: 0.030665495592032428, Avg loss: 0.054024404840549586\n",
      "Last loss: 0.00785349869994613, Avg loss: 0.04961600954744415\n",
      "Last loss: 0.07094527941778653, Avg loss: 0.04890724383829148\n",
      "Last loss: 0.035942363877385114, Avg loss: 0.04480854492166313\n",
      "Last loss: 0.023109974975543696, Avg loss: 0.04213476740437831\n",
      "Last loss: 0.07068218995360728, Avg loss: 0.04160346858578355\n",
      "Last loss: 0.08630317600322361, Avg loss: 0.04020670914675079\n",
      "Last loss: 0.031029441934938295, Avg loss: 0.03644294547679397\n",
      "Last loss: 0.019144452468005814, Avg loss: 0.03447241555441709\n",
      "Last loss: 0.033319067737476976, Avg loss: 0.03365440218240453\n",
      "Last loss: 0.008774601148222524, Avg loss: 0.03151024628416611\n",
      "Last loss: 0.03584041338950801, Avg loss: 0.03108854742288417\n",
      "Last loss: 0.009679290706183855, Avg loss: 0.029158986983785336\n",
      "Last loss: 0.01679786603303101, Avg loss: 0.028379623014862933\n",
      "Last loss: 0.01056816610515189, Avg loss: 0.027286470377703174\n",
      "Last loss: 0.008160225396639454, Avg loss: 0.02649526963610224\n",
      "Last loss: 0.024996894268882297, Avg loss: 0.0262987638711263\n",
      "Last loss: 0.04135468412119535, Avg loss: 0.026283832311536172\n",
      "Last loss: 0.044592720309675093, Avg loss: 0.025803861524950292\n",
      "Last loss: 0.05210183094595815, Avg loss: 0.025501134377228136\n",
      "Last loss: 0.018169940167859024, Avg loss: 0.023648293006937122\n",
      "Last loss: 0.011588016931363482, Avg loss: 0.02282504634288129\n",
      "Last loss: 0.02546848641898385, Avg loss: 0.023023768660957324\n",
      "Last loss: 0.013617252823731057, Avg loss: 0.02202193474639471\n",
      "Last loss: 0.011826805596317085, Avg loss: 0.02160428423312896\n",
      "Last loss: 0.007788032720193406, Avg loss: 0.021010742458124863\n",
      "Last loss: 0.5421572652220574, Avg loss: 0.560361775999038\n",
      "Last loss: 0.43056556202763974, Avg loss: 0.3869220928625217\n",
      "Last loss: 0.35969368681992164, Avg loss: 0.3264935447745564\n",
      "Last loss: 0.28242234705555824, Avg loss: 0.29044923426962366\n",
      "Last loss: 0.20811245682568696, Avg loss: 0.2509404925605477\n",
      "Last loss: 0.3182723821503077, Avg loss: 0.24255144035637086\n",
      "Last loss: 0.12671349264917034, Avg loss: 0.21681255917198383\n",
      "Last loss: 0.34971234912223914, Avg loss: 0.2224043410895444\n",
      "Last loss: 0.1253532313528481, Avg loss: 0.19687982254716982\n",
      "Last loss: 0.3173507527521643, Avg loss: 0.20020569745030137\n",
      "Last loss: 0.07259850555794972, Avg loss: 0.18573669335172432\n",
      "Last loss: 0.12251993340432171, Avg loss: 0.1818934459126793\n",
      "Last loss: 0.40541362454259683, Avg loss: 0.18899006316114833\n",
      "Last loss: 0.27462473072033733, Avg loss: 0.1817659072797792\n",
      "Last loss: 0.13530697677428588, Avg loss: 0.17451630563281875\n",
      "Last loss: 0.10288727681975395, Avg loss: 0.17094991633966844\n",
      "Last loss: 0.1760350950266948, Avg loss: 0.17167399453366033\n",
      "Last loss: 0.13095747255555198, Avg loss: 0.16353111665341202\n",
      "Last loss: 0.3386182909108056, Avg loss: 0.16632763903110512\n",
      "Last loss: 0.08595710464815033, Avg loss: 0.16246496299466112\n",
      "Last loss: 0.05543437913144057, Avg loss: 0.15559800335355958\n",
      "Last loss: 0.19312155955784394, Avg loss: 0.1618289886005716\n",
      "Last loss: 0.14694214123922894, Avg loss: 0.1552188789348531\n",
      "Last loss: 0.244358819995751, Avg loss: 0.15268937002366248\n",
      "Last loss: 0.2249329225594643, Avg loss: 0.15165204220958778\n",
      "Last loss: 0.24122685673061744, Avg loss: 0.15085365893092192\n",
      "Last loss: 0.09583097289107109, Avg loss: 0.13722365182149854\n",
      "Last loss: 0.06676623908447633, Avg loss: 0.13928576770579976\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 324.45it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 332.91it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.07479252405269873, Avg loss: 0.1340408735678678\n",
      "Last loss: 0.16444275566497105, Avg loss: 0.14212388364100143\n",
      "Last loss: 0.0988366255324972, Avg loss: 0.13051610642519026\n",
      "Last loss: 0.03159152280861245, Avg loss: 0.12556889575731464\n",
      "Last loss: 0.04814056721317142, Avg loss: 0.1302622938594245\n",
      "Last loss: 0.1814378753661734, Avg loss: 0.12427410295978565\n",
      "Last loss: 0.12228579157316846, Avg loss: 0.12533405701068517\n",
      "Last loss: 0.20194655329971156, Avg loss: 0.12295771469963035\n",
      "Last loss: 0.0677734753638577, Avg loss: 0.11696269717886719\n",
      "Last loss: 0.05685828844531903, Avg loss: 0.11654114291727921\n",
      "Last loss: 0.1284566097963973, Avg loss: 0.11597720345165893\n",
      "Last loss: 0.05518061709723503, Avg loss: 0.11494374995731291\n",
      "Last loss: 0.34342510735243004, Avg loss: 1.106680033070993\n",
      "Last loss: 0.9340478720658445, Avg loss: 0.5722421824762605\n",
      "Last loss: 0.3096146997054565, Avg loss: 0.5188192329959147\n",
      "Last loss: 0.5351164181742494, Avg loss: 0.46305396863455645\n",
      "Last loss: 0.3864825192567466, Avg loss: 0.4616923299956518\n",
      "Last loss: 0.1815079771662537, Avg loss: 0.3949210345717959\n",
      "Last loss: 0.09368527506519778, Avg loss: 0.34508267687934097\n",
      "Last loss: 0.09200742433443973, Avg loss: 0.3349225495328495\n",
      "Last loss: 0.12084778104883609, Avg loss: 0.26685577004419386\n",
      "Last loss: 0.2627726655507983, Avg loss: 0.26619057598394225\n",
      "Last loss: 0.33256685540823455, Avg loss: 0.26899890718039954\n",
      "Last loss: 0.2293644766917231, Avg loss: 0.24338464436999616\n",
      "Last loss: 0.21999749881473535, Avg loss: 0.22179737708252936\n",
      "Last loss: 0.14253368098712993, Avg loss: 0.19928028067773015\n",
      "Last loss: 0.07055462665804034, Avg loss: 0.18433592950421662\n",
      "Last loss: 0.1090123394932043, Avg loss: 0.17734048977511502\n",
      "Last loss: 0.3241753369235677, Avg loss: 0.1751731193169484\n",
      "Last loss: 0.10598831325844484, Avg loss: 0.16779032221768553\n",
      "Last loss: 0.10913647298357858, Avg loss: 0.15417895254193342\n",
      "Last loss: 0.10136264525108818, Avg loss: 0.1431185510560646\n",
      "Last loss: 0.09327623915115825, Avg loss: 0.1429439477530783\n",
      "Last loss: 0.10805611732013456, Avg loss: 0.1353545350460818\n",
      "Last loss: 0.18649949759258083, Avg loss: 0.14087058056719887\n",
      "Last loss: 0.07507764344206755, Avg loss: 0.14286741812413653\n",
      "Last loss: 0.06209176944258621, Avg loss: 0.12441245214226905\n",
      "Last loss: 0.14409751260140904, Avg loss: 0.12226610475643179\n",
      "Last loss: 0.06478708269278992, Avg loss: 0.11365876388612364\n",
      "Last loss: 0.06053055571733553, Avg loss: 0.11259958586223545\n",
      "Last loss: 0.11789150949887643, Avg loss: 0.1123905806494705\n",
      "Last loss: 0.1449054751873547, Avg loss: 0.11745427504619751\n",
      "Last loss: 0.11142943519810787, Avg loss: 0.10855300888578273\n",
      "Last loss: 0.0571783819615762, Avg loss: 0.10377380201100211\n",
      "Last loss: 0.051774616427583305, Avg loss: 0.10160481629930097\n",
      "Last loss: 0.0741592186730031, Avg loss: 0.10220362471337571\n",
      "Last loss: 0.15420002378827805, Avg loss: 0.10555931288176433\n",
      "Last loss: 0.13060231000806571, Avg loss: 0.09780710205652972\n",
      "Last loss: 0.17422805984224146, Avg loss: 0.09256024714970709\n",
      "Last loss: 0.08760450696337763, Avg loss: 0.08445130374820647\n",
      "Last loss: 0.10844281700683424, Avg loss: 0.08881107717065245\n",
      "Last loss: 0.0461317685325056, Avg loss: 0.08254170737052215\n",
      "Last loss: 0.3212734812484316, Avg loss: 5.314172345051683\n",
      "Last loss: 0.09466393227173199, Avg loss: 0.39014386484381497\n",
      "Last loss: 0.06117537232940663, Avg loss: 0.28754774955868717\n",
      "Last loss: 0.19424820422392103, Avg loss: 0.23938114242945702\n",
      "Last loss: 0.5221958843971103, Avg loss: 0.20938337825533695\n",
      "Last loss: 0.11737116850855268, Avg loss: 0.17075512890522598\n",
      "Last loss: 0.0508284957848137, Avg loss: 0.15389820478756902\n",
      "Last loss: 0.09187815695052995, Avg loss: 0.14469469028998408\n",
      "Last loss: 0.32113506237956135, Avg loss: 0.14183335745406744\n",
      "Last loss: 0.0887842880625854, Avg loss: 0.1269190174528066\n",
      "Last loss: 0.0801998699631509, Avg loss: 0.11953099693839883\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 321.84it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 348.65it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.305339696682828, Avg loss: 0.12113894413851453\n",
      "Last loss: 0.07605481873664392, Avg loss: 0.11268033357618079\n",
      "Last loss: 0.13380818019852006, Avg loss: 0.10687345740483457\n",
      "Last loss: 0.043314407236984404, Avg loss: 0.09973361248212588\n",
      "Last loss: 0.17323174640093086, Avg loss: 0.10082334373802065\n",
      "Last loss: 0.04109666910070653, Avg loss: 0.0923179168651591\n",
      "Last loss: 0.009494808137355683, Avg loss: 0.08829712525944841\n",
      "Last loss: 0.037363724898444084, Avg loss: 0.08654480612437612\n",
      "Last loss: 0.03714546983663672, Avg loss: 0.0834168782734536\n",
      "Last loss: 0.04288860235476491, Avg loss: 0.08151660157755115\n",
      "Last loss: 0.07961115412441727, Avg loss: 0.0804515828609054\n",
      "Last loss: 0.050832385136091264, Avg loss: 0.07766438065264028\n",
      "Last loss: 0.050137007387792684, Avg loss: 0.07509079386300378\n",
      "Last loss: 0.09055041238829592, Avg loss: 0.07544297586838564\n",
      "Last loss: 0.09994594740541722, Avg loss: 0.07352484800164488\n",
      "Last loss: 0.01578581197494276, Avg loss: 0.06897747342916544\n",
      "Last loss: 0.05992834394279574, Avg loss: 0.06929192117172575\n",
      "Last loss: 0.2068826399306163, Avg loss: 0.07226390618587931\n",
      "Last loss: 0.10981095629777174, Avg loss: 0.06746093992782973\n",
      "Last loss: 0.14574319329208585, Avg loss: 0.0661900102959345\n",
      "Last loss: 0.04616342278363157, Avg loss: 0.06067991357137175\n",
      "Last loss: 0.03164896995492573, Avg loss: 0.058441415106449186\n",
      "Last loss: 0.04185100813217242, Avg loss: 0.05771799704629699\n",
      "Last loss: 0.1613551538537769, Avg loss: 0.06120034359458766\n",
      "Last loss: 0.06107205915533638, Avg loss: 0.05633334652848653\n",
      "Last loss: 0.020176438330279413, Avg loss: 0.053438332494733384\n",
      "Last loss: 0.016866994554925175, Avg loss: 0.05218789888726313\n",
      "Last loss: 0.032955538089183806, Avg loss: 0.05183967220796796\n",
      "Last loss: 0.04857964510215412, Avg loss: 0.050910226326495285\n",
      "Last loss: 1.1189152315113693, Avg loss: 4.157962025334867\n",
      "Last loss: 0.3177724703576707, Avg loss: 0.49620450287537\n",
      "Last loss: 0.6854366566993852, Avg loss: 0.41845293715687054\n",
      "Last loss: 0.12527733230877938, Avg loss: 0.3591408637376424\n",
      "Last loss: 0.5172254465047876, Avg loss: 0.3465724655822345\n",
      "Last loss: 0.3938974619272874, Avg loss: 0.3289625607041438\n",
      "Last loss: 0.44729516782130085, Avg loss: 0.31537444329867603\n",
      "Last loss: 0.5092225643745261, Avg loss: 0.3100723819143685\n",
      "Last loss: 0.45272360500393194, Avg loss: 0.2970396984052383\n",
      "Last loss: 0.25809603522631674, Avg loss: 0.27879032385673697\n",
      "Last loss: 0.1635413313750671, Avg loss: 0.268823231449845\n",
      "Last loss: 0.19195331380250236, Avg loss: 0.2637058631527335\n",
      "Last loss: 0.3347657157750841, Avg loss: 0.2602095902573755\n",
      "Last loss: 0.3407459972044053, Avg loss: 0.25393169219755884\n",
      "Last loss: 0.465372943251914, Avg loss: 0.2539372827952202\n",
      "Last loss: 0.2233939704738549, Avg loss: 0.2418273292846928\n",
      "Last loss: 0.22100167868050047, Avg loss: 0.23665303805376106\n",
      "Last loss: 0.11362031253671236, Avg loss: 0.2218559603605534\n",
      "Last loss: 0.26690311711528564, Avg loss: 0.2278094694783113\n",
      "Last loss: 0.35988537911850826, Avg loss: 0.22635720825058311\n",
      "Last loss: 0.32508763696611986, Avg loss: 0.22029586430201886\n",
      "Last loss: 0.2680064312369261, Avg loss: 0.21132571031048505\n",
      "Last loss: 0.2344419007449562, Avg loss: 0.20560184804341797\n",
      "Last loss: 0.4211886072054666, Avg loss: 0.20789706965103266\n",
      "Last loss: 0.17250828766004425, Avg loss: 0.19457085287380754\n",
      "Last loss: 0.17664574612735373, Avg loss: 0.19146852883755353\n",
      "Last loss: 0.08813773325483872, Avg loss: 0.18617275092998756\n",
      "Last loss: 0.27958382260334447, Avg loss: 0.18855924800668886\n",
      "Last loss: 0.21503199068431705, Avg loss: 0.18046091059032882\n",
      "Last loss: 0.16249686938628552, Avg loss: 0.1763357958173676\n",
      "Last loss: 0.05204127950822666, Avg loss: 0.16981278834871746\n",
      "Last loss: 0.07664550194663623, Avg loss: 0.1715589338021498\n",
      "Last loss: 0.21712929961069954, Avg loss: 0.1684376664514111\n",
      "Last loss: 0.08228203572812945, Avg loss: 0.15993271688498042\n",
      "Last loss: 0.10508862445489373, Avg loss: 0.1575766629391446\n",
      "Last loss: 0.13017515847738295, Avg loss: 0.15807617827180018\n",
      "Last loss: 0.1085866497603192, Avg loss: 0.15495253495638145\n",
      "Last loss: 0.21440650741113382, Avg loss: 0.15646183901355556\n",
      "Last loss: 0.18472757843971443, Avg loss: 0.15438237207247746\n",
      "Last loss: 0.04023138480965972, Avg loss: 0.1444840747555825\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 40/40 [00:00<00:00, 337.50it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 4.190144457066046, Avg loss: 7.449533810870518\n",
      "Last loss: 1.6169900061821756, Avg loss: 2.420560266405791\n",
      "Last loss: 0.8744299466837422, Avg loss: 1.1631851376991744\n",
      "Last loss: 0.47943219913359464, Avg loss: 0.6976505415747531\n",
      "Last loss: 0.5395855711689739, Avg loss: 0.5391208240526335\n",
      "Last loss: 0.5466559677994651, Avg loss: 0.4683673378669696\n",
      "Last loss: 0.34501476323534636, Avg loss: 0.41586870873217846\n",
      "Last loss: 0.45172186599207187, Avg loss: 0.38744756925367313\n",
      "Last loss: 0.6089876437850216, Avg loss: 0.37057045858424825\n",
      "Last loss: 0.10024738087974092, Avg loss: 0.3401517134208599\n",
      "Last loss: 0.3911563109542322, Avg loss: 0.33041984352515275\n",
      "Last loss: 0.3867843367262727, Avg loss: 0.32008288490598874\n",
      "Last loss: 0.4148124330610221, Avg loss: 0.308304829653833\n",
      "Last loss: 0.11203147294647801, Avg loss: 0.2888985249632379\n",
      "Last loss: 0.4472727066541606, Avg loss: 0.2935374621851229\n",
      "Last loss: 0.1205694222701349, Avg loss: 0.27556646839496823\n",
      "Last loss: 0.3115024166189462, Avg loss: 0.2710335828066418\n",
      "Last loss: 0.24937899769952504, Avg loss: 0.2620150193882129\n",
      "Last loss: 0.24423185390784352, Avg loss: 0.2539966053872482\n",
      "Last loss: 0.21844017059719129, Avg loss: 0.24774170626988934\n",
      "Last loss: 0.21478907431177147, Avg loss: 0.2423996200045567\n",
      "Last loss: 0.5660916890936971, Avg loss: 0.2504904031845742\n",
      "Last loss: 0.3885061758108122, Avg loss: 0.23876059558850843\n",
      "Last loss: 0.31784792038097676, Avg loss: 0.23994902743517407\n",
      "Last loss: 0.055200449971211624, Avg loss: 0.2179289910498394\n",
      "Last loss: 0.2777504104006259, Avg loss: 0.22536563140782448\n",
      "Last loss: 0.06290114241127262, Avg loss: 0.21011968760128616\n",
      "Last loss: 0.13446051835218306, Avg loss: 0.21311468036639988\n",
      "Last loss: 0.20803877850709365, Avg loss: 0.21171980798565215\n",
      "Last loss: 0.43173664011139046, Avg loss: 0.217303587049616\n",
      "Last loss: 0.41692160571795767, Avg loss: 0.21040675382231583\n",
      "Last loss: 0.16047962302498586, Avg loss: 0.20071391572326333\n",
      "Last loss: 0.10724727883646096, Avg loss: 0.19575582351781978\n",
      "Last loss: 0.20551573126326422, Avg loss: 0.198482174519695\n",
      "Last loss: 0.20528021699582938, Avg loss: 0.19334944967735035\n",
      "Last loss: 0.3701952930613879, Avg loss: 0.19880918331133607\n",
      "Last loss: 0.265013965545276, Avg loss: 0.19029452379361914\n",
      "Last loss: 0.15595675270634338, Avg loss: 0.18500157244513188\n",
      "Last loss: 0.16058347445092958, Avg loss: 0.18243251844153832\n",
      "Last loss: 0.2814891506501431, Avg loss: 0.18632934764389947\n",
      "Last loss: 0.39345336461699787, Avg loss: 2.075959244426784\n",
      "Last loss: 0.3265752306538908, Avg loss: 0.3028473184794925\n",
      "Last loss: 0.17129454576064568, Avg loss: 0.236430381809634\n",
      "Last loss: 0.09320734349216919, Avg loss: 0.20294974979245642\n",
      "Last loss: 0.137132277756082, Avg loss: 0.1881959828298559\n",
      "Last loss: 0.1570458521379171, Avg loss: 0.18048656944109928\n",
      "Last loss: 0.16246326462728633, Avg loss: 0.1710107018655665\n",
      "Last loss: 0.09327015967414728, Avg loss: 0.15997853991584543\n",
      "Last loss: 0.15638944107990138, Avg loss: 0.15752140081843258\n",
      "Last loss: 0.0875231237660016, Avg loss: 0.14934410465630302\n",
      "Last loss: 0.12027744974702459, Avg loss: 0.1483019340470948\n",
      "Last loss: 0.09571349188040468, Avg loss: 0.141216002339967\n",
      "Last loss: 0.06311539749970531, Avg loss: 0.1353265522785059\n",
      "Last loss: 0.16110230048283006, Avg loss: 0.13529945905214918\n",
      "Last loss: 0.1848827144683306, Avg loss: 0.13260347388303909\n",
      "Last loss: 0.22246044409508783, Avg loss: 0.13310040822488453\n",
      "Last loss: 0.2554835367659534, Avg loss: 0.12961324701611548\n",
      "Last loss: 0.154846008076915, Avg loss: 0.12499488216760514\n",
      "Last loss: 0.1742251016804655, Avg loss: 0.1204526254981914\n",
      "Last loss: 0.05095473557820626, Avg loss: 0.11202823721189999\n",
      "Last loss: 0.13604606008983405, Avg loss: 0.11498868721338543\n",
      "Last loss: 0.16985606582134338, Avg loss: 0.11417272943657686\n",
      "Last loss: 0.05767600148121753, Avg loss: 0.10672413955872292\n",
      "Last loss: 0.08234190726001815, Avg loss: 0.10605185564515654\n",
      "Last loss: 0.15445355310565437, Avg loss: 0.10789176894543108\n",
      "Last loss: 0.16652349339555556, Avg loss: 0.10494884371263855\n",
      "Last loss: 0.07024333959011356, Avg loss: 0.10310589664979249\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 321.44it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 340.81it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.0562272962947648, Avg loss: 0.0978906691439641\n",
      "Last loss: 0.109151136812011, Avg loss: 0.09828440124930368\n",
      "Last loss: 0.08606356341075302, Avg loss: 0.09577921136464385\n",
      "Last loss: 0.08335123661478805, Avg loss: 0.09496963307080025\n",
      "Last loss: 0.022740265191271543, Avg loss: 0.0900601765478983\n",
      "Last loss: 0.1111328296077321, Avg loss: 0.09245429978072747\n",
      "Last loss: 0.10347417001916226, Avg loss: 0.0902477564211221\n",
      "Last loss: 0.2259321115026845, Avg loss: 0.09361645966391206\n",
      "Last loss: 0.17418600677631496, Avg loss: 0.09148828172118752\n",
      "Last loss: 0.04695605117322092, Avg loss: 0.0846751739436262\n",
      "Last loss: 0.16419683247004735, Avg loss: 0.08861130235835973\n",
      "Last loss: 0.05567607860323298, Avg loss: 0.08239391726860021\n",
      "Last loss: 0.16705332344052737, Avg loss: 0.08672267226315994\n",
      "Last loss: 0.4173215234415235, Avg loss: 4.877595503911201\n",
      "Last loss: 0.2390556306972667, Avg loss: 0.34714663326021566\n",
      "Last loss: 0.3057399799255517, Avg loss: 0.2811662768021394\n",
      "Last loss: 0.3004605460725812, Avg loss: 0.25533100679226073\n",
      "Last loss: 0.3249216322153505, Avg loss: 0.2401719659937388\n",
      "Last loss: 0.13068359635575158, Avg loss: 0.22383033176028272\n",
      "Last loss: 0.17225348324932266, Avg loss: 0.219477080786285\n",
      "Last loss: 0.2423156610204481, Avg loss: 0.21524425894044452\n",
      "Last loss: 0.07094618208333044, Avg loss: 0.1970848927335542\n",
      "Last loss: 0.3138725818678832, Avg loss: 0.2041054826292003\n",
      "Last loss: 0.08924976535695842, Avg loss: 0.19246802414548242\n",
      "Last loss: 0.28007470893584785, Avg loss: 0.1890642854860086\n",
      "Last loss: 0.19042219315807984, Avg loss: 0.1827558636731275\n",
      "Last loss: 0.05079152841416788, Avg loss: 0.17698267124312744\n",
      "Last loss: 0.1500253474471137, Avg loss: 0.17713963992982137\n",
      "Last loss: 0.211219505148578, Avg loss: 0.18129436020788298\n",
      "Last loss: 0.13503946177459106, Avg loss: 0.1688332667105616\n",
      "Last loss: 0.05498453024481291, Avg loss: 0.16351670151663159\n",
      "Last loss: 0.10588687605104248, Avg loss: 0.15800732293254502\n",
      "Last loss: 0.10679962723332681, Avg loss: 0.1569625033312499\n",
      "Last loss: 0.2974093552431421, Avg loss: 0.16468667777261617\n",
      "Last loss: 0.17051401103382627, Avg loss: 0.1533686109523334\n",
      "Last loss: 0.20057864181713264, Avg loss: 0.15166737912291853\n",
      "Last loss: 0.17808212273280258, Avg loss: 0.14967522311353204\n",
      "Last loss: 0.1850171245612739, Avg loss: 0.14650781759318743\n",
      "Last loss: 0.22251993397705733, Avg loss: 0.14754327298052639\n",
      "Last loss: 0.08126832629731141, Avg loss: 0.1388543429997914\n",
      "Last loss: 0.06935747350253772, Avg loss: 0.1383706036886017\n",
      "Last loss: 0.2136913594368385, Avg loss: 0.14280273022915202\n",
      "Last loss: 0.07859823887719389, Avg loss: 0.13383373027450957\n",
      "Last loss: 0.22062251928145105, Avg loss: 0.13702095023317507\n",
      "Last loss: 0.07215101611051519, Avg loss: 0.128516673773389\n",
      "Last loss: 0.18214771747122308, Avg loss: 0.13244134613669908\n",
      "Last loss: 0.13049408200590473, Avg loss: 0.12983575413537554\n",
      "Last loss: 0.07646501441580085, Avg loss: 0.1257918148663815\n",
      "Last loss: 0.18978931199916516, Avg loss: 0.12719317302657246\n",
      "Last loss: 0.19796237957940238, Avg loss: 0.12444302139521103\n",
      "Last loss: 0.11715684297419032, Avg loss: 0.12096359252657371\n",
      "Last loss: 0.10694163989760674, Avg loss: 0.12104397698446437\n",
      "Last loss: 0.16872847140051667, Avg loss: 0.12055076192194608\n",
      "Last loss: 0.8482826743163561, Avg loss: 3.359662147272348\n",
      "Last loss: 0.7763565575507132, Avg loss: 0.6488680945434625\n",
      "Last loss: 0.5540251416696453, Avg loss: 0.5400593428768196\n",
      "Last loss: 0.5575770480002055, Avg loss: 0.485849394596073\n",
      "Last loss: 0.523968368566296, Avg loss: 0.4444132033094516\n",
      "Last loss: 0.19894028416894977, Avg loss: 0.39922529984865035\n",
      "Last loss: 0.4222786093869104, Avg loss: 0.3815198928960676\n",
      "Last loss: 0.35664677284307816, Avg loss: 0.35423913110426053\n",
      "Last loss: 0.3019631076415983, Avg loss: 0.33139872970036854\n",
      "Last loss: 0.2002880980715325, Avg loss: 0.3139989428499994\n",
      "Last loss: 0.17206320300918665, Avg loss: 0.29463320227138723\n",
      "Last loss: 0.4109477369167739, Avg loss: 0.29058431682195274\n",
      "Last loss: 0.15200505372443932, Avg loss: 0.2702408290841999\n",
      "Last loss: 0.23247584260598927, Avg loss: 0.26037943248956863\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 340.56it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 337.07it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.25196210877094666, Avg loss: 0.25240926570413047\n",
      "Last loss: 0.35763940717879006, Avg loss: 0.24812090900845696\n",
      "Last loss: 0.25238886804148636, Avg loss: 0.23591479965363205\n",
      "Last loss: 0.18206120135908493, Avg loss: 0.2274638323873958\n",
      "Last loss: 0.18019485661822704, Avg loss: 0.2190182516317703\n",
      "Last loss: 0.33184764104881326, Avg loss: 0.2206661104822816\n",
      "Last loss: 0.19216199885467267, Avg loss: 0.210257759618754\n",
      "Last loss: 0.33204060178499734, Avg loss: 0.21050981843738104\n",
      "Last loss: 0.19109880572800791, Avg loss: 0.20069479557577996\n",
      "Last loss: 0.25311235289164535, Avg loss: 0.2003021491054907\n",
      "Last loss: 0.21804944397292483, Avg loss: 0.1950307858009473\n",
      "Last loss: 0.260375965798923, Avg loss: 0.193404022112496\n",
      "Last loss: 0.1040479532898041, Avg loss: 0.18495381400822294\n",
      "Last loss: 0.194103528238006, Avg loss: 0.18585974479994133\n",
      "Last loss: 0.24871397759910266, Avg loss: 0.18626374643503513\n",
      "Last loss: 0.11428117655789655, Avg loss: 0.17840053510222315\n",
      "Last loss: 0.14245672178650148, Avg loss: 0.17808520800082858\n",
      "Last loss: 0.10751209628919683, Avg loss: 0.17429731965274112\n",
      "Last loss: 0.11538651358195338, Avg loss: 0.17315389544937398\n",
      "Last loss: 0.15432348297571963, Avg loss: 0.17447459919084723\n",
      "Last loss: 0.08149458606092612, Avg loss: 0.17012270328947302\n",
      "Last loss: 0.0725415060057496, Avg loss: 0.16694877816460701\n",
      "Last loss: 0.14996629638199946, Avg loss: 0.1685687877817406\n",
      "Last loss: 0.20121532259528613, Avg loss: 0.17033687399396152\n",
      "Last loss: 0.09133738765425917, Avg loss: 0.16374037894307653\n",
      "Last loss: 0.36036050980827006, Avg loss: 0.1718523918657575\n",
      "Last loss: 0.3380802114219438, Avg loss: 1.4146739397545158\n",
      "Last loss: 0.19911883203707048, Avg loss: 0.25250869727551106\n",
      "Last loss: 0.23662490131519776, Avg loss: 0.2073532463597048\n",
      "Last loss: 0.3733694946977445, Avg loss: 0.18341158018840337\n",
      "Last loss: 0.266512963247559, Avg loss: 0.16560204031419712\n",
      "Last loss: 0.08063804323316963, Avg loss: 0.14289770023574921\n",
      "Last loss: 0.1378647308770046, Avg loss: 0.1380247428635391\n",
      "Last loss: 0.12319586903615426, Avg loss: 0.13236307828550817\n",
      "Last loss: 0.16849048412765663, Avg loss: 0.12429284392727803\n",
      "Last loss: 0.12646541292055394, Avg loss: 0.12080278044048204\n",
      "Last loss: 0.06398424750778997, Avg loss: 0.11292368272072008\n",
      "Last loss: 0.1291636391645032, Avg loss: 0.11082534842434523\n",
      "Last loss: 0.03324802600885607, Avg loss: 0.1045135008644368\n",
      "Last loss: 0.08284353749111933, Avg loss: 0.10203467180608704\n",
      "Last loss: 0.1927369489416848, Avg loss: 0.10548305751346335\n",
      "Last loss: 0.05635015587752942, Avg loss: 0.09895567880043118\n",
      "Last loss: 0.04798089120754517, Avg loss: 0.09436594499118783\n",
      "Last loss: 0.05997200315326549, Avg loss: 0.09181299266766496\n",
      "Last loss: 0.05413303768885353, Avg loss: 0.08979012740438827\n",
      "Last loss: 0.05604666068858603, Avg loss: 0.08827694737118982\n",
      "Last loss: 0.11207993711264977, Avg loss: 0.08657363824477475\n",
      "Last loss: 0.11695820063418581, Avg loss: 0.08555654312609162\n",
      "Last loss: 0.07667812188675341, Avg loss: 0.08098739511582265\n",
      "Last loss: 0.06610895749898554, Avg loss: 0.07959334955886874\n",
      "Last loss: 0.04786803996668841, Avg loss: 0.07747601795435868\n",
      "Last loss: 0.10952362684194406, Avg loss: 0.07865208259103341\n",
      "Last loss: 0.03779841859270787, Avg loss: 0.07632607380992554\n",
      "Last loss: 0.06679798189859558, Avg loss: 0.07307710602465722\n",
      "Last loss: 0.024730442802711264, Avg loss: 0.07067492640591079\n",
      "Last loss: 0.051582927991620856, Avg loss: 0.07082200593259541\n",
      "Last loss: 0.029147208081527903, Avg loss: 0.06639374646402652\n",
      "Last loss: 0.03487375831601311, Avg loss: 0.06592600915780333\n",
      "Last loss: 0.045892387517346596, Avg loss: 0.06522389426622167\n",
      "Last loss: 0.06394307450622268, Avg loss: 0.06423886143848095\n",
      "Last loss: 0.01103965592662381, Avg loss: 0.05973455374144881\n",
      "Last loss: 0.061758145420807965, Avg loss: 0.061698761180193615\n",
      "Last loss: 0.09791228074579808, Avg loss: 0.0613471983717061\n",
      "Last loss: 0.03648366084703507, Avg loss: 0.057638420676476426\n",
      "Last loss: 0.09922859612636888, Avg loss: 0.05913825939402832\n",
      "Last loss: 0.034404639550942154, Avg loss: 0.054201917028612316\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 325.65it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 3.8703185412057177, Avg loss: 9.30478983603925\n",
      "Last loss: 1.3993449988969942, Avg loss: 1.0932091599980276\n",
      "Last loss: 0.41467199314429715, Avg loss: 0.33081262518501653\n",
      "Last loss: 0.11593499710260997, Avg loss: 0.19550313951277293\n",
      "Last loss: 0.08809315374919721, Avg loss: 0.15082784785403866\n",
      "Last loss: 0.14931815688135192, Avg loss: 0.12664402188017007\n",
      "Last loss: 0.10100641694951872, Avg loss: 0.10730265881397516\n",
      "Last loss: 0.04576627692748791, Avg loss: 0.0927020823629385\n",
      "Last loss: 0.042222716554427976, Avg loss: 0.08330395537994309\n",
      "Last loss: 0.09715668716062283, Avg loss: 0.0785783982240798\n",
      "Last loss: 0.09788442325897477, Avg loss: 0.07221839446270034\n",
      "Last loss: 0.07469084268236538, Avg loss: 0.06618037190523428\n",
      "Last loss: 0.09039045275378255, Avg loss: 0.06239581669554401\n",
      "Last loss: 0.14537220589574587, Avg loss: 0.06100031344775309\n",
      "Last loss: 0.024685472452676306, Avg loss: 0.05338715693033159\n",
      "Last loss: 0.03928423149602606, Avg loss: 0.051313150341362856\n",
      "Last loss: 0.02061006078519615, Avg loss: 0.04837059848236104\n",
      "Last loss: 0.06388879753151161, Avg loss: 0.0480219249794991\n",
      "Last loss: 0.07754565949832762, Avg loss: 0.04656311513944364\n",
      "Last loss: 0.043928767314156895, Avg loss: 0.04358824153240372\n",
      "Last loss: 0.06911586522823202, Avg loss: 0.04312436585106798\n",
      "Last loss: 0.014657696433691495, Avg loss: 0.03958078063099028\n",
      "Last loss: 0.03181974137979176, Avg loss: 0.03903971984403365\n",
      "Last loss: 0.02641438384126242, Avg loss: 0.03775127788191679\n",
      "Last loss: 0.045505958947263045, Avg loss: 0.03740135425059336\n",
      "Last loss: 0.060196220406717876, Avg loss: 0.0370065587030205\n",
      "Last loss: 0.010195859172181994, Avg loss: 0.03400240253241893\n",
      "Last loss: 0.027916668763470216, Avg loss: 0.03382515409132442\n",
      "Last loss: 0.06136676688692466, Avg loss: 0.034348815477564125\n",
      "Last loss: 0.026159763634141193, Avg loss: 0.03212907546690974\n",
      "Last loss: 0.019399164853841307, Avg loss: 0.031190835246465057\n",
      "Last loss: 0.026598526440315663, Avg loss: 0.030828249468142905\n",
      "Last loss: 0.03269123878177747, Avg loss: 0.03041763540886309\n",
      "Last loss: 0.019079311488880587, Avg loss: 0.02921037527793611\n",
      "Last loss: 0.04473111788636178, Avg loss: 0.029635219031972825\n",
      "Last loss: 0.017357010798806752, Avg loss: 0.02813110650544881\n",
      "Last loss: 0.011198121339704556, Avg loss: 0.027284039520525265\n",
      "Last loss: 0.010054955380994319, Avg loss: 0.026798271883284444\n",
      "Last loss: 0.016526207414533176, Avg loss: 0.026466164309977377\n",
      "Last loss: 0.01630289578888553, Avg loss: 0.025999111613174888\n",
      "Last loss: 0.6399388386096855, Avg loss: 0.9327468117990757\n",
      "Last loss: 0.10951838409533358, Avg loss: 0.28269846751759825\n",
      "Last loss: 0.07120659188086015, Avg loss: 0.2008146027148484\n",
      "Last loss: 0.3076208099511945, Avg loss: 0.16561869293814044\n",
      "Last loss: 0.1406529975909354, Avg loss: 0.13533739281317725\n",
      "Last loss: 0.17085610105373727, Avg loss: 0.11647050629759269\n",
      "Last loss: 0.05174288249641236, Avg loss: 0.10203318011557198\n",
      "Last loss: 0.08619267048004833, Avg loss: 0.09204560917279139\n",
      "Last loss: 0.04693681530631087, Avg loss: 0.08330695345410968\n",
      "Last loss: 0.1341526194700048, Avg loss: 0.08008093745509275\n",
      "Last loss: 0.025760683270346113, Avg loss: 0.07127961308906033\n",
      "Last loss: 0.04692344539126314, Avg loss: 0.06807746921981853\n",
      "Last loss: 0.025383872382591828, Avg loss: 0.0632294801024116\n",
      "Last loss: 0.11601427216753962, Avg loss: 0.06320472357521584\n",
      "Last loss: 0.0798018601004726, Avg loss: 0.058083144911912576\n",
      "Last loss: 0.03971151936463219, Avg loss: 0.05500563615020778\n",
      "Last loss: 0.04129998919233266, Avg loss: 0.052153496721398754\n",
      "Last loss: 0.025109375051042045, Avg loss: 0.049736294485091835\n",
      "Last loss: 0.06950669143457511, Avg loss: 0.0490733489568809\n",
      "Last loss: 0.06334660370157684, Avg loss: 0.04762494367311143\n",
      "Last loss: 0.0357602007660763, Avg loss: 0.04432184673208343\n",
      "Last loss: 0.026352059552030252, Avg loss: 0.04263441685723945\n",
      "Last loss: 0.06463968666355221, Avg loss: 0.04260255804112418\n",
      "Last loss: 0.02313071727395736, Avg loss: 0.04029917107756035\n",
      "Last loss: 0.013190735241628093, Avg loss: 0.03912362190677031\n",
      "Last loss: 0.03286734499894258, Avg loss: 0.03821719153771343\n",
      "Last loss: 0.08163545408137968, Avg loss: 0.038962021844404796\n",
      "Last loss: 0.04352570012576634, Avg loss: 0.036429973676807476\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 327.19it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 352.09it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.033468670914860985, Avg loss: 0.035838994335186086\n",
      "Last loss: 0.028601630888844487, Avg loss: 0.03453798050529477\n",
      "Last loss: 0.03304209665820171, Avg loss: 0.033872349657535335\n",
      "Last loss: 0.07488482530086524, Avg loss: 0.0346762784643526\n",
      "Last loss: 0.0327193225990849, Avg loss: 0.032389019515563625\n",
      "Last loss: 0.018071925729005584, Avg loss: 0.030972322373103114\n",
      "Last loss: 0.018389983421640804, Avg loss: 0.030281963923570607\n",
      "Last loss: 0.027864558812989524, Avg loss: 0.03018246189326698\n",
      "Last loss: 0.014531449417364655, Avg loss: 0.029117055494906837\n",
      "Last loss: 0.027198952533757766, Avg loss: 0.028924965655830952\n",
      "Last loss: 0.0119892298226709, Avg loss: 0.02787320832263249\n",
      "Last loss: 0.07323387281482334, Avg loss: 0.029382893583300897\n",
      "Last loss: 0.5979293205916798, Avg loss: 0.7864075537629124\n",
      "Last loss: 0.5191465270617703, Avg loss: 0.4815549815279192\n",
      "Last loss: 0.36459723948751555, Avg loss: 0.402756638124206\n",
      "Last loss: 0.3275665022677861, Avg loss: 0.35330928452851584\n",
      "Last loss: 0.39864764568645694, Avg loss: 0.31912751345662765\n",
      "Last loss: 0.5360892818268618, Avg loss: 0.2950758739301088\n",
      "Last loss: 0.22143869679012418, Avg loss: 0.25862315866357577\n",
      "Last loss: 0.25932239314142125, Avg loss: 0.2405408042407617\n",
      "Last loss: 0.2182507690968047, Avg loss: 0.22237112262027459\n",
      "Last loss: 0.17774307024095698, Avg loss: 0.20698333192748708\n",
      "Last loss: 0.288484153557007, Avg loss: 0.19881242141078248\n",
      "Last loss: 0.1654999618680379, Avg loss: 0.18344590498259933\n",
      "Last loss: 0.14954021349857666, Avg loss: 0.17362768619510105\n",
      "Last loss: 0.04713646285606519, Avg loss: 0.16128713181785503\n",
      "Last loss: 0.1537542615144343, Avg loss: 0.157880396183055\n",
      "Last loss: 0.1228942081636649, Avg loss: 0.1495728339600308\n",
      "Last loss: 0.153778163152144, Avg loss: 0.14487746529459758\n",
      "Last loss: 0.145598156197429, Avg loss: 0.1383568122734332\n",
      "Last loss: 0.1433021388552844, Avg loss: 0.13334839262540088\n",
      "Last loss: 0.13805340179171233, Avg loss: 0.12817990578902622\n",
      "Last loss: 0.11252745875573598, Avg loss: 0.12265698957670759\n",
      "Last loss: 0.16364192315592635, Avg loss: 0.12038165779558\n",
      "Last loss: 0.13161788983483436, Avg loss: 0.11525700542078576\n",
      "Last loss: 0.10582900974157768, Avg loss: 0.11029751856188422\n",
      "Last loss: 0.06641915632019438, Avg loss: 0.10537148735692667\n",
      "Last loss: 0.1114194816332379, Avg loss: 0.10368251013303238\n",
      "Last loss: 0.08302268724485923, Avg loss: 0.0994861575271476\n",
      "Last loss: 0.14267126184830753, Avg loss: 0.09907887079559254\n",
      "Last loss: 0.10390094231521757, Avg loss: 0.09461184762681016\n",
      "Last loss: 0.07890213917776392, Avg loss: 0.09085292775394108\n",
      "Last loss: 0.08685870160699455, Avg loss: 0.08884177158842896\n",
      "Last loss: 0.0737680888602563, Avg loss: 0.08596977724594275\n",
      "Last loss: 0.07585927910598722, Avg loss: 0.08360470124213704\n",
      "Last loss: 0.07596353012633345, Avg loss: 0.08136617002482797\n",
      "Last loss: 0.0797458619381366, Avg loss: 0.07944994317508886\n",
      "Last loss: 0.09184200472432492, Avg loss: 0.07806653134466666\n",
      "Last loss: 0.051492238607309726, Avg loss: 0.07453062878674105\n",
      "Last loss: 0.09282681944622828, Avg loss: 0.07447032907301847\n",
      "Last loss: 0.03924745473228804, Avg loss: 0.07060386786725029\n",
      "Last loss: 0.07783102580098872, Avg loss: 0.0703646091920219\n",
      "Last loss: 0.7236835920355644, Avg loss: 1.5926027757199914\n",
      "Last loss: 0.3028683363310386, Avg loss: 0.597537977135791\n",
      "Last loss: 0.4015114942905703, Avg loss: 0.40359404682783845\n",
      "Last loss: 0.1557308861860674, Avg loss: 0.33164280464274537\n",
      "Last loss: 0.1953728672826607, Avg loss: 0.3026553756397851\n",
      "Last loss: 0.26969816052253864, Avg loss: 0.27999358530327806\n",
      "Last loss: 0.26110309051991043, Avg loss: 0.2419739105128113\n",
      "Last loss: 0.12192059066209532, Avg loss: 0.2317226590527683\n",
      "Last loss: 0.26030551009840797, Avg loss: 0.22343028552230035\n",
      "Last loss: 0.1916741819319922, Avg loss: 0.20899174303256837\n",
      "Last loss: 0.026385903488397464, Avg loss: 0.2736299819827471\n",
      "Last loss: 0.23910391469475611, Avg loss: 0.22820415303588978\n",
      "Last loss: 0.18556622887807203, Avg loss: 0.19377166817344135\n",
      "Last loss: 0.3190044506580678, Avg loss: 0.17628425533661846\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 330.05it/s]\n",
      " 78%|███████▊  | 31/40 [00:00<00:00, 301.96it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.23278799486353408, Avg loss: 0.21288844690187053\n",
      "Last loss: 0.19474649600173186, Avg loss: 0.187747584132586\n",
      "Last loss: 0.06103886902137434, Avg loss: 0.1555376389469198\n",
      "Last loss: 0.07384280990013628, Avg loss: 0.13754566014104036\n",
      "Last loss: 0.10192806704577453, Avg loss: 0.1586309616211705\n",
      "Last loss: 0.22149121141632988, Avg loss: 0.12182757560090335\n",
      "Last loss: 0.029428076409495588, Avg loss: 0.12217942483102892\n",
      "Last loss: 0.07259544679642052, Avg loss: 0.11435622831387439\n",
      "Last loss: 0.13690382089406902, Avg loss: 0.10103514388521963\n",
      "Last loss: 0.1124774036316034, Avg loss: 0.10007310277250957\n",
      "Last loss: 0.13664987634519846, Avg loss: 0.09703855066065117\n",
      "Last loss: 0.08644796126011157, Avg loss: 0.0906354477665937\n",
      "Last loss: 0.026707162058655767, Avg loss: 0.0882839437266955\n",
      "Last loss: 0.08031776657602285, Avg loss: 0.09787882066737234\n",
      "Last loss: 0.051314226841203195, Avg loss: 0.09255238799959437\n",
      "Last loss: 0.036519110566421174, Avg loss: 0.08317229156105051\n",
      "Last loss: 0.059212195358276765, Avg loss: 0.08797232755298569\n",
      "Last loss: 0.08554648479735982, Avg loss: 0.07853528647749228\n",
      "Last loss: 0.08909136648062416, Avg loss: 0.07930173391317494\n",
      "Last loss: 0.13198890061447688, Avg loss: 0.0778948650018314\n",
      "Last loss: 0.04868219388348147, Avg loss: 0.07451194004481723\n",
      "Last loss: 0.08722085908008675, Avg loss: 0.07363539059682214\n",
      "Last loss: 0.017613690762353522, Avg loss: 0.06794764613257635\n",
      "Last loss: 0.06313647628933569, Avg loss: 0.07982163469788096\n",
      "Last loss: 0.04349688295803239, Avg loss: 0.0694450082291305\n",
      "Last loss: 0.08858353645026706, Avg loss: 0.06934288358278667\n",
      "Last loss: 0.3688263202449081, Avg loss: 1.9613328749641914\n",
      "Last loss: 0.07100964645123015, Avg loss: 0.24108315769220615\n",
      "Last loss: 0.11008706315742067, Avg loss: 0.12881539441442552\n",
      "Last loss: 0.15082443145943752, Avg loss: 0.11768874742165372\n",
      "Last loss: 0.0311021408314987, Avg loss: 0.10513109615866473\n",
      "Last loss: 0.2584521215851764, Avg loss: 0.10838119435038528\n",
      "Last loss: 0.10763607104257641, Avg loss: 0.09882740072724976\n",
      "Last loss: 0.10543433429584545, Avg loss: 0.09449207060222038\n",
      "Last loss: 0.14392293789137423, Avg loss: 0.09319446997880232\n",
      "Last loss: 0.03381796370349644, Avg loss: 0.08439256187467062\n",
      "Last loss: 0.08020227890572977, Avg loss: 0.08385889024106079\n",
      "Last loss: 0.05951317821321435, Avg loss: 0.08042656892695864\n",
      "Last loss: 0.062413029042541285, Avg loss: 0.07806458528491597\n",
      "Last loss: 0.10119876455711785, Avg loss: 0.07784166692982247\n",
      "Last loss: 0.06026151978393109, Avg loss: 0.0739506561480392\n",
      "Last loss: 0.168490183225785, Avg loss: 0.07652906789396206\n",
      "Last loss: 0.16567675014681227, Avg loss: 0.07452535635678\n",
      "Last loss: 0.07833769363999413, Avg loss: 0.07009461015504313\n",
      "Last loss: 0.12634758477676616, Avg loss: 0.0697582658846784\n",
      "Last loss: 0.043950428314086035, Avg loss: 0.06557928899957072\n",
      "Last loss: 0.05586587550135427, Avg loss: 0.06411747417672443\n",
      "Last loss: 0.013322275777373067, Avg loss: 0.06129788868886023\n",
      "Last loss: 0.10513248168997513, Avg loss: 0.06301366619133691\n",
      "Last loss: 0.08780440415045183, Avg loss: 0.06101594017869574\n",
      "Last loss: 0.1466337573439843, Avg loss: 0.06300338814615167\n",
      "Last loss: 0.12627930464042436, Avg loss: 0.06028249785657788\n",
      "Last loss: 0.02003088746960418, Avg loss: 0.055246712171625245\n",
      "Last loss: 0.04341670224426354, Avg loss: 0.055383260149728046\n",
      "Last loss: 0.04227014003331071, Avg loss: 0.053846206940718164\n",
      "Last loss: 0.027236058524694864, Avg loss: 0.053159646446936305\n",
      "Last loss: 0.06712118276207857, Avg loss: 0.0535888721480452\n",
      "Last loss: 0.047445934614295485, Avg loss: 0.052004155029753016\n",
      "Last loss: 0.03156861487627112, Avg loss: 0.050398305500888425\n",
      "Last loss: 0.016876840128800766, Avg loss: 0.04939499090288756\n",
      "Last loss: 0.028731355303823165, Avg loss: 0.04919554071427205\n",
      "Last loss: 0.09827013445113493, Avg loss: 0.05054661434407799\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 295.67it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 321.53it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.07444216162271612, Avg loss: 0.04914801485335911\n",
      "Last loss: 0.023251758300209466, Avg loss: 0.047207310964373535\n",
      "Last loss: 0.01919401570013518, Avg loss: 0.04589017417938745\n",
      "Last loss: 0.09093308438192882, Avg loss: 0.04845533926950712\n",
      "Last loss: 0.4688675651426949, Avg loss: 2.9541757975366303\n",
      "Last loss: 0.2950915592563462, Avg loss: 0.3505664172147072\n",
      "Last loss: 0.05211311346486725, Avg loss: 0.3334050502010343\n",
      "Last loss: 0.36956635032384266, Avg loss: 0.30863336527331486\n",
      "Last loss: 0.09498853924728357, Avg loss: 0.2899627014050404\n",
      "Last loss: 0.058151242976101386, Avg loss: 0.2820851528887522\n",
      "Last loss: 0.16035252602397587, Avg loss: 0.28136712499280836\n",
      "Last loss: 0.368965258639903, Avg loss: 0.2813842711743113\n",
      "Last loss: 0.04693299001625737, Avg loss: 0.2665025928976727\n",
      "Last loss: 0.09327071492574526, Avg loss: 0.25604482515180077\n",
      "Last loss: 0.5576668545322443, Avg loss: 0.26454545925529704\n",
      "Last loss: 0.04138315579943132, Avg loss: 0.2433457455788504\n",
      "Last loss: 0.03343726843198761, Avg loss: 0.23713470757139074\n",
      "Last loss: 0.19750605275246114, Avg loss: 0.24200396134782962\n",
      "Last loss: 0.07356411597223028, Avg loss: 0.224289335432233\n",
      "Last loss: 0.1752720577334975, Avg loss: 0.2250110372428923\n",
      "Last loss: 0.14525720618119067, Avg loss: 0.21151414339074448\n",
      "Last loss: 0.3158721694064962, Avg loss: 0.21879800907714003\n",
      "Last loss: 0.19208138525078616, Avg loss: 0.22568441083790455\n",
      "Last loss: 0.20467434112839936, Avg loss: 0.21972824001450247\n",
      "Last loss: 0.18758970172680395, Avg loss: 0.20527841542575959\n",
      "Last loss: 0.27651120341022284, Avg loss: 0.21197564855277143\n",
      "Last loss: 0.2292253661421793, Avg loss: 0.20271313758756276\n",
      "Last loss: 0.36367673124653466, Avg loss: 0.21322219888122143\n",
      "Last loss: 0.15246354605607346, Avg loss: 0.19622252975644022\n",
      "Last loss: 0.1694301153488568, Avg loss: 0.18189488866658096\n",
      "Last loss: 0.4491986101938541, Avg loss: 0.20503789352894478\n",
      "Last loss: 0.1275534049854779, Avg loss: 0.18391490770145122\n",
      "Last loss: 0.05897960066934245, Avg loss: 0.1829520738532625\n",
      "Last loss: 0.07291446632992955, Avg loss: 0.17808908609753643\n",
      "Last loss: 0.23671774180517569, Avg loss: 0.19007356532171832\n",
      "Last loss: 0.2863859669170981, Avg loss: 0.18628209478796057\n",
      "Last loss: 0.5926880743245915, Avg loss: 0.19320366960194427\n",
      "Last loss: 0.09973660373839843, Avg loss: 0.16851312749433228\n",
      "Last loss: 0.015967657815551903, Avg loss: 0.1629281866412654\n",
      "Last loss: 0.32560263992713356, Avg loss: 0.175652356477246\n",
      "Last loss: 0.19509661188625685, Avg loss: 0.1742008815746222\n",
      "Last loss: 0.2236020383056434, Avg loss: 0.1719189837310202\n",
      "Last loss: 0.035098818792935645, Avg loss: 0.15768199342008982\n",
      "Last loss: 0.11541105348963969, Avg loss: 0.15432757531791236\n",
      "Last loss: 1.229822300229673, Avg loss: 7.672554288297994\n",
      "Last loss: 1.4948456377307124, Avg loss: 1.2480864266780043\n",
      "Last loss: 0.41003304579530764, Avg loss: 0.7347848897939533\n",
      "Last loss: 0.5091648970567106, Avg loss: 0.5615219619846278\n",
      "Last loss: 0.40629298582384576, Avg loss: 0.4648574283131977\n",
      "Last loss: 0.41843945825647644, Avg loss: 0.41547510841515084\n",
      "Last loss: 0.16166152363971692, Avg loss: 0.3630142817528995\n",
      "Last loss: 0.09490868575998271, Avg loss: 0.3247500219424812\n",
      "Last loss: 0.6486648973325599, Avg loss: 0.31029048871086834\n",
      "Last loss: 0.25914479821055303, Avg loss: 0.26766518264069405\n",
      "Last loss: 0.08593639581491946, Avg loss: 0.23067113269815745\n",
      "Last loss: 0.05062050662034198, Avg loss: 0.2157549718340035\n",
      "Last loss: 0.07366724552638695, Avg loss: 0.1975386251499796\n",
      "Last loss: 0.11903621495209407, Avg loss: 0.18650560850705192\n",
      "Last loss: 0.21297050842433962, Avg loss: 0.18064006072814764\n",
      "Last loss: 0.08757089510943997, Avg loss: 0.16590982723036182\n",
      "Last loss: 0.21487726252716924, Avg loss: 0.16350448548004742\n",
      "Last loss: 0.3523312768697432, Avg loss: 0.15974010564945001\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 282.69it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.13553852662967503, Avg loss: 0.14486340277006418\n",
      "Last loss: 0.12201282987087791, Avg loss: 0.13632123380944575\n",
      "Last loss: 0.07150378383569064, Avg loss: 0.1295502042455255\n",
      "Last loss: 0.08886523636879905, Avg loss: 0.12411836156356128\n",
      "Last loss: 0.0789348856864536, Avg loss: 0.1206180809319838\n",
      "Last loss: 0.12028584575154562, Avg loss: 0.11837790340081776\n",
      "Last loss: 0.15001065312741435, Avg loss: 0.1185029459501652\n",
      "Last loss: 0.1305595393729746, Avg loss: 0.11130865763777426\n",
      "Last loss: 0.19997630629427968, Avg loss: 0.10991122172286119\n",
      "Last loss: 0.22931261327187408, Avg loss: 0.11078234450159244\n",
      "Last loss: 0.1099842243133944, Avg loss: 0.10097751280288864\n",
      "Last loss: 0.1783082496354606, Avg loss: 0.10251570208339482\n",
      "Last loss: 0.3325016466247682, Avg loss: 0.10494234379393588\n",
      "Last loss: 0.12049342360636836, Avg loss: 0.09361913782850645\n",
      "Last loss: 0.1752196032151905, Avg loss: 0.09446495109843608\n",
      "Last loss: 0.12141720575963191, Avg loss: 0.08978224031096706\n",
      "Last loss: 0.1890220466111354, Avg loss: 0.09075133939257185\n",
      "Last loss: 0.07659687547542846, Avg loss: 0.08419862720608025\n",
      "Last loss: 0.06336750409789063, Avg loss: 0.08145627884061821\n",
      "Last loss: 0.09215199814921554, Avg loss: 0.08105670179316936\n",
      "Last loss: 0.02900684702511453, Avg loss: 0.07588613636036555\n",
      "Last loss: 0.04469961078407693, Avg loss: 0.0759091747882754\n",
      "Accuracy 1.0\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MMDJM4qFQuvT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1610650443415,
     "user_tz": -180,
     "elapsed": 3774,
     "user": {
      "displayName": "Iaroslav Sokolov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHt_w9-U1o8Sy_kutVo5pzmBrbbw0dWtxW9hnv=s64",
      "userId": "04885912710307977692"
     }
    },
    "outputId": "bf71424d-ec40-4606-ef33-22a7a84b5561"
   },
   "source": [
    "X, y = make_blobs(400, 2, centers=[[0, 0], [2.5, 2.5], [-2.5, 3]])\n",
    "X_test, y_test = make_blobs(400, 2, centers=[[0, 0], [2.5, 2.5], [-2.5, 3]])\n",
    "best_acc = 0\n",
    "for _ in range(25):\n",
    "    p = MLPClassifier([\n",
    "        Linear(2, 16),\n",
    "        ReLU(),\n",
    "        Linear(16, 16),\n",
    "        ReLU(),\n",
    "        Linear(16, 3)\n",
    "    ])\n",
    "\n",
    "    p.fit(X, y)\n",
    "    best_acc = max(np.mean(p.predict(X_test) == y_test), best_acc)\n",
    "print(\"Accuracy\", best_acc)"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 305.39it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 1.4258586345473887, Avg loss: 4.199172416204117\n",
      "Last loss: 0.4937068515456965, Avg loss: 0.7145994095005276\n",
      "Last loss: 0.22536989912494773, Avg loss: 0.47824637887467386\n",
      "Last loss: 0.4768859029245679, Avg loss: 0.39460238750982896\n",
      "Last loss: 0.211007588775706, Avg loss: 0.3313163537808286\n",
      "Last loss: 0.31528981648117466, Avg loss: 0.2972181163634676\n",
      "Last loss: 0.2453218660051566, Avg loss: 0.2655138105363038\n",
      "Last loss: 0.4886782184534984, Avg loss: 0.2580311320167462\n",
      "Last loss: 0.19976040323354072, Avg loss: 0.23195507756737557\n",
      "Last loss: 0.1901925769996477, Avg loss: 0.22562911505672925\n",
      "Last loss: 0.5035200176224813, Avg loss: 0.2193085213221553\n",
      "Last loss: 0.11587863851838417, Avg loss: 0.19308153889952706\n",
      "Last loss: 0.15400566008761576, Avg loss: 0.19153446702882765\n",
      "Last loss: 0.08245708207904108, Avg loss: 0.18632613993571703\n",
      "Last loss: 0.20923064407745323, Avg loss: 0.1797210435569146\n",
      "Last loss: 0.01862406353997157, Avg loss: 0.16868209421931274\n",
      "Last loss: 0.28699546750097504, Avg loss: 0.17406586565050844\n",
      "Last loss: 0.07921676345694406, Avg loss: 0.16236791206450893\n",
      "Last loss: 0.026590434143316438, Avg loss: 0.15903089233713733\n",
      "Last loss: 0.09797318398694602, Avg loss: 0.15695870132582299\n",
      "Last loss: 0.18752878310428117, Avg loss: 0.16131776569006157\n",
      "Last loss: 0.17004230579675322, Avg loss: 0.15378822907564535\n",
      "Last loss: 0.017842452232232738, Avg loss: 0.1476540211427202\n",
      "Last loss: 0.24062299334387277, Avg loss: 0.15271285493076595\n",
      "Last loss: 0.01794953068827114, Avg loss: 0.14241315335430693\n",
      "Last loss: 0.5240570742440147, Avg loss: 0.1582913313027144\n",
      "Last loss: 0.16881949949894093, Avg loss: 0.14236009317784412\n",
      "Last loss: 0.2583192969476874, Avg loss: 0.14656348353489776\n",
      "Last loss: 0.1834493991015566, Avg loss: 0.14110767826761916\n",
      "Last loss: 0.0406688508526088, Avg loss: 0.1338881704613433\n",
      "Last loss: 0.08745430462148587, Avg loss: 0.1322603728504453\n",
      "Last loss: 0.12002614769302564, Avg loss: 0.13278234197136846\n",
      "Last loss: 0.07029095496802862, Avg loss: 0.13175412745615728\n",
      "Last loss: 0.27468764507776033, Avg loss: 0.1369982442036789\n",
      "Last loss: 0.054695982938308366, Avg loss: 0.1301870662508184\n",
      "Last loss: 0.2682223110743735, Avg loss: 0.13219231425833886\n",
      "Last loss: 0.040530467652402626, Avg loss: 0.12708890596580577\n",
      "Last loss: 0.10015570465411879, Avg loss: 0.12973010474368826\n",
      "Last loss: 0.5558290211658885, Avg loss: 0.14557635975313887\n",
      "Last loss: 0.008792285905041743, Avg loss: 0.12058384019507623\n",
      "Last loss: 0.7353812952540676, Avg loss: 3.1811863250009838\n",
      "Last loss: 0.6267192426855792, Avg loss: 0.678355503827392\n",
      "Last loss: 0.10832594843762552, Avg loss: 0.3690241379606312\n",
      "Last loss: 0.15335283936093977, Avg loss: 0.288315958271232\n",
      "Last loss: 0.22031858196932602, Avg loss: 0.25438080496333965\n",
      "Last loss: 0.036927162566066395, Avg loss: 0.21898442450235975\n",
      "Last loss: 0.3329017625535173, Avg loss: 0.21845113247628842\n",
      "Last loss: 0.7745929228617344, Avg loss: 0.21347741482156612\n",
      "Last loss: 0.0892166711951884, Avg loss: 0.1780915100083051\n",
      "Last loss: 0.6041455437125276, Avg loss: 0.1941755002367785\n",
      "Last loss: 0.12471789620247559, Avg loss: 0.16733614977769268\n",
      "Last loss: 0.12708240489250508, Avg loss: 0.16283885199231513\n",
      "Last loss: 0.4435164101714155, Avg loss: 0.17351021385319632\n",
      "Last loss: 0.03942023835235953, Avg loss: 0.15621793979424153\n",
      "Last loss: 0.03583423943725949, Avg loss: 0.15588575750700606\n",
      "Last loss: 0.026122354977224232, Avg loss: 0.15435823554841985\n",
      "Last loss: 0.04722919177384624, Avg loss: 0.15485039226543568\n",
      "Last loss: 0.4434005011832402, Avg loss: 0.16319553721418215\n",
      "Last loss: 0.04111347943508041, Avg loss: 0.15254460460237304\n",
      "Last loss: 0.04062099068318101, Avg loss: 0.149975803295953\n",
      "Last loss: 0.4742492271221007, Avg loss: 0.1646734645891696\n",
      "Last loss: 0.05799459301004577, Avg loss: 0.1473164950520924\n",
      "Last loss: 0.1675854050545588, Avg loss: 0.15249344700719664\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 307.86it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 287.61it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.34743552655635823, Avg loss: 0.15715708091707872\n",
      "Last loss: 0.03622857745870445, Avg loss: 0.14832558948603303\n",
      "Last loss: 0.10084590213857084, Avg loss: 0.14567346879591878\n",
      "Last loss: 0.03522826300266749, Avg loss: 0.1464915413831928\n",
      "Last loss: 0.04806785978873061, Avg loss: 0.1426587068215573\n",
      "Last loss: 0.05293537837020743, Avg loss: 0.1430415916199832\n",
      "Last loss: 0.014656861360768887, Avg loss: 0.13829022407050184\n",
      "Last loss: 0.4533244399968783, Avg loss: 0.15535212007316546\n",
      "Last loss: 0.10355249383440035, Avg loss: 0.1432419606499586\n",
      "Last loss: 0.3287281491793057, Avg loss: 0.14898959721612748\n",
      "Last loss: 0.28597310582365787, Avg loss: 0.15126831620429051\n",
      "Last loss: 0.1960130779935297, Avg loss: 0.1455087933862497\n",
      "Last loss: 0.3243174348383479, Avg loss: 0.14710155542263068\n",
      "Last loss: 0.03170558481952727, Avg loss: 0.14011828989366168\n",
      "Last loss: 0.026472650159209535, Avg loss: 0.13681680042675592\n",
      "Last loss: 0.11505844588115863, Avg loss: 0.14017996010032172\n",
      "Last loss: 0.08252892755109874, Avg loss: 0.13746843736062972\n",
      "Last loss: 2.2526471342361596, Avg loss: 9.197321720098921\n",
      "Last loss: 0.4794525638056507, Avg loss: 1.5172469715476238\n",
      "Last loss: 0.6958412949598375, Avg loss: 0.6388094654611611\n",
      "Last loss: 0.4736174600717985, Avg loss: 0.4248554845810743\n",
      "Last loss: 0.9178118259098267, Avg loss: 0.35879757737035195\n",
      "Last loss: 0.702398632834826, Avg loss: 0.3022696485206816\n",
      "Last loss: 0.20002719258875995, Avg loss: 0.2574329101194983\n",
      "Last loss: 0.183199797635189, Avg loss: 0.24080034972851838\n",
      "Last loss: 0.16970964923964932, Avg loss: 0.22109414373117398\n",
      "Last loss: 0.17795555171770705, Avg loss: 0.21207008960672727\n",
      "Last loss: 0.07082074971621556, Avg loss: 0.2013675699543439\n",
      "Last loss: 0.02875972388690449, Avg loss: 0.19480603798038404\n",
      "Last loss: 0.03401274599220232, Avg loss: 0.1890377520151234\n",
      "Last loss: 0.5638071445311621, Avg loss: 0.20426274570831035\n",
      "Last loss: 0.07382450747211795, Avg loss: 0.18260336874554894\n",
      "Last loss: 0.09410509013041464, Avg loss: 0.17607218566957508\n",
      "Last loss: 0.6953245997363853, Avg loss: 0.19658677911288078\n",
      "Last loss: 0.1541165498153793, Avg loss: 0.1720192486809312\n",
      "Last loss: 0.49784358606092133, Avg loss: 0.18338684026851151\n",
      "Last loss: 0.012240985447364645, Avg loss: 0.16212450810383228\n",
      "Last loss: 0.20949091825582963, Avg loss: 0.16563483203008159\n",
      "Last loss: 0.1403132321228513, Avg loss: 0.16133651363566232\n",
      "Last loss: 0.12465675465603333, Avg loss: 0.15918629764712014\n",
      "Last loss: 0.16910720716520478, Avg loss: 0.1601948048947337\n",
      "Last loss: 0.337782972794314, Avg loss: 0.16582995682292132\n",
      "Last loss: 0.032520846342028356, Avg loss: 0.15268565555158112\n",
      "Last loss: 0.04729933115198135, Avg loss: 0.14998830478912578\n",
      "Last loss: 0.2869151182799363, Avg loss: 0.1581295504867371\n",
      "Last loss: 0.04758321661691074, Avg loss: 0.14906129435169344\n",
      "Last loss: 0.17468352836000348, Avg loss: 0.1514664688200909\n",
      "Last loss: 0.11915571374138953, Avg loss: 0.14834153438845477\n",
      "Last loss: 0.028487615841650327, Avg loss: 0.14368087403506224\n",
      "Last loss: 0.01630608733654179, Avg loss: 0.14287203164746898\n",
      "Last loss: 0.19257923953470657, Avg loss: 0.14894111933229565\n",
      "Last loss: 0.061673334127824095, Avg loss: 0.14341512508078916\n",
      "Last loss: 0.08200952727841326, Avg loss: 0.14201079842492528\n",
      "Last loss: 0.03699504469990285, Avg loss: 0.14166475103387197\n",
      "Last loss: 0.04020149844707841, Avg loss: 0.14012915425622532\n",
      "Last loss: 0.0394417017856773, Avg loss: 0.1399461534621812\n",
      "Last loss: 0.040536141427448956, Avg loss: 0.1380987799458076\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 291.28it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 1.010995957245423, Avg loss: 4.115476311275398\n",
      "Last loss: 0.6774031933805637, Avg loss: 0.8068833316099131\n",
      "Last loss: 0.02517245679909668, Avg loss: 0.4798646698046344\n",
      "Last loss: 0.02622739457013795, Avg loss: 0.37840406178107855\n",
      "Last loss: 0.0019210652581593416, Avg loss: 0.4173999418029367\n",
      "Last loss: 0.26901905050079306, Avg loss: 0.45168557778585455\n",
      "Last loss: 0.36432407986248205, Avg loss: 0.36313233674445233\n",
      "Last loss: 1.0461873030188675, Avg loss: 0.42531114180252605\n",
      "Last loss: 0.27996912728339923, Avg loss: 0.38056773241545866\n",
      "Last loss: 0.00782928842073296, Avg loss: 0.2932181457982574\n",
      "Last loss: 2.0254209876995177, Avg loss: 0.4072673561789959\n",
      "Last loss: 1.6427835983782384, Avg loss: 0.4187911410776325\n",
      "Last loss: 0.061837325191887255, Avg loss: 0.3269481199592974\n",
      "Last loss: 0.4343265714676744, Avg loss: 0.32742027043034266\n",
      "Last loss: 0.0008364275444322548, Avg loss: 0.27369885628136453\n",
      "Last loss: 0.2449505989600762, Avg loss: 0.32057816702636593\n",
      "Last loss: 0.029368172228218553, Avg loss: 0.2554004972839428\n",
      "Last loss: 0.0717747836796649, Avg loss: 0.27092356969794684\n",
      "Last loss: 0.24168198393140553, Avg loss: 0.27276064145953255\n",
      "Last loss: 0.06402087479346817, Avg loss: 0.2812798302095879\n",
      "Last loss: 0.3452985443421981, Avg loss: 0.27087397283610515\n",
      "Last loss: 0.436046227864283, Avg loss: 0.2748709542656951\n",
      "Last loss: 0.0050608423598691235, Avg loss: 0.23847695158546575\n",
      "Last loss: 0.2242739533923568, Avg loss: 0.24413743618697423\n",
      "Last loss: 0.7706478096077567, Avg loss: 0.29802546761732984\n",
      "Last loss: 0.02196562677160841, Avg loss: 0.2542523144401038\n",
      "Last loss: 0.39685638890782454, Avg loss: 0.23068120332046138\n",
      "Last loss: 0.662377060922348, Avg loss: 0.3787605416859996\n",
      "Last loss: 0.0007382969883045421, Avg loss: 0.29019773122993653\n",
      "Last loss: 0.010029791861157317, Avg loss: 0.24705439514257346\n",
      "Last loss: 0.7191625137434747, Avg loss: 0.22574424404175022\n",
      "Last loss: 0.03942653911312888, Avg loss: 0.21501385570074374\n",
      "Last loss: 0.001176983084435021, Avg loss: 0.20714094486498896\n",
      "Last loss: 0.7293643404484875, Avg loss: 0.22499719291038378\n",
      "Last loss: 0.1487167186092644, Avg loss: 0.2226631516850799\n",
      "Last loss: 0.6229776853718945, Avg loss: 0.2587966217912492\n",
      "Last loss: 0.08618858422582097, Avg loss: 0.1833913718413884\n",
      "Last loss: 0.015796745650775164, Avg loss: 0.21767985728328412\n",
      "Last loss: 1.1027551485341192, Avg loss: 0.271737538523432\n",
      "Last loss: 0.230576231955523, Avg loss: 0.18636491437585018\n",
      "Last loss: 4.8295992802218795, Avg loss: 6.703053294669919\n",
      "Last loss: 6.062877746723343, Avg loss: 3.373120958666729\n",
      "Last loss: 1.5510239238175552, Avg loss: 1.8666587427181698\n",
      "Last loss: 0.6447146363341486, Avg loss: 1.0329655629320043\n",
      "Last loss: 0.02415665319627121, Avg loss: 0.6632491168139949\n",
      "Last loss: 0.018885682395617227, Avg loss: 0.5545128432033514\n",
      "Last loss: 0.3222923335523258, Avg loss: 0.511675809007075\n",
      "Last loss: 0.14623966692000265, Avg loss: 0.45139774430058127\n",
      "Last loss: 0.7694516457985849, Avg loss: 0.4444426859235267\n",
      "Last loss: 0.08108098195504009, Avg loss: 0.38593337909460207\n",
      "Last loss: 0.5313759148946485, Avg loss: 0.36993372190506013\n",
      "Last loss: 0.1369353547316191, Avg loss: 0.33771307841304665\n",
      "Last loss: 0.5280423528120505, Avg loss: 0.33686584687046817\n",
      "Last loss: 0.6451540689356283, Avg loss: 0.3230655413973626\n",
      "Last loss: 0.007204120915992843, Avg loss: 0.2866840159805266\n",
      "Last loss: 0.026404804827530954, Avg loss: 0.2749676616525597\n",
      "Last loss: 0.5763431490991673, Avg loss: 0.28200800829172173\n",
      "Last loss: 0.1191575446049886, Avg loss: 0.25763869316013316\n",
      "Last loss: 0.08807648000793158, Avg loss: 0.24889753400780254\n",
      "Last loss: 0.009041195122923713, Avg loss: 0.23891279305318625\n",
      "Last loss: 0.34174575141855557, Avg loss: 0.24327027340581472\n",
      "Last loss: 0.00260896860462218, Avg loss: 0.22756112981714485\n",
      "Last loss: 0.1235569778509901, Avg loss: 0.22722528615255344\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 313.28it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 318.52it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.6740530064893158, Avg loss: 0.2421308143525018\n",
      "Last loss: 0.09388121995840654, Avg loss: 0.21620522562257133\n",
      "Last loss: 0.0823790716967389, Avg loss: 0.21157374368117676\n",
      "Last loss: 0.04685957843381558, Avg loss: 0.20034061536273168\n",
      "Last loss: 0.005232857291551902, Avg loss: 0.19808431070184188\n",
      "Last loss: 0.2406082977962119, Avg loss: 0.2017512324514271\n",
      "Last loss: 0.5026559930254263, Avg loss: 0.21024235276585113\n",
      "Last loss: 0.5253459457862704, Avg loss: 0.20688310514358366\n",
      "Last loss: 0.15409818768864164, Avg loss: 0.1939951259997565\n",
      "Last loss: 0.3020950463307591, Avg loss: 0.19243533505668936\n",
      "Last loss: 0.04726022729175412, Avg loss: 0.17788367484700926\n",
      "Last loss: 0.1298110745498128, Avg loss: 0.1824730517690298\n",
      "Last loss: 0.02566934981113579, Avg loss: 0.17258511155090622\n",
      "Last loss: 0.45643781901341235, Avg loss: 0.18632035277583642\n",
      "Last loss: 0.060337318735823604, Avg loss: 0.17395932583829649\n",
      "Last loss: 0.38994295005294094, Avg loss: 0.18073704699877868\n",
      "Last loss: 0.1382124038688294, Avg loss: 0.16906490089039702\n",
      "Last loss: 2.8807416448253043, Avg loss: 5.5147014090534165\n",
      "Last loss: 1.0529695071178602, Avg loss: 0.8036636540927622\n",
      "Last loss: 0.5789135182122722, Avg loss: 0.4096448878965742\n",
      "Last loss: 0.11338458922184597, Avg loss: 0.316050521937857\n",
      "Last loss: 0.062484592181930546, Avg loss: 0.28400251294935347\n",
      "Last loss: 0.25359649563421327, Avg loss: 0.2580608520022892\n",
      "Last loss: 0.07616691383718587, Avg loss: 0.2517758695501573\n",
      "Last loss: 0.12022663553358971, Avg loss: 0.23959228265577187\n",
      "Last loss: 0.5334189133226899, Avg loss: 0.246814701276122\n",
      "Last loss: 0.4215422296579089, Avg loss: 0.23873526869755674\n",
      "Last loss: 0.7751012996785039, Avg loss: 0.2420829680033356\n",
      "Last loss: 0.2589708910299987, Avg loss: 0.22274463159188068\n",
      "Last loss: 0.0793747401449593, Avg loss: 0.2005576203806233\n",
      "Last loss: 0.5693368155106892, Avg loss: 0.21741962814650873\n",
      "Last loss: 0.04322762059993235, Avg loss: 0.1886815214619183\n",
      "Last loss: 0.20013932754646385, Avg loss: 0.1894980024594289\n",
      "Last loss: 0.0441810087299074, Avg loss: 0.1806480953918904\n",
      "Last loss: 0.1922871343123949, Avg loss: 0.18279076909292022\n",
      "Last loss: 0.3725821855561634, Avg loss: 0.19394650182094278\n",
      "Last loss: 0.1442795145531017, Avg loss: 0.17122641865947172\n",
      "Last loss: 0.09410685178399775, Avg loss: 0.16286359069990033\n",
      "Last loss: 0.24646713309592838, Avg loss: 0.1687915494001797\n",
      "Last loss: 0.06696800298902605, Avg loss: 0.16417867958801463\n",
      "Last loss: 0.39537273437658504, Avg loss: 0.1761877227593091\n",
      "Last loss: 0.2106732629414026, Avg loss: 0.16322368677234766\n",
      "Last loss: 0.20641249339982032, Avg loss: 0.16185558591724292\n",
      "Last loss: 0.4151173192088845, Avg loss: 0.16668793067537738\n",
      "Last loss: 0.2703398745563632, Avg loss: 0.15768161976307843\n",
      "Last loss: 0.21053466769303397, Avg loss: 0.1508148115873685\n",
      "Last loss: 0.3110907704150597, Avg loss: 0.1544541586672366\n",
      "Last loss: 0.015761797730231102, Avg loss: 0.1301531492615804\n",
      "Last loss: 0.12192500716296974, Avg loss: 0.1431523473992126\n",
      "Last loss: 0.17019944395339412, Avg loss: 0.14153578900315636\n",
      "Last loss: 0.2195746203120914, Avg loss: 0.1452632545377273\n",
      "Last loss: 0.5801755998981226, Avg loss: 0.15219605021115779\n",
      "Last loss: 0.1374256113448839, Avg loss: 0.14579413253843773\n",
      "Last loss: 0.005480474917785504, Avg loss: 0.13220262286139065\n",
      "Last loss: 0.12641127646643432, Avg loss: 0.13446256323889821\n",
      "Last loss: 0.0016833122160918625, Avg loss: 0.13074087966714892\n",
      "Last loss: 0.06785865789807484, Avg loss: 0.12760293739669387\n",
      "Last loss: 4.709862011103331, Avg loss: 6.080109065658882\n",
      "Last loss: 1.0811199758324495, Avg loss: 2.986251207172932\n",
      "Last loss: 0.7429592880152047, Avg loss: 1.5874615830510115\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 314.32it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 1.3705119447154277, Avg loss: 0.9636774901130114\n",
      "Last loss: 0.9641745767740929, Avg loss: 0.6414226048004192\n",
      "Last loss: 1.372308887817666, Avg loss: 0.5194452556136452\n",
      "Last loss: 0.45383411919954353, Avg loss: 0.39280996963059855\n",
      "Last loss: 0.08912617149530865, Avg loss: 0.32679574855575694\n",
      "Last loss: 0.8587619480983361, Avg loss: 0.31527859546087483\n",
      "Last loss: 0.20028077915514075, Avg loss: 0.289738267583783\n",
      "Last loss: 0.03876076524335305, Avg loss: 0.24025621277661577\n",
      "Last loss: 0.5726150990294839, Avg loss: 0.24061604830286185\n",
      "Last loss: 0.11402641896895103, Avg loss: 0.21072522540238248\n",
      "Last loss: 0.7865137799294273, Avg loss: 0.22485765056499163\n",
      "Last loss: 0.3432002664410537, Avg loss: 0.20769218174084397\n",
      "Last loss: 0.10112667003918434, Avg loss: 0.18878506595418465\n",
      "Last loss: 0.4132753984884844, Avg loss: 0.19126688330655756\n",
      "Last loss: 0.006833238086852955, Avg loss: 0.16888905315932395\n",
      "Last loss: 0.7091209323538421, Avg loss: 0.19617334326378738\n",
      "Last loss: 0.04187247659984829, Avg loss: 0.16873022016895708\n",
      "Last loss: 0.15587326788918862, Avg loss: 0.1701000330371699\n",
      "Last loss: 0.04043151831439289, Avg loss: 0.16339838663374032\n",
      "Last loss: 0.09821649105104592, Avg loss: 0.1610868939073588\n",
      "Last loss: 0.031080835471713364, Avg loss: 0.1616350810146064\n",
      "Last loss: 0.21423759045387547, Avg loss: 0.1723107661931755\n",
      "Last loss: 0.5608943505580508, Avg loss: 0.1757086203125279\n",
      "Last loss: 0.01199287393848854, Avg loss: 0.15545477365285812\n",
      "Last loss: 0.04041141332133759, Avg loss: 0.1533229808058572\n",
      "Last loss: 0.4144381275603472, Avg loss: 0.16854553554329693\n",
      "Last loss: 0.1436261073009419, Avg loss: 0.15416153495259566\n",
      "Last loss: 0.3216171500756766, Avg loss: 0.16169389818338256\n",
      "Last loss: 0.5360230385079694, Avg loss: 0.16604456214384256\n",
      "Last loss: 0.12029759425759906, Avg loss: 0.1512531167599169\n",
      "Last loss: 0.356666445583246, Avg loss: 0.16135126772957298\n",
      "Last loss: 0.009421484892226257, Avg loss: 0.14626422890626906\n",
      "Last loss: 0.12324645192488828, Avg loss: 0.143876242958883\n",
      "Last loss: 0.30852600923265994, Avg loss: 0.15554821554143777\n",
      "Last loss: 0.23679766070810923, Avg loss: 0.1505415445523548\n",
      "Last loss: 0.01175536019375372, Avg loss: 0.1449790255207342\n",
      "Last loss: 0.3085082120705971, Avg loss: 0.15234136894401898\n",
      "Last loss: 2.0220246963901616, Avg loss: 4.965729238452829\n",
      "Last loss: 0.1276781010591842, Avg loss: 1.1663414333746132\n",
      "Last loss: 0.15701049075950296, Avg loss: 0.6375702189904392\n",
      "Last loss: 0.47061618229406593, Avg loss: 0.4087252982965651\n",
      "Last loss: 0.22360229924894076, Avg loss: 0.28526221281603525\n",
      "Last loss: 0.692263414164425, Avg loss: 0.26437895398067734\n",
      "Last loss: 0.11418608358862256, Avg loss: 0.22279086887319866\n",
      "Last loss: 0.12523744848745444, Avg loss: 0.20172490659173964\n",
      "Last loss: 0.07189765030159591, Avg loss: 0.19328481499926445\n",
      "Last loss: 0.5909993061318075, Avg loss: 0.19797171496519234\n",
      "Last loss: 0.017565474382549216, Avg loss: 0.17816717951693045\n",
      "Last loss: 0.5981012505702181, Avg loss: 0.1878861753043479\n",
      "Last loss: 0.04134118610235594, Avg loss: 0.15889483925919404\n",
      "Last loss: 0.02017919286660467, Avg loss: 0.1708342248259377\n",
      "Last loss: 0.08257305724109945, Avg loss: 0.15845303837157124\n",
      "Last loss: 0.46769544854358747, Avg loss: 0.17760299270362725\n",
      "Last loss: 0.0018547472958870136, Avg loss: 0.1562720929003605\n",
      "Last loss: 0.057584937939738456, Avg loss: 0.15684635192601346\n",
      "Last loss: 0.07691254478496107, Avg loss: 0.1583474797881021\n",
      "Last loss: 0.19587479100549904, Avg loss: 0.15795360671587183\n",
      "Last loss: 0.11945521466429188, Avg loss: 0.15841272347536886\n",
      "Last loss: 0.0831105034150418, Avg loss: 0.15115567405749114\n",
      "Last loss: 0.26732368156949016, Avg loss: 0.16092993472348677\n",
      "Last loss: 0.16674938943401763, Avg loss: 0.1513092667840366\n",
      "Last loss: 0.34816528165629146, Avg loss: 0.16864487321319283\n",
      "Last loss: 0.08280188634889518, Avg loss: 0.14846456941962313\n",
      "Last loss: 0.029856527098698057, Avg loss: 0.14102301241334836\n",
      "Last loss: 0.2535967180296686, Avg loss: 0.154855041640429\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 300.53it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 339.64it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.018831162876257145, Avg loss: 0.14576319398575388\n",
      "Last loss: 0.18960054958377914, Avg loss: 0.1473538989314151\n",
      "Last loss: 0.014135947248449321, Avg loss: 0.15197195693629906\n",
      "Last loss: 0.2784470489962103, Avg loss: 0.14794414760130567\n",
      "Last loss: 0.011103366673311594, Avg loss: 0.1355639002948082\n",
      "Last loss: 0.37215411699108725, Avg loss: 0.1578059498828877\n",
      "Last loss: 0.2047566304411546, Avg loss: 0.148112556895664\n",
      "Last loss: 0.028456664166511277, Avg loss: 0.1428487451365833\n",
      "Last loss: 0.20720828931469215, Avg loss: 0.13997985971952331\n",
      "Last loss: 0.16150052279062177, Avg loss: 0.1430961432071811\n",
      "Last loss: 0.029398141191051826, Avg loss: 0.14145139774356763\n",
      "Last loss: 0.19867380615719715, Avg loss: 0.14255986793281855\n",
      "Last loss: 1.6384730280943003, Avg loss: 6.303805104827948\n",
      "Last loss: 0.6254011796028235, Avg loss: 0.8055438547131062\n",
      "Last loss: 0.16216227154928986, Avg loss: 0.5969225928985072\n",
      "Last loss: 0.4700964852096722, Avg loss: 0.4509189021585114\n",
      "Last loss: 0.5933177289366646, Avg loss: 0.34307445476844234\n",
      "Last loss: 0.7241340731857704, Avg loss: 0.28801976806089313\n",
      "Last loss: 0.26315936430403036, Avg loss: 0.24341704185539675\n",
      "Last loss: 0.006070026936187892, Avg loss: 0.22508220519747704\n",
      "Last loss: 0.2688748513457091, Avg loss: 0.21572319014648844\n",
      "Last loss: 0.01783233753015184, Avg loss: 0.19736439468813494\n",
      "Last loss: 0.39543169643193227, Avg loss: 0.20189981718224084\n",
      "Last loss: 0.06889036959520613, Avg loss: 0.1917042786527812\n",
      "Last loss: 0.3260934946979407, Avg loss: 0.20394232453210523\n",
      "Last loss: 0.18871199131778535, Avg loss: 0.18268157118472983\n",
      "Last loss: 0.040358101098742646, Avg loss: 0.18126854150492586\n",
      "Last loss: 0.3214664409016036, Avg loss: 0.19523885594368473\n",
      "Last loss: 0.6180334523814844, Avg loss: 0.20093681711072356\n",
      "Last loss: 0.03634803603172511, Avg loss: 0.18547949730590665\n",
      "Last loss: 0.2917382235119103, Avg loss: 0.17461376058105643\n",
      "Last loss: 0.08764580757766074, Avg loss: 0.16809858698633767\n",
      "Last loss: 0.5105319625806283, Avg loss: 0.18577978510513882\n",
      "Last loss: 0.03223120716957524, Avg loss: 0.16052357251247484\n",
      "Last loss: 0.016687955312876132, Avg loss: 0.16818060124731898\n",
      "Last loss: 0.3556066859284058, Avg loss: 0.17768874783991684\n",
      "Last loss: 0.03710248472270473, Avg loss: 0.16464872147105122\n",
      "Last loss: 0.015056419347906563, Avg loss: 0.15285705203613456\n",
      "Last loss: 0.1559233879937371, Avg loss: 0.1758693985399367\n",
      "Last loss: 0.15820611580645746, Avg loss: 0.16145465954317342\n",
      "Last loss: 0.07567364411009314, Avg loss: 0.16403597239571488\n",
      "Last loss: 0.04179776997037761, Avg loss: 0.16022884276081534\n",
      "Last loss: 0.23920472357412137, Avg loss: 0.1739776742212705\n",
      "Last loss: 0.028744227863523646, Avg loss: 0.15617769652856753\n",
      "Last loss: 0.08327125508196051, Avg loss: 0.16390761167768644\n",
      "Last loss: 0.13738678086584516, Avg loss: 0.1602542600845399\n",
      "Last loss: 0.09891808236940577, Avg loss: 0.15671184217267944\n",
      "Last loss: 0.05945582319572802, Avg loss: 0.15728337759175098\n",
      "Last loss: 0.02175011026828934, Avg loss: 0.15794638061625493\n",
      "Last loss: 0.7465629393895999, Avg loss: 0.18505019454807117\n",
      "Last loss: 0.043706255994913526, Avg loss: 0.15529635067806022\n",
      "Last loss: 0.302133849912089, Avg loss: 0.16660715312172053\n",
      "Last loss: 2.6946356936586477, Avg loss: 4.082997287877884\n",
      "Last loss: 0.695597284526493, Avg loss: 1.697962250763575\n",
      "Last loss: 2.7689687304802213, Avg loss: 1.0821132687538018\n",
      "Last loss: 0.5528262943190004, Avg loss: 0.6382085713851734\n",
      "Last loss: 0.1982075258293552, Avg loss: 0.45646538561063493\n",
      "Last loss: 0.997832304867547, Avg loss: 0.3949172072691161\n",
      "Last loss: 0.3954393333019783, Avg loss: 0.3136511725672816\n",
      "Last loss: 0.07039427005860581, Avg loss: 0.2695689416391749\n",
      "Last loss: 0.0772921596624582, Avg loss: 0.2538794198888987\n",
      "Last loss: 0.07605031232177975, Avg loss: 0.2317706241315021\n",
      "Last loss: 0.039244154470745043, Avg loss: 0.21653965397193994\n",
      "Last loss: 0.08834919430472285, Avg loss: 0.20759693104057053\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 325.71it/s]\n",
      " 88%|████████▊ | 35/40 [00:00<00:00, 346.22it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.09900593586457668, Avg loss: 0.20096599647769972\n",
      "Last loss: 0.07554632133122965, Avg loss: 0.1950122317318392\n",
      "Last loss: 0.3973555150431655, Avg loss: 0.2059800095533385\n",
      "Last loss: 0.6902560870984947, Avg loss: 0.21232103166886834\n",
      "Last loss: 0.08602845855194724, Avg loss: 0.18112737176425392\n",
      "Last loss: 0.04517397284686456, Avg loss: 0.17711312282203004\n",
      "Last loss: 0.12481912505319395, Avg loss: 0.1775798989817557\n",
      "Last loss: 0.11147843222618045, Avg loss: 0.1799016112670532\n",
      "Last loss: 0.23150315766533833, Avg loss: 0.17573767732327059\n",
      "Last loss: 0.013253244742162693, Avg loss: 0.17132097433800705\n",
      "Last loss: 0.18755497669721333, Avg loss: 0.1749752334693526\n",
      "Last loss: 0.014511828613061795, Avg loss: 0.1633171617680927\n",
      "Last loss: 0.29246237432512895, Avg loss: 0.1732110317104736\n",
      "Last loss: 0.31085364890036343, Avg loss: 0.17273358527385577\n",
      "Last loss: 0.041627188222144465, Avg loss: 0.15853985074265442\n",
      "Last loss: 0.24021297658362845, Avg loss: 0.1664196111172205\n",
      "Last loss: 0.02919921471760227, Avg loss: 0.16103380009815121\n",
      "Last loss: 0.08642912932845669, Avg loss: 0.16143314744692133\n",
      "Last loss: 0.02668476120205339, Avg loss: 0.1548849087992273\n",
      "Last loss: 0.4068390963865455, Avg loss: 0.17020505494492977\n",
      "Last loss: 0.05117789853246372, Avg loss: 0.15570416211880958\n",
      "Last loss: 0.04001047577630583, Avg loss: 0.15256471225124674\n",
      "Last loss: 0.007552665089937068, Avg loss: 0.1494297193403355\n",
      "Last loss: 0.16834379665891322, Avg loss: 0.15809343816397378\n",
      "Last loss: 0.21085005200738227, Avg loss: 0.15788072798296066\n",
      "Last loss: 0.0664300962366755, Avg loss: 0.15217835218756667\n",
      "Last loss: 0.2489375978491504, Avg loss: 0.15904584062612076\n",
      "Last loss: 0.2136824937693787, Avg loss: 0.1570163446412354\n",
      "Last loss: 0.7175622660669012, Avg loss: 5.595901343714197\n",
      "Last loss: 0.06908954718161853, Avg loss: 0.43507773361951063\n",
      "Last loss: 0.25567149934895084, Avg loss: 0.34780102613266284\n",
      "Last loss: 0.276195517631774, Avg loss: 0.28598271950365156\n",
      "Last loss: 0.7597814166065953, Avg loss: 0.26695647855982063\n",
      "Last loss: 0.1940911066072527, Avg loss: 0.21353492818610637\n",
      "Last loss: 0.21928249564969673, Avg loss: 0.1957164958716229\n",
      "Last loss: 0.3086959600713909, Avg loss: 0.181072243210988\n",
      "Last loss: 0.01034929936798765, Avg loss: 0.16616835670523364\n",
      "Last loss: 0.1459335149449854, Avg loss: 0.15747650094535637\n",
      "Last loss: 0.31652664384239876, Avg loss: 0.15678123071000855\n",
      "Last loss: 0.27225464651471104, Avg loss: 0.15593613244864352\n",
      "Last loss: 0.028942147929164903, Avg loss: 0.14051157562282499\n",
      "Last loss: 0.20493521044957347, Avg loss: 0.14393338462843733\n",
      "Last loss: 0.2685080881643509, Avg loss: 0.1479982308180154\n",
      "Last loss: 0.054227919458665064, Avg loss: 0.13723735171259266\n",
      "Last loss: 0.17392796413729658, Avg loss: 0.1380190800672434\n",
      "Last loss: 0.1306545005794169, Avg loss: 0.13151139958523228\n",
      "Last loss: 0.021084317715124676, Avg loss: 0.12962644857163905\n",
      "Last loss: 0.005357365152030533, Avg loss: 0.1236178108441812\n",
      "Last loss: 0.34022937673875614, Avg loss: 0.14002293723497805\n",
      "Last loss: 0.16025658614444277, Avg loss: 0.1305880617913922\n",
      "Last loss: 0.031103882130156787, Avg loss: 0.12730437860568228\n",
      "Last loss: 0.12237509114559164, Avg loss: 0.12633153497884994\n",
      "Last loss: 0.09694580930995093, Avg loss: 0.12584021381268656\n",
      "Last loss: 0.02422775899317708, Avg loss: 0.12323505170765428\n",
      "Last loss: 0.013816005908434811, Avg loss: 0.12615920142680379\n",
      "Last loss: 0.17978458620975044, Avg loss: 0.12725305308893312\n",
      "Last loss: 0.17776759455833738, Avg loss: 0.12731937514527172\n",
      "Last loss: 0.020486946041610343, Avg loss: 0.11948417308872009\n",
      "Last loss: 0.10453315330895968, Avg loss: 0.1263009429200384\n",
      "Last loss: 0.029514184336887152, Avg loss: 0.11858999043790813\n",
      "Last loss: 0.01125125470662845, Avg loss: 0.12071618324338532\n",
      "Last loss: 0.23452490528004818, Avg loss: 0.1275376525946004\n",
      "Last loss: 0.3526388887651591, Avg loss: 0.13285089773421305\n",
      "Last loss: 0.39175151618621096, Avg loss: 0.13323416204426802\n",
      "Last loss: 0.1416526151098317, Avg loss: 0.12338299646973461\n",
      "Last loss: 0.07196617024521194, Avg loss: 0.11971637882862686\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 328.05it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 285.71it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.24889106135078282, Avg loss: 0.12685139356991482\n",
      "Last loss: 0.14574246052826123, Avg loss: 0.1259803549374158\n",
      "Last loss: 4.59528228987001, Avg loss: 5.581091473323724\n",
      "Last loss: 0.4790504684388722, Avg loss: 0.9041718554506841\n",
      "Last loss: 0.02016482228840187, Avg loss: 0.3511961999555311\n",
      "Last loss: 0.05049973467507602, Avg loss: 0.2380257650644026\n",
      "Last loss: 0.1274003250533247, Avg loss: 0.18866354504168859\n",
      "Last loss: 0.05487786727664742, Avg loss: 0.1675194038886801\n",
      "Last loss: 0.05296012128926285, Avg loss: 0.1577865735776913\n",
      "Last loss: 0.09837010853667205, Avg loss: 0.14872750420468692\n",
      "Last loss: 0.06432650841973267, Avg loss: 0.1375018056517559\n",
      "Last loss: 0.028441776257247864, Avg loss: 0.13946772540692448\n",
      "Last loss: 0.0425000747527816, Avg loss: 0.13564336891112483\n",
      "Last loss: 0.024160013636720264, Avg loss: 0.1353684484046253\n",
      "Last loss: 0.01968419015828078, Avg loss: 0.13494032396449454\n",
      "Last loss: 0.18540890427510517, Avg loss: 0.13802767070691976\n",
      "Last loss: 0.19254703981694968, Avg loss: 0.13380970966763142\n",
      "Last loss: 0.2364459987858583, Avg loss: 0.13967029516958937\n",
      "Last loss: 0.06966540053023305, Avg loss: 0.12525372224759382\n",
      "Last loss: 0.06786532118947775, Avg loss: 0.12494120237752278\n",
      "Last loss: 0.11385339560715435, Avg loss: 0.1291822510954921\n",
      "Last loss: 0.011549356617484238, Avg loss: 0.12345528265429931\n",
      "Last loss: 0.02882021364754151, Avg loss: 0.12085053363813174\n",
      "Last loss: 0.33016467087303053, Avg loss: 0.13102466153566275\n",
      "Last loss: 0.19624147134580888, Avg loss: 0.12560365785549932\n",
      "Last loss: 0.13594064908180398, Avg loss: 0.1285740992967823\n",
      "Last loss: 0.055521392255331606, Avg loss: 0.11935351690172101\n",
      "Last loss: 0.22793435474151877, Avg loss: 0.12359053461665585\n",
      "Last loss: 0.24013617394157638, Avg loss: 0.1286246587323561\n",
      "Last loss: 0.20604810210902016, Avg loss: 0.12502189785744985\n",
      "Last loss: 0.10255244466897374, Avg loss: 0.12241502656085637\n",
      "Last loss: 0.05900994724380072, Avg loss: 0.11891004144156936\n",
      "Last loss: 0.1871590984986387, Avg loss: 0.12107639379424863\n",
      "Last loss: 0.05001269222869288, Avg loss: 0.11430621943928859\n",
      "Last loss: 0.06827185214694645, Avg loss: 0.11534713034310749\n",
      "Last loss: 0.013643934079308227, Avg loss: 0.11283232027086693\n",
      "Last loss: 0.0334512610736831, Avg loss: 0.11160871917647505\n",
      "Last loss: 0.027551111895724345, Avg loss: 0.10797832512095001\n",
      "Last loss: 0.30291062770077937, Avg loss: 0.1298866816914775\n",
      "Last loss: 0.03808183746593539, Avg loss: 0.11923382305346243\n",
      "Last loss: 0.347967325975898, Avg loss: 0.1282275156103866\n",
      "Last loss: 0.09836456778929278, Avg loss: 0.11790102159875306\n",
      "Last loss: 0.7520958795810984, Avg loss: 7.213898080258247\n",
      "Last loss: 0.12890527322100107, Avg loss: 0.6058166947790057\n",
      "Last loss: 0.4973777225867344, Avg loss: 0.35834889280184706\n",
      "Last loss: 0.12478716437207321, Avg loss: 0.2873828917438516\n",
      "Last loss: 0.3935921468057233, Avg loss: 0.26925847471983627\n",
      "Last loss: 0.09433781730352883, Avg loss: 0.23865201834886549\n",
      "Last loss: 0.27713744398345497, Avg loss: 0.2296702403525034\n",
      "Last loss: 0.7200237091511701, Avg loss: 0.2348771305574694\n",
      "Last loss: 0.5374906942852532, Avg loss: 0.21761702221328091\n",
      "Last loss: 0.1841858014386304, Avg loss: 0.19536224836522254\n",
      "Last loss: 0.13651276269848686, Avg loss: 0.18832708964993916\n",
      "Last loss: 0.3345707382454032, Avg loss: 0.18839039727167\n",
      "Last loss: 0.3482494018547013, Avg loss: 0.18705414129966888\n",
      "Last loss: 0.3309497198910676, Avg loss: 0.18147693542866947\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 291.80it/s]\n",
      " 82%|████████▎ | 33/40 [00:00<00:00, 325.94it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.27846911990055595, Avg loss: 0.17510786050030908\n",
      "Last loss: 0.061386799272298065, Avg loss: 0.16455888648636058\n",
      "Last loss: 0.24708825131699225, Avg loss: 0.17029612049712822\n",
      "Last loss: 0.053561905338969984, Avg loss: 0.15795370940308473\n",
      "Last loss: 0.1873796347193834, Avg loss: 0.16124379807560957\n",
      "Last loss: 0.04831453745764286, Avg loss: 0.15153131246003196\n",
      "Last loss: 0.06215217640163096, Avg loss: 0.15134496084933502\n",
      "Last loss: 0.038995167472258715, Avg loss: 0.14717252618984442\n",
      "Last loss: 0.020515739383524703, Avg loss: 0.14478584186656132\n",
      "Last loss: 0.031148065046072693, Avg loss: 0.14339881810067254\n",
      "Last loss: 0.13293237413513168, Avg loss: 0.14515123059120338\n",
      "Last loss: 0.3402777043040313, Avg loss: 0.15053548769154107\n",
      "Last loss: 0.20499345704981164, Avg loss: 0.14668039108033393\n",
      "Last loss: 0.21741333516045033, Avg loss: 0.1436894246641072\n",
      "Last loss: 0.25642011036452683, Avg loss: 0.1454199736604974\n",
      "Last loss: 0.3282487305699491, Avg loss: 0.1480063215576249\n",
      "Last loss: 0.048549003606181695, Avg loss: 0.13431898114783805\n",
      "Last loss: 0.14501644949730533, Avg loss: 0.13851799041052595\n",
      "Last loss: 0.07057404077604638, Avg loss: 0.1349892255596915\n",
      "Last loss: 0.151328063584049, Avg loss: 0.1367641760257901\n",
      "Last loss: 0.11916585276874703, Avg loss: 0.13352136240858298\n",
      "Last loss: 0.10505795290646983, Avg loss: 0.13327409274713486\n",
      "Last loss: 0.019065390029112186, Avg loss: 0.12984869860614945\n",
      "Last loss: 0.017743217288734958, Avg loss: 0.12947921122522216\n",
      "Last loss: 0.023563178933227655, Avg loss: 0.1275455384904348\n",
      "Last loss: 0.11332008918581771, Avg loss: 0.1309347784761507\n",
      "Last loss: 4.176575278626245, Avg loss: 9.250357003754509\n",
      "Last loss: 0.7400589763600753, Avg loss: 1.9179274349540627\n",
      "Last loss: 0.27545697406343517, Avg loss: 0.7306858360700015\n",
      "Last loss: 0.0769896734147326, Avg loss: 0.4322552587680246\n",
      "Last loss: 0.9935311285984062, Avg loss: 0.3149256739550064\n",
      "Last loss: 0.1647582570807629, Avg loss: 0.24130910200253025\n",
      "Last loss: 0.08552554588072284, Avg loss: 0.2184918501435867\n",
      "Last loss: 0.07580468945033808, Avg loss: 0.21244341175689757\n",
      "Last loss: 0.16878361835817732, Avg loss: 0.20612419716819433\n",
      "Last loss: 0.01543087153490352, Avg loss: 0.16566470861566845\n",
      "Last loss: 0.09987323073314325, Avg loss: 0.1789330482932286\n",
      "Last loss: 0.27888635331138867, Avg loss: 0.17346960810404946\n",
      "Last loss: 0.05070138271985803, Avg loss: 0.1560669947610376\n",
      "Last loss: 0.3858147889177651, Avg loss: 0.17141838271514587\n",
      "Last loss: 0.034565704663299, Avg loss: 0.15787254514872798\n",
      "Last loss: 0.09004086006605168, Avg loss: 0.15399868076711198\n",
      "Last loss: 0.04191224653900229, Avg loss: 0.14475963154951682\n",
      "Last loss: 0.1943150721622377, Avg loss: 0.14644060119010552\n",
      "Last loss: 0.1915628430350821, Avg loss: 0.15432107833445408\n",
      "Last loss: 0.017458138795754693, Avg loss: 0.14141787127028285\n",
      "Last loss: 0.14709474845338724, Avg loss: 0.15173850340723427\n",
      "Last loss: 0.15320695948704569, Avg loss: 0.14500115346442488\n",
      "Last loss: 0.59270217613449, Avg loss: 0.15744183784807594\n",
      "Last loss: 0.044650702157570325, Avg loss: 0.14149552133541127\n",
      "Last loss: 0.06651523809937322, Avg loss: 0.1460974707104234\n",
      "Last loss: 0.03610835970303321, Avg loss: 0.14159339461905437\n",
      "Last loss: 0.1789311057447858, Avg loss: 0.14004445780861446\n",
      "Last loss: 0.1463685159923752, Avg loss: 0.14573846163431964\n",
      "Last loss: 0.19691327986918888, Avg loss: 0.14132977089160154\n",
      "Last loss: 0.10236409405526015, Avg loss: 0.1359725764774318\n",
      "Last loss: 0.4485057769644507, Avg loss: 0.1436099940180548\n",
      "Last loss: 0.03339706252562484, Avg loss: 0.13352291537725947\n",
      "Last loss: 0.17338883933412985, Avg loss: 0.13431984063495236\n",
      "Last loss: 0.3331832494403704, Avg loss: 0.1411773765757494\n",
      "Last loss: 0.12316460441772979, Avg loss: 0.1406746108936986\n",
      "Last loss: 0.015962134429484857, Avg loss: 0.13124191112113887\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 313.84it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 302.19it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.2232471779019805, Avg loss: 0.13257558162270458\n",
      "Last loss: 0.08329161004951274, Avg loss: 0.13194871545841025\n",
      "Last loss: 0.08563800412864753, Avg loss: 0.13166335444323057\n",
      "Last loss: 0.08358504313919551, Avg loss: 0.13296107168692112\n",
      "Last loss: 0.2925518672209805, Avg loss: 3.014483559142687\n",
      "Last loss: 0.7245571336650465, Avg loss: 0.3025451203056699\n",
      "Last loss: 0.12090198094766019, Avg loss: 0.23151776638653337\n",
      "Last loss: 0.33082902833331057, Avg loss: 0.20343048615374537\n",
      "Last loss: 0.21371661996458988, Avg loss: 0.1852644826191251\n",
      "Last loss: 0.054899700357144365, Avg loss: 0.16683755280338944\n",
      "Last loss: 0.012743742394782952, Avg loss: 0.15637250619624213\n",
      "Last loss: 0.06971378954723795, Avg loss: 0.15978531444743818\n",
      "Last loss: 0.2065991664070846, Avg loss: 0.1612376938423397\n",
      "Last loss: 0.03150769255824811, Avg loss: 0.15251704811500794\n",
      "Last loss: 0.026347048210561986, Avg loss: 0.14841426453427486\n",
      "Last loss: 0.03238349645291987, Avg loss: 0.15061163342099587\n",
      "Last loss: 0.1093812494090185, Avg loss: 0.14846757797714008\n",
      "Last loss: 0.22488652785635438, Avg loss: 0.14929652858848946\n",
      "Last loss: 0.1659522494152698, Avg loss: 0.14859503309637923\n",
      "Last loss: 0.006948335081198717, Avg loss: 0.13960557607216015\n",
      "Last loss: 0.007573121942232591, Avg loss: 0.1415448221735726\n",
      "Last loss: 0.07203684597636767, Avg loss: 0.139931161389735\n",
      "Last loss: 0.01667148481065573, Avg loss: 0.1375930867026362\n",
      "Last loss: 0.4823198887162064, Avg loss: 0.15297338724696655\n",
      "Last loss: 0.17664210749668954, Avg loss: 0.14325638908273383\n",
      "Last loss: 0.5358010557664595, Avg loss: 0.1564598601711052\n",
      "Last loss: 0.40385350130476566, Avg loss: 0.1486315389174973\n",
      "Last loss: 0.5840788013226129, Avg loss: 0.15232600737394988\n",
      "Last loss: 0.30528269750268516, Avg loss: 0.1450361058012022\n",
      "Last loss: 0.10048152142183239, Avg loss: 0.1355268124395109\n",
      "Last loss: 0.4259070148710796, Avg loss: 0.14716354872474482\n",
      "Last loss: 0.24868454154829006, Avg loss: 0.13848183664215982\n",
      "Last loss: 0.04565322248224523, Avg loss: 0.12904038843408838\n",
      "Last loss: 0.018317960194871214, Avg loss: 0.1337723291436067\n",
      "Last loss: 0.05072596183820606, Avg loss: 0.13106923985639907\n",
      "Last loss: 0.21331748278015192, Avg loss: 0.13518856997038345\n",
      "Last loss: 0.18228648006390658, Avg loss: 0.13751578385503133\n",
      "Last loss: 0.21273736959815187, Avg loss: 0.13643344263302948\n",
      "Last loss: 0.0865284748570645, Avg loss: 0.13131795277066555\n",
      "Last loss: 0.20869331156879045, Avg loss: 0.13372746547209974\n",
      "Last loss: 0.43038821963409013, Avg loss: 0.1434063158878614\n",
      "Last loss: 0.31340448129118, Avg loss: 0.13925590063986976\n",
      "Last loss: 0.022329043175915612, Avg loss: 0.1260504744645113\n",
      "Last loss: 0.02786070397784167, Avg loss: 0.12447183869320029\n",
      "Last loss: 3.250153548110565, Avg loss: 5.978957833675247\n",
      "Last loss: 1.3292212679001296, Avg loss: 2.368174211650347\n",
      "Last loss: 1.0801628363910527, Avg loss: 0.9953514385322825\n",
      "Last loss: 0.01592655692418389, Avg loss: 0.4617388989913743\n",
      "Last loss: 0.03967234353306748, Avg loss: 0.32712462498840394\n",
      "Last loss: 0.18302543144709624, Avg loss: 0.2591298632704583\n",
      "Last loss: 0.10641200199926315, Avg loss: 0.21840725967599978\n",
      "Last loss: 0.12799744940885247, Avg loss: 0.2084712508369931\n",
      "Last loss: 0.055060827041460376, Avg loss: 0.1937461106032561\n",
      "Last loss: 0.2556109149416608, Avg loss: 0.19282582693840464\n",
      "Last loss: 0.0058777442226394545, Avg loss: 0.1771608726436236\n",
      "Last loss: 0.24281929128908653, Avg loss: 0.1799442365702427\n",
      "Last loss: 0.03627866563857095, Avg loss: 0.16852902627144337\n",
      "Last loss: 0.24025891318713244, Avg loss: 0.17295187569337353\n",
      "Last loss: 0.3885567846898699, Avg loss: 0.1779645757818657\n",
      "Last loss: 0.09511720887020933, Avg loss: 0.16071308779517957\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 290.58it/s]\n",
      " 85%|████████▌ | 34/40 [00:00<00:00, 336.66it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.12161426587066144, Avg loss: 0.15959722467474696\n",
      "Last loss: 0.25679608702011497, Avg loss: 0.16094110984980073\n",
      "Last loss: 0.07876407542704415, Avg loss: 0.1514839990948677\n",
      "Last loss: 0.1497262998205473, Avg loss: 0.15090837233230878\n",
      "Last loss: 0.10280822018147091, Avg loss: 0.14994853900356445\n",
      "Last loss: 0.018375091070838213, Avg loss: 0.1436993947252714\n",
      "Last loss: 0.07004463330157802, Avg loss: 0.14377598458457577\n",
      "Last loss: 0.01638791180558956, Avg loss: 0.1405784813796269\n",
      "Last loss: 0.022211565581871824, Avg loss: 0.1395727363424972\n",
      "Last loss: 0.14303078141768574, Avg loss: 0.14221697240782805\n",
      "Last loss: 0.007870000289567473, Avg loss: 0.13526802955588854\n",
      "Last loss: 0.07207929003141166, Avg loss: 0.13614320106704703\n",
      "Last loss: 0.16110005799354699, Avg loss: 0.13908165066736683\n",
      "Last loss: 0.2690125182492055, Avg loss: 0.1475676252318212\n",
      "Last loss: 0.005155315645632743, Avg loss: 0.13372322575336518\n",
      "Last loss: 0.16714490567211715, Avg loss: 0.13839335305484665\n",
      "Last loss: 0.03740908194149386, Avg loss: 0.1333873120347469\n",
      "Last loss: 0.3062659386284186, Avg loss: 0.14098686616089995\n",
      "Last loss: 0.178968003905132, Avg loss: 0.13611812765759804\n",
      "Last loss: 0.20777575119076455, Avg loss: 0.1378454664481052\n",
      "Last loss: 0.017629230457939737, Avg loss: 0.13171895033922956\n",
      "Last loss: 0.3299991936952545, Avg loss: 0.13915863439228343\n",
      "Last loss: 0.02698137387261945, Avg loss: 0.12900450447339382\n",
      "Last loss: 0.021679349148659564, Avg loss: 0.12683536346821414\n",
      "Last loss: 0.12231920509725128, Avg loss: 1.2556849953160074\n",
      "Last loss: 0.6051712509510793, Avg loss: 0.3762554879326971\n",
      "Last loss: 0.03598206317725417, Avg loss: 0.2874107824933946\n",
      "Last loss: 0.03914717432829186, Avg loss: 0.2478600902850681\n",
      "Last loss: 1.317197466252423, Avg loss: 0.2770295699541422\n",
      "Last loss: 0.07810590177152245, Avg loss: 0.21737925689644427\n",
      "Last loss: 0.19347045905931967, Avg loss: 0.20445501543546046\n",
      "Last loss: 0.8494555970734369, Avg loss: 0.2189951624666103\n",
      "Last loss: 0.07956337596565158, Avg loss: 0.18844854796520583\n",
      "Last loss: 0.17941125427806182, Avg loss: 0.1829480193654302\n",
      "Last loss: 0.02999721925171979, Avg loss: 0.1733442049677045\n",
      "Last loss: 0.18612448022515196, Avg loss: 0.17531718428145046\n",
      "Last loss: 0.16567005282593125, Avg loss: 0.17147165864628203\n",
      "Last loss: 0.0671734915561286, Avg loss: 0.1647460782222222\n",
      "Last loss: 0.15364358025243438, Avg loss: 0.16632953013086038\n",
      "Last loss: 0.04488711835075744, Avg loss: 0.15928410192881207\n",
      "Last loss: 0.023853252823466523, Avg loss: 0.1568523558978883\n",
      "Last loss: 0.4287045046263996, Avg loss: 0.17073797026243429\n",
      "Last loss: 0.0927372635064956, Avg loss: 0.15455224296898232\n",
      "Last loss: 0.07684871825439829, Avg loss: 0.152077798633081\n",
      "Last loss: 0.26340770969576, Avg loss: 0.15830070734763788\n",
      "Last loss: 0.3869291763810203, Avg loss: 0.1605894971138399\n",
      "Last loss: 0.18407863272979313, Avg loss: 0.1495071010428917\n",
      "Last loss: 0.17575277774683973, Avg loss: 0.15197060147330393\n",
      "Last loss: 0.3473546494150569, Avg loss: 0.15420534705960579\n",
      "Last loss: 0.014183562282039206, Avg loss: 0.14072920506579578\n",
      "Last loss: 0.12447476163899995, Avg loss: 0.14444155828334462\n",
      "Last loss: 0.009276393874792385, Avg loss: 0.13682980282592255\n",
      "Last loss: 0.19733883420747542, Avg loss: 0.14187269491456866\n",
      "Last loss: 0.20340216198106187, Avg loss: 0.1428209728822743\n",
      "Last loss: 0.0746519845916688, Avg loss: 0.13647705535322055\n",
      "Last loss: 0.23974499208136965, Avg loss: 0.14134797501899096\n",
      "Last loss: 0.3153747565672627, Avg loss: 0.14375121258334736\n",
      "Last loss: 0.03600851770605636, Avg loss: 0.13266803074154665\n",
      "Last loss: 0.2223766288252453, Avg loss: 0.1394650757041625\n",
      "Last loss: 0.04979832309720888, Avg loss: 0.1311844361951366\n",
      "Last loss: 0.021823576974131717, Avg loss: 0.1282303130574243\n",
      "Last loss: 0.012619463863134603, Avg loss: 0.12959842965411011\n",
      "Last loss: 0.24859812657551528, Avg loss: 0.1369382908089161\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 317.61it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 284.68it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.22224700329480207, Avg loss: 0.13735234440750496\n",
      "Last loss: 0.05051457944776522, Avg loss: 3.0693851116303748\n",
      "Last loss: 0.022816940978270056, Avg loss: 0.49699047220169695\n",
      "Last loss: 0.26421762181142394, Avg loss: 0.4234403564823342\n",
      "Last loss: 0.02135051501006576, Avg loss: 0.34949388731835207\n",
      "Last loss: 0.8403250981303892, Avg loss: 0.3343806151372045\n",
      "Last loss: 0.06550758984975841, Avg loss: 0.2706420883550819\n",
      "Last loss: 0.2774117357536953, Avg loss: 0.2494004119719681\n",
      "Last loss: 0.44841222471175246, Avg loss: 0.23978589139509984\n",
      "Last loss: 0.06791926856349256, Avg loss: 0.20866640470187056\n",
      "Last loss: 0.3031083497319287, Avg loss: 0.2096178159043217\n",
      "Last loss: 0.05350733860973, Avg loss: 0.19087369591402673\n",
      "Last loss: 0.3796249536741978, Avg loss: 0.19815954946983094\n",
      "Last loss: 0.16137524468095948, Avg loss: 0.19048286116103283\n",
      "Last loss: 0.12507340083619825, Avg loss: 0.18337825669843888\n",
      "Last loss: 0.015046907120626927, Avg loss: 0.1718574557552818\n",
      "Last loss: 0.11370951588820954, Avg loss: 0.1780908314562461\n",
      "Last loss: 0.02011141648913823, Avg loss: 0.17215388370874563\n",
      "Last loss: 0.08616720912533511, Avg loss: 0.16840606763484858\n",
      "Last loss: 0.04017997167851907, Avg loss: 0.16824943108384682\n",
      "Last loss: 0.017987744157570495, Avg loss: 0.16433031570642656\n",
      "Last loss: 0.1494973390061332, Avg loss: 0.16493703747801763\n",
      "Last loss: 0.30937276036866734, Avg loss: 0.1735114447980555\n",
      "Last loss: 0.1940574497520929, Avg loss: 0.16592718066128054\n",
      "Last loss: 0.04878397307718105, Avg loss: 0.1635677586566586\n",
      "Last loss: 0.2028600933278581, Avg loss: 0.1636143264236261\n",
      "Last loss: 0.01694737598098259, Avg loss: 0.15549592581519328\n",
      "Last loss: 0.04609749516520518, Avg loss: 0.15300133544159666\n",
      "Last loss: 0.10737332958300516, Avg loss: 0.1551583360167054\n",
      "Last loss: 0.005921628084836901, Avg loss: 0.1500605427699546\n",
      "Last loss: 0.12995870543015983, Avg loss: 0.15388000990078896\n",
      "Last loss: 0.03517061113880154, Avg loss: 0.15185336917413936\n",
      "Last loss: 0.19633769664084516, Avg loss: 0.1536391154421784\n",
      "Last loss: 0.22343142724852716, Avg loss: 0.15759264803683887\n",
      "Last loss: 0.1941074104925583, Avg loss: 0.15266332569930335\n",
      "Last loss: 0.028495126766712765, Avg loss: 0.14647735612272242\n",
      "Last loss: 0.19502854327797162, Avg loss: 0.157275950691292\n",
      "Last loss: 0.06067809826870843, Avg loss: 0.1426705091896457\n",
      "Last loss: 0.04858811841425583, Avg loss: 0.14295439937464857\n",
      "Last loss: 0.1542390085702354, Avg loss: 0.14131687201791115\n",
      "Last loss: 0.11291153318167597, Avg loss: 0.14583971262907028\n",
      "Last loss: 0.6165502296131895, Avg loss: 5.879191957284714\n",
      "Last loss: 0.290806846609142, Avg loss: 0.9734060207137701\n",
      "Last loss: 1.391912780035025, Avg loss: 0.6233172051396583\n",
      "Last loss: 0.23126092215749117, Avg loss: 0.41734884487392876\n",
      "Last loss: 0.06790781231592621, Avg loss: 0.2971843261906473\n",
      "Last loss: 0.10581093774587733, Avg loss: 0.2599543829508376\n",
      "Last loss: 0.22227774664488317, Avg loss: 0.24610945752562713\n",
      "Last loss: 0.43614563867921974, Avg loss: 0.24025641915535595\n",
      "Last loss: 0.10855849741367933, Avg loss: 0.21808891266756378\n",
      "Last loss: 0.09258806742161582, Avg loss: 0.20714357478263495\n",
      "Last loss: 0.09508470404338112, Avg loss: 0.20552774089103754\n",
      "Last loss: 0.10670337430998493, Avg loss: 0.19590073234831867\n",
      "Last loss: 0.07437909160281518, Avg loss: 0.18901612317545718\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 267.72it/s]\n",
      " 78%|███████▊  | 31/40 [00:00<00:00, 303.66it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.2767379274618031, Avg loss: 0.197378207052943\n",
      "Last loss: 0.019845375258383313, Avg loss: 0.18348115120133032\n",
      "Last loss: 0.05220558956537186, Avg loss: 0.18484230348866668\n",
      "Last loss: 0.1063270631928669, Avg loss: 0.17539960900928497\n",
      "Last loss: 0.21265427126574477, Avg loss: 0.19207084343032413\n",
      "Last loss: 0.15247807076343284, Avg loss: 0.17295027614619507\n",
      "Last loss: 0.5564196988638945, Avg loss: 0.18911951212307565\n",
      "Last loss: 0.06230164468647617, Avg loss: 0.17511810441681894\n",
      "Last loss: 0.0606566594700881, Avg loss: 0.1740495048353822\n",
      "Last loss: 0.16381907023419778, Avg loss: 0.1714414469650758\n",
      "Last loss: 0.14965552467012283, Avg loss: 0.17190695228324998\n",
      "Last loss: 0.22933604441302513, Avg loss: 0.1728727108668616\n",
      "Last loss: 0.21001710821389827, Avg loss: 0.1693127309985891\n",
      "Last loss: 0.15217862504784732, Avg loss: 0.16561041913677108\n",
      "Last loss: 0.023551609285867883, Avg loss: 0.15869770702822494\n",
      "Last loss: 0.046266177718181936, Avg loss: 0.15727727937823752\n",
      "Last loss: 0.13679819404453325, Avg loss: 0.16844344084770443\n",
      "Last loss: 0.04925301762831943, Avg loss: 0.16303286322712046\n",
      "Last loss: 0.13652987287873336, Avg loss: 0.15959291662660866\n",
      "Last loss: 0.09398434924241414, Avg loss: 0.157325759818889\n",
      "Last loss: 0.07356245872944996, Avg loss: 0.15643811656171608\n",
      "Last loss: 0.7481738986607329, Avg loss: 0.18097060827798547\n",
      "Last loss: 0.06978720488352229, Avg loss: 0.16581246077902279\n",
      "Last loss: 0.05737892334126077, Avg loss: 0.1498695347433978\n",
      "Last loss: 0.10941950547101177, Avg loss: 0.1530279471786768\n",
      "Last loss: 0.07219928816665296, Avg loss: 0.15122540345176583\n",
      "Last loss: 0.25889301112383883, Avg loss: 0.15885209620500634\n",
      "Last loss: 1.1231682719271052, Avg loss: 5.007782334408991\n",
      "Last loss: 0.1748966970584353, Avg loss: 0.784255776948611\n",
      "Last loss: 0.1697747709250644, Avg loss: 0.4482089158062893\n",
      "Last loss: 0.47629725556771857, Avg loss: 0.30696745695767746\n",
      "Last loss: 0.07904545246857513, Avg loss: 0.23259408562630346\n",
      "Last loss: 0.2113742224362286, Avg loss: 0.22526220433640878\n",
      "Last loss: 0.10736814095016393, Avg loss: 0.21718508004777878\n",
      "Last loss: 0.15615129099435665, Avg loss: 0.2046687872900252\n",
      "Last loss: 0.09441927037788746, Avg loss: 0.18070973678210056\n",
      "Last loss: 0.2646167133760319, Avg loss: 0.1836637160333123\n",
      "Last loss: 0.3488203842053832, Avg loss: 0.17736950889132916\n",
      "Last loss: 0.11421673904209842, Avg loss: 0.16983240514374534\n",
      "Last loss: 0.049056322178663805, Avg loss: 0.16235872555601785\n",
      "Last loss: 0.02048447181243775, Avg loss: 0.16654725621222832\n",
      "Last loss: 0.12561945936568628, Avg loss: 0.17254722643128156\n",
      "Last loss: 0.04918803177344297, Avg loss: 0.14865607891051646\n",
      "Last loss: 0.04180921435721663, Avg loss: 0.15448494248172906\n",
      "Last loss: 0.052847816585230994, Avg loss: 0.15844133543805816\n",
      "Last loss: 0.15405111980041686, Avg loss: 0.19909422449904446\n",
      "Last loss: 0.10407573107768972, Avg loss: 0.15614237379862764\n",
      "Last loss: 0.1360035998929855, Avg loss: 0.15624016371271315\n",
      "Last loss: 0.28911207107878684, Avg loss: 0.15376529169280884\n",
      "Last loss: 0.04623554989699607, Avg loss: 0.16027251583214722\n",
      "Last loss: 0.06538711430759785, Avg loss: 0.14651898840607872\n",
      "Last loss: 0.2617999210907553, Avg loss: 0.15543202588359878\n",
      "Last loss: 0.015251110576199538, Avg loss: 0.14556236568303552\n",
      "Last loss: 0.015779168077698236, Avg loss: 0.15239981309060793\n",
      "Last loss: 0.15926003044041684, Avg loss: 0.14446370416367332\n",
      "Last loss: 0.2852219519096572, Avg loss: 0.149915250781891\n",
      "Last loss: 0.15107638768576345, Avg loss: 0.15185152878489355\n",
      "Last loss: 0.05642474860954054, Avg loss: 0.1420509773858204\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 295.79it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 282.08it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.09413283826264403, Avg loss: 0.14271892635401803\n",
      "Last loss: 0.10051888763986583, Avg loss: 0.152141277103156\n",
      "Last loss: 0.2981548423180919, Avg loss: 0.15030606792160042\n",
      "Last loss: 0.06616926496137258, Avg loss: 0.13474344894415466\n",
      "Last loss: 0.053490192147027, Avg loss: 0.13537550132971568\n",
      "Last loss: 0.2165983477475257, Avg loss: 0.1417198155031008\n",
      "Last loss: 0.05436673831126877, Avg loss: 0.13242087407298542\n",
      "Last loss: 0.02675997685483511, Avg loss: 0.13702522418055116\n",
      "Last loss: 0.22104616628999924, Avg loss: 0.1416464535886008\n",
      "Last loss: 2.3197155857715286, Avg loss: 8.112569359914097\n",
      "Last loss: 3.128316054644319, Avg loss: 2.6622265116383366\n",
      "Last loss: 2.548762837629219, Avg loss: 1.817833168172948\n",
      "Last loss: 0.5277170338964321, Avg loss: 1.1066723376298928\n",
      "Last loss: 0.250893898393034, Avg loss: 0.7323712526300145\n",
      "Last loss: 0.27540243519812385, Avg loss: 0.5137114027662147\n",
      "Last loss: 0.38837508525430753, Avg loss: 0.41214421567515236\n",
      "Last loss: 0.16108154860061047, Avg loss: 0.3382988734660018\n",
      "Last loss: 0.2020402581377361, Avg loss: 0.3114329909066433\n",
      "Last loss: 0.60586623962812, Avg loss: 0.30899017996499845\n",
      "Last loss: 0.11112119425622319, Avg loss: 0.2701254266418793\n",
      "Last loss: 0.13657514426488415, Avg loss: 0.25935231897937777\n",
      "Last loss: 0.3650407537871523, Avg loss: 0.25374180241583705\n",
      "Last loss: 0.1393343640479882, Avg loss: 0.23589117292734735\n",
      "Last loss: 0.2128778955017054, Avg loss: 0.23088286662616525\n",
      "Last loss: 0.20198485665137725, Avg loss: 0.22424988242090246\n",
      "Last loss: 0.31765459265996016, Avg loss: 0.21938516545096498\n",
      "Last loss: 0.16214482254207993, Avg loss: 0.20642101921643752\n",
      "Last loss: 0.13335063640838887, Avg loss: 0.19662557728617444\n",
      "Last loss: 0.09503503801833971, Avg loss: 0.19734870378789507\n",
      "Last loss: 0.28738439311592867, Avg loss: 0.19633842431745327\n",
      "Last loss: 0.3280707633497707, Avg loss: 0.19003116930643577\n",
      "Last loss: 0.10310751792680746, Avg loss: 0.18248210629720513\n",
      "Last loss: 0.08979579475305335, Avg loss: 0.18240621493126433\n",
      "Last loss: 0.11061644927330766, Avg loss: 0.17383909185683108\n",
      "Last loss: 0.10696199530258935, Avg loss: 0.17322701782986868\n",
      "Last loss: 0.10732163520503116, Avg loss: 0.17054381745099073\n",
      "Last loss: 0.24258972669674178, Avg loss: 0.1730521705753246\n",
      "Last loss: 0.2548753938835558, Avg loss: 0.1753357644387687\n",
      "Last loss: 0.40019762052497476, Avg loss: 0.17838488962092341\n",
      "Last loss: 0.22095478403760505, Avg loss: 0.16746822699580843\n",
      "Last loss: 0.3793885493149758, Avg loss: 0.17522180101956752\n",
      "Last loss: 0.11922254885214875, Avg loss: 0.15921926555998647\n",
      "Last loss: 0.20449319237880142, Avg loss: 0.1596078217015519\n",
      "Last loss: 0.03422279198423843, Avg loss: 0.14952555430395262\n",
      "Last loss: 0.06167216350963505, Avg loss: 0.15395311592554575\n",
      "Last loss: 0.3877953065222648, Avg loss: 0.1621078032423876\n",
      "Last loss: 0.2457450918945283, Avg loss: 0.15841526438280712\n",
      "Last loss: 0.1774299730027531, Avg loss: 0.15239672169229335\n",
      "Last loss: 0.10818397763543268, Avg loss: 0.14977954005161592\n",
      "Last loss: 0.20093323042386452, Avg loss: 2.0121666054714598\n",
      "Last loss: 0.6890624931718119, Avg loss: 0.9577304797041126\n",
      "Last loss: 0.9921685912645174, Avg loss: 0.5383700717736585\n",
      "Last loss: 0.508077649589478, Avg loss: 0.33734044216998615\n",
      "Last loss: 0.10310289171545861, Avg loss: 0.25899460592716256\n",
      "Last loss: 0.04056281836355166, Avg loss: 0.2199364588747488\n",
      "Last loss: 0.19834756649187524, Avg loss: 0.21927687328108517\n",
      "Last loss: 0.23257748563171143, Avg loss: 0.21100873389325456\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 321.29it/s]\n",
      " 85%|████████▌ | 34/40 [00:00<00:00, 333.25it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.332583533758046, Avg loss: 0.2033315016751431\n",
      "Last loss: 0.025759826739092426, Avg loss: 0.18640641114344847\n",
      "Last loss: 0.09987788070352246, Avg loss: 0.19046768056137145\n",
      "Last loss: 0.023469248076849483, Avg loss: 0.1788952141695526\n",
      "Last loss: 0.5784217841523661, Avg loss: 0.19581662635892005\n",
      "Last loss: 0.07671353223750793, Avg loss: 0.1738909489734485\n",
      "Last loss: 0.06160133492854387, Avg loss: 0.16594184558413178\n",
      "Last loss: 0.2191328310034875, Avg loss: 0.16475055104407904\n",
      "Last loss: 0.3284340303139654, Avg loss: 0.16409817805895416\n",
      "Last loss: 0.01712658635529963, Avg loss: 0.15537264137827317\n",
      "Last loss: 0.15234020745615234, Avg loss: 0.15179124951481138\n",
      "Last loss: 0.2959170338447885, Avg loss: 0.15034462380373065\n",
      "Last loss: 0.004856942603242435, Avg loss: 0.15150035911478377\n",
      "Last loss: 0.1546702927715903, Avg loss: 0.1473562697676829\n",
      "Last loss: 0.015019246695374942, Avg loss: 0.15126397871701494\n",
      "Last loss: 0.23003170240674042, Avg loss: 0.1502570702815126\n",
      "Last loss: 0.055062073742599825, Avg loss: 0.13940322033315578\n",
      "Last loss: 0.2710015497673906, Avg loss: 0.1485023813562555\n",
      "Last loss: 0.1795300237113892, Avg loss: 0.14316183355153936\n",
      "Last loss: 0.3152885946554198, Avg loss: 0.1483753643406351\n",
      "Last loss: 0.024544509305681646, Avg loss: 0.1333280923885783\n",
      "Last loss: 0.019856972707449246, Avg loss: 0.1317238312379808\n",
      "Last loss: 0.26847715287864116, Avg loss: 0.1485931865130706\n",
      "Last loss: 0.03159927592879812, Avg loss: 0.1368911620610945\n",
      "Last loss: 0.10493707241398176, Avg loss: 0.13620361627799873\n",
      "Last loss: 0.1697438802973772, Avg loss: 0.1351126183024191\n",
      "Last loss: 0.1234196895182111, Avg loss: 0.138445790372284\n",
      "Last loss: 0.04171291315071798, Avg loss: 0.12902253317758033\n",
      "Last loss: 0.05261386041007783, Avg loss: 0.13392015232957358\n",
      "Last loss: 0.16834348285696527, Avg loss: 0.1362765911935647\n",
      "Last loss: 0.03277052797782992, Avg loss: 0.12748385541276955\n",
      "Last loss: 0.013158707236580556, Avg loss: 0.13419003622607253\n",
      "Last loss: 0.9699057607797736, Avg loss: 6.471548966004428\n",
      "Last loss: 0.6502421269711337, Avg loss: 0.7639115959146116\n",
      "Last loss: 0.07274478493601658, Avg loss: 0.5077199093344665\n",
      "Last loss: 0.4875403391711858, Avg loss: 0.4292865646432315\n",
      "Last loss: 0.5867027142635797, Avg loss: 0.37078806975922035\n",
      "Last loss: 0.47486845158005037, Avg loss: 0.3259662577141118\n",
      "Last loss: 0.0678428468928079, Avg loss: 0.272783078975911\n",
      "Last loss: 0.3292601680779413, Avg loss: 0.2648692231138879\n",
      "Last loss: 0.11033876578154997, Avg loss: 0.24264657054568636\n",
      "Last loss: 0.127660604099896, Avg loss: 0.22724850510099698\n",
      "Last loss: 0.29152540344210925, Avg loss: 0.21960587647526758\n",
      "Last loss: 0.036293596921467934, Avg loss: 0.19940434761018902\n",
      "Last loss: 0.022702065717081235, Avg loss: 0.1946362686575134\n",
      "Last loss: 0.06356818594748814, Avg loss: 0.19156213651631696\n",
      "Last loss: 0.11291537894345695, Avg loss: 0.19060869337407724\n",
      "Last loss: 0.13998097950163724, Avg loss: 0.18640315560574472\n",
      "Last loss: 0.004370217600800898, Avg loss: 0.180450504607155\n",
      "Last loss: 0.047982455135241735, Avg loss: 0.17440886375958958\n",
      "Last loss: 0.16920208311464552, Avg loss: 0.17591110268231097\n",
      "Last loss: 0.1933894993882989, Avg loss: 0.17114651057359545\n",
      "Last loss: 0.19735465241190125, Avg loss: 0.1719794730516287\n",
      "Last loss: 0.15973339491765234, Avg loss: 0.16690054984949337\n",
      "Last loss: 0.3231831707206602, Avg loss: 0.16946589801292125\n",
      "Last loss: 0.07131730054098906, Avg loss: 0.15794620629269873\n",
      "Last loss: 0.1880534690752363, Avg loss: 0.1619973492095448\n",
      "Last loss: 0.11203310694179507, Avg loss: 0.1583051557778079\n",
      "Last loss: 0.15387860932237818, Avg loss: 0.1534757588461784\n",
      "Last loss: 0.15168046777811167, Avg loss: 0.15604667989214369\n",
      "Last loss: 0.08803147571698994, Avg loss: 0.15124166071656833\n",
      "Last loss: 0.06899514124975478, Avg loss: 0.1498804669140269\n",
      "Last loss: 0.19333915187144188, Avg loss: 0.15344954284565102\n",
      "Last loss: 0.012816630102280908, Avg loss: 0.14601254373634595\n",
      "Last loss: 0.021036604262423323, Avg loss: 0.14800896031786703\n",
      "Last loss: 0.3698322621944717, Avg loss: 0.15501683855923892\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 317.66it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 341.91it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.07420229495514641, Avg loss: 0.14712638865225358\n",
      "Last loss: 0.117302088666142, Avg loss: 0.14266198034819266\n",
      "Last loss: 0.10368754067258266, Avg loss: 0.14013623387020896\n",
      "Last loss: 0.038666033325640975, Avg loss: 0.1378925559940646\n",
      "Last loss: 0.10537790792929967, Avg loss: 0.14191370305858608\n",
      "Last loss: 0.15234758494163594, Avg loss: 0.14476994826162945\n",
      "Last loss: 0.39657588496650215, Avg loss: 5.507172730317351\n",
      "Last loss: 0.048418893495296454, Avg loss: 1.0092934701583767\n",
      "Last loss: 0.2317690141177669, Avg loss: 0.7040868309271748\n",
      "Last loss: 0.3032897808398349, Avg loss: 0.5496699314730128\n",
      "Last loss: 0.13042473231623714, Avg loss: 0.4392193694948393\n",
      "Last loss: 0.6608752156833375, Avg loss: 0.39676552390408926\n",
      "Last loss: 1.3630715357358731, Avg loss: 0.37373643951922186\n",
      "Last loss: 0.15226711858897402, Avg loss: 0.28680761014128175\n",
      "Last loss: 0.08651629837150414, Avg loss: 0.25549191132454674\n",
      "Last loss: 0.10697637235136086, Avg loss: 0.2401299893561369\n",
      "Last loss: 0.2577801350695938, Avg loss: 0.22351153009188268\n",
      "Last loss: 0.29245655863179065, Avg loss: 0.22358332529868613\n",
      "Last loss: 0.2948585893271511, Avg loss: 0.20789679811148107\n",
      "Last loss: 0.020901419100485628, Avg loss: 0.19214457677121882\n",
      "Last loss: 0.11350822615382948, Avg loss: 0.19501867590933677\n",
      "Last loss: 0.18862156061879937, Avg loss: 0.1900794187614565\n",
      "Last loss: 0.05367720736922153, Avg loss: 0.17739013419578922\n",
      "Last loss: 0.5213067688640788, Avg loss: 0.18938869208034875\n",
      "Last loss: 0.2106721253035239, Avg loss: 0.18104458627488879\n",
      "Last loss: 0.07549934839648828, Avg loss: 0.168792226746742\n",
      "Last loss: 0.4506890579962509, Avg loss: 0.1842833770976577\n",
      "Last loss: 0.059912307235138094, Avg loss: 0.16454749573754773\n",
      "Last loss: 0.01760522332705727, Avg loss: 0.1543793109800093\n",
      "Last loss: 0.10523644704618638, Avg loss: 0.15673687067636966\n",
      "Last loss: 0.003961124264416229, Avg loss: 0.15166999209163864\n",
      "Last loss: 0.05190318281410834, Avg loss: 0.15055111267955862\n",
      "Last loss: 0.12117276460836052, Avg loss: 0.1539829054713995\n",
      "Last loss: 0.032528026425666184, Avg loss: 0.14932643877897483\n",
      "Last loss: 0.40531503840431216, Avg loss: 0.15957008417123417\n",
      "Last loss: 0.5844713972092154, Avg loss: 0.1642065707516494\n",
      "Last loss: 0.6475376095702146, Avg loss: 0.16932054262478152\n",
      "Last loss: 0.256716753382201, Avg loss: 0.15178473355466957\n",
      "Last loss: 0.33461728466858104, Avg loss: 0.15276501615431531\n",
      "Last loss: 0.06470987362701464, Avg loss: 0.14481702157696505\n",
      "Last loss: 0.11673431650492366, Avg loss: 0.14042518008233074\n",
      "Last loss: 0.04006214404823208, Avg loss: 0.13547862221399137\n",
      "Last loss: 0.011702105426639457, Avg loss: 0.13626606605194805\n",
      "Last loss: 0.08574345870745542, Avg loss: 0.13727953238543075\n",
      "Last loss: 0.1935040468672815, Avg loss: 0.13865059988055856\n",
      "Last loss: 0.06760746819547794, Avg loss: 0.1313596891157499\n",
      "Last loss: 1.0935566351907675, Avg loss: 5.011656104694164\n",
      "Last loss: 0.5234549794150953, Avg loss: 0.7134090782778161\n",
      "Last loss: 0.42570408395497245, Avg loss: 0.5535042514202867\n",
      "Last loss: 0.32498794663002695, Avg loss: 0.447561223793942\n",
      "Last loss: 0.05788195641830438, Avg loss: 0.41317574396535506\n",
      "Last loss: 0.09588793805788051, Avg loss: 0.33053620214507384\n",
      "Last loss: 0.2880906769947305, Avg loss: 0.3359710755702575\n",
      "Last loss: 0.008257268212098053, Avg loss: 0.2839972135640502\n",
      "Last loss: 0.4599671918054076, Avg loss: 0.28797106804730044\n",
      "Last loss: 0.6526652374992, Avg loss: 0.28986396124485553\n",
      "Last loss: 0.7549869941822172, Avg loss: 0.2865477991939417\n",
      "Last loss: 0.5703272717160155, Avg loss: 0.2825985817948928\n",
      "Last loss: 0.5567843456660182, Avg loss: 0.26947473875231986\n",
      "Last loss: 0.002751082308751239, Avg loss: 0.2265713955375887\n",
      "Last loss: 0.01645235746112612, Avg loss: 0.217952815588321\n",
      "Last loss: 0.4710711166345668, Avg loss: 0.23007894404036552\n",
      "Last loss: 0.09076734631300458, Avg loss: 0.21426049212868448\n",
      "Last loss: 0.10068422909683054, Avg loss: 0.20458746571972147\n",
      "Last loss: 0.024912369032217975, Avg loss: 0.2155670363327816\n",
      "Last loss: 0.17310123098324162, Avg loss: 0.20973890506701695\n",
      "Last loss: 0.6405105433523451, Avg loss: 0.22996797357383908\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 318.20it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Last loss: 0.0019610322784182027, Avg loss: 0.19335644182600845\n",
      "Last loss: 0.04306472005883719, Avg loss: 0.19974493173930655\n",
      "Last loss: 0.17342155710232127, Avg loss: 0.19862972149230912\n",
      "Last loss: 0.3699679655915916, Avg loss: 0.20432542129009323\n",
      "Last loss: 0.003349230739524295, Avg loss: 0.18670678505095728\n",
      "Last loss: 0.10890698455814446, Avg loss: 0.18317695704712605\n",
      "Last loss: 0.24511791044983144, Avg loss: 0.1870666690992107\n",
      "Last loss: 0.5079175991557509, Avg loss: 0.20095940666506648\n",
      "Last loss: 0.23823336624172348, Avg loss: 0.17387646536619392\n",
      "Last loss: 0.018004332549348512, Avg loss: 0.16481219333834096\n",
      "Last loss: 0.021229145510877664, Avg loss: 0.1574968063441438\n",
      "Last loss: 0.014670967908569919, Avg loss: 0.1665302426901021\n",
      "Last loss: 0.13849596710229914, Avg loss: 0.1563763725543575\n",
      "Last loss: 0.060915063383026206, Avg loss: 0.159355255824392\n",
      "Last loss: 0.02660029946766679, Avg loss: 0.14999222053858266\n",
      "Last loss: 0.013294239530855824, Avg loss: 0.15003532585839113\n",
      "Last loss: 0.24637775427075753, Avg loss: 0.1480825479082381\n",
      "Last loss: 0.013110616106294687, Avg loss: 0.16198541342744555\n",
      "Last loss: 0.40983043640608985, Avg loss: 0.16901678976005666\n",
      "Accuracy 0.9675\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPbVTFnMQuvW"
   },
   "source": [
    "## PyTorch\n",
    "\n",
    "Для выполнения следующего задания понадобится PyTorch. [Инструкция по установке](https://pytorch.org/get-started/locally/)\n",
    "\n",
    "Если у вас нет GPU, то можно использовать [Google Colab](https://colab.research.google.com/)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tV0mJLu-QuvX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1610650452944,
     "user_tz": -180,
     "elapsed": 4170,
     "user": {
      "displayName": "Iaroslav Sokolov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHt_w9-U1o8Sy_kutVo5pzmBrbbw0dWtxW9hnv=s64",
      "userId": "04885912710307977692"
     }
    }
   },
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VUC_QqpAQuva",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100,
     "referenced_widgets": [
      "5a7bd829ccdd44449266badd69bd0262",
      "ad76684efa9e4d9a80d46ec35c62a757",
      "3023475c672047f1937e78184214f2ca",
      "ac41fc0e882e424fb22466eed075fc60",
      "964b11cd843f4d7dbb8d00d1005fd5cc",
      "167cc9e8396f480f90cf141a22019ff0",
      "15db84e2b3844384b872e907ae4a6a57",
      "2668cebcf3e34cbd9454be153897e192"
     ]
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1610650457833,
     "user_tz": -180,
     "elapsed": 8455,
     "user": {
      "displayName": "Iaroslav Sokolov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHt_w9-U1o8Sy_kutVo5pzmBrbbw0dWtxW9hnv=s64",
      "userId": "04885912710307977692"
     }
    },
    "outputId": "59d0dca1-cc62-4ab5-aa5e-988b75d10954"
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "t = transforms.ToTensor()\n",
    "\n",
    "cifar_train = datasets.CIFAR10(\"datasets/cifar10\", download=True, train=True, transform=t)\n",
    "train_loader = DataLoader(cifar_train, batch_size=1024, shuffle=True, pin_memory=torch.cuda.is_available())\n",
    "cifar_test = datasets.CIFAR10(\"datasets/cifar10\", download=True, train=False, transform=t)\n",
    "test_loader = DataLoader(cifar_test, batch_size=1024, shuffle=False, pin_memory=torch.cuda.is_available())"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to datasets/cifar10/cifar-10-python.tar.gz\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7bd829ccdd44449266badd69bd0262",
       "version_minor": 0,
       "version_major": 2
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "Extracting datasets/cifar10/cifar-10-python.tar.gz to datasets/cifar10\n",
      "Files already downloaded and verified\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGmpjcFfQuvd"
   },
   "source": [
    "### Задание 4 (3 балла)\n",
    "А теперь поработам с настоящими нейронными сетями и настоящими данными. Необходимо реализовать сверточную нейронную сеть, которая будет классифицировать изображения из датасета CIFAR10. Имплементируйте класс `Model` и функцию `calculate_loss`. \n",
    "\n",
    "Обратите внимание, что `Model` должна считать в конце `softmax`, т.к. мы решаем задачу классификации. Соответствеено, функция `calculate_loss` считает cross-entropy.\n",
    "\n",
    "Для успешного выполнения задания необходимо, чтобы `accuracy`, `mean precision` и `mean recall` были больше 0.5\n",
    "\n",
    "__Можно пользоваться всем содержимым библиотеки PyTorch.__"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wS75MzDXPUKf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1610650460167,
     "user_tz": -180,
     "elapsed": 571,
     "user": {
      "displayName": "Iaroslav Sokolov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHt_w9-U1o8Sy_kutVo5pzmBrbbw0dWtxW9hnv=s64",
      "userId": "04885912710307977692"
     }
    }
   },
   "source": [
    "class MyVGG(nn.Module):\n",
    "    def __init__(self, n_blocks: int, num_channels: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_ch = 3\n",
    "        out_ch = num_channels\n",
    "        for i in range(n_blocks):\n",
    "            num_convs = 2\n",
    "            layers.extend(\n",
    "                self._get_VGG_block(num_convs, in_ch, out_ch)\n",
    "            )\n",
    "            in_ch = out_ch\n",
    "            out_ch *= 2\n",
    "            \n",
    "        layers.append(nn.Flatten())\n",
    "        result_dim = (32 // (2 ** n_blocks)) ** 2 * (out_ch // 2)\n",
    "        \n",
    "        layers.append(nn.Linear(result_dim, result_dim // 4))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(result_dim // 4, 10))\n",
    "        layers.append(nn.Softmax())\n",
    "        \n",
    "        self._nn = nn.Sequential(*layers)\n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self._nn(x)\n",
    "        \n",
    "    def _get_VGG_block(self, num_convs: int, in_ch: int, out_ch: int) -> List[nn.Module]:\n",
    "        layers = []\n",
    "        \n",
    "        for _ in range(num_convs):\n",
    "            layers.append(nn.Conv2d(in_ch, out_ch, 3, padding=1))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_ch = out_ch\n",
    "            \n",
    "        layers.append(nn.MaxPool2d(2))\n",
    "        return layers\n",
    "        \n",
    "        "
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5sRmTKwKQuve",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1610650462972,
     "user_tz": -180,
     "elapsed": 712,
     "user": {
      "displayName": "Iaroslav Sokolov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHt_w9-U1o8Sy_kutVo5pzmBrbbw0dWtxW9hnv=s64",
      "userId": "04885912710307977692"
     }
    },
    "outputId": "5609b9aa-eda6-4589-9d9f-299ce7612888"
   },
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._nn = MyVGG(n_blocks=5, num_channels=8)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self._nn(x)\n",
    "        \n",
    "def calculate_loss(X: torch.Tensor, y: torch.Tensor, model: Model):\n",
    "    \"\"\"\n",
    "    Cчитает cross-entropy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : torch.Tensor\n",
    "        Данные для обучения.\n",
    "    y : torch.Tensor\n",
    "        Метки классов.\n",
    "    model : Model\n",
    "        Модель, которую будем обучать.\n",
    "\n",
    "    \"\"\"\n",
    "    preds = model(X)\n",
    "    return torch.nn.functional.cross_entropy(preds, y)\n",
    "\n",
    "model = Model()\n",
    "print(sum(n.numel() for n in model.parameters()))\n",
    "print(model)\n",
    "del model"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "299506\n",
      "Model(\n",
      "  (_nn): MyVGG(\n",
      "    (_nn): Sequential(\n",
      "      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU()\n",
      "      (7): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): ReLU()\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU()\n",
      "      (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU()\n",
      "      (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (15): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (16): ReLU()\n",
      "      (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): ReLU()\n",
      "      (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (20): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (21): ReLU()\n",
      "      (22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (23): ReLU()\n",
      "      (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (25): Flatten(start_dim=1, end_dim=-1)\n",
      "      (26): Linear(in_features=128, out_features=32, bias=True)\n",
      "      (27): ReLU()\n",
      "      (28): Linear(in_features=32, out_features=10, bias=True)\n",
      "      (29): Softmax(dim=None)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAsLmkUqQuvh"
   },
   "source": [
    "Теперь обучим нашу модель. Для этого используем ранее созданные batch loader'ы."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "k5G8iMCeQuvh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1610650517958,
     "user_tz": -180,
     "elapsed": 958,
     "user": {
      "displayName": "Iaroslav Sokolov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHt_w9-U1o8Sy_kutVo5pzmBrbbw0dWtxW9hnv=s64",
      "userId": "04885912710307977692"
     }
    }
   },
   "source": [
    "def train(model, epochs=100):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for i in tqdm(range(epochs)):\n",
    "        #Train\n",
    "        loss_mean = 0\n",
    "        elements = 0\n",
    "        for X, y in iter(train_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            loss = calculate_loss(X, y, model)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_mean += loss.item() * len(X)\n",
    "            elements += len(X)\n",
    "        train_losses.append(loss_mean / elements)\n",
    "        #Test\n",
    "        loss_mean = 0 \n",
    "        elements = 0\n",
    "        for X, y in iter(test_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            loss = calculate_loss(X, y, model)\n",
    "            loss_mean += loss.item() * len(X)\n",
    "            elements += len(X)\n",
    "        test_losses.append(loss_mean / elements)\n",
    "        print(\"Epoch\", i, \"| Train loss\", train_losses[-1], \"| Test loss\", test_losses[-1])\n",
    "    return train_losses, test_losses"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vmD9eWJOQuvl",
    "scrolled": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1610651349244,
     "user_tz": -180,
     "elapsed": 831714,
     "user": {
      "displayName": "Iaroslav Sokolov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHt_w9-U1o8Sy_kutVo5pzmBrbbw0dWtxW9hnv=s64",
      "userId": "04885912710307977692"
     }
    },
    "outputId": "ca22d352-d491-4f1d-c4ea-7ed79e84549a"
   },
   "source": [
    "model = Model().to(device)\n",
    "train_l, test_l = train(model)"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001B[A\u001B[A/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "\n",
      "\n",
      "  1%|          | 1/100 [00:08<13:30,  8.18s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train loss 2.302628239974976 | Test loss 2.302139254760742\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  2%|▏         | 2/100 [00:16<13:20,  8.16s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train loss 2.2546442179107666 | Test loss 2.2036627304077148\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  3%|▎         | 3/100 [00:24<13:21,  8.26s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train loss 2.195166576080322 | Test loss 2.1917566360473635\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  4%|▍         | 4/100 [00:32<13:08,  8.22s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train loss 2.1694925844573976 | Test loss 2.1399007530212404\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  5%|▌         | 5/100 [00:41<13:02,  8.23s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train loss 2.1375945639038085 | Test loss 2.1224499572753905\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  6%|▌         | 6/100 [00:49<12:58,  8.28s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train loss 2.135072658233643 | Test loss 2.112924698257446\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  7%|▋         | 7/100 [00:57<12:51,  8.30s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train loss 2.1179899033355714 | Test loss 2.1349908966064453\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  8%|▊         | 8/100 [01:06<12:53,  8.41s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train loss 2.111645665740967 | Test loss 2.097199210357666\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  9%|▉         | 9/100 [01:14<12:40,  8.36s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train loss 2.105873511428833 | Test loss 2.097117166519165\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 10%|█         | 10/100 [01:23<12:33,  8.37s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train loss 2.0987769203948976 | Test loss 2.098669612503052\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 11%|█         | 11/100 [01:31<12:24,  8.36s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train loss 2.0976305503845216 | Test loss 2.0831280349731447\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 12%|█▏        | 12/100 [01:39<12:15,  8.36s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train loss 2.0878525782012938 | Test loss 2.081660581970215\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 13%|█▎        | 13/100 [01:48<12:07,  8.36s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train loss 2.089254570083618 | Test loss 2.0848813594818116\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 14%|█▍        | 14/100 [01:56<11:58,  8.36s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train loss 2.0755881746673586 | Test loss 2.077370405960083\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 15%|█▌        | 15/100 [02:04<11:45,  8.30s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train loss 2.076414569015503 | Test loss 2.0698908748626708\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 16%|█▌        | 16/100 [02:12<11:28,  8.19s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train loss 2.0770684607696532 | Test loss 2.064329238128662\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 17%|█▋        | 17/100 [02:21<11:33,  8.36s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 16 | Train loss 2.0642788166046144 | Test loss 2.0633263023376465\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 18%|█▊        | 18/100 [02:29<11:25,  8.35s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 17 | Train loss 2.0594812631225587 | Test loss 2.045135305786133\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 19%|█▉        | 19/100 [02:37<11:05,  8.22s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 18 | Train loss 2.046304886016846 | Test loss 2.0387560276031493\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 20%|██        | 20/100 [02:45<10:55,  8.19s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 19 | Train loss 2.0395990552520753 | Test loss 2.0539548763275146\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 21%|██        | 21/100 [02:54<10:53,  8.28s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 20 | Train loss 2.03895608253479 | Test loss 2.0482369171142576\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 22%|██▏       | 22/100 [03:02<10:46,  8.29s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 21 | Train loss 2.0424453451538085 | Test loss 2.028343857574463\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 23%|██▎       | 23/100 [03:11<10:44,  8.37s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 22 | Train loss 2.026204627075195 | Test loss 2.027991608810425\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 24%|██▍       | 24/100 [03:19<10:42,  8.45s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 23 | Train loss 2.014660909729004 | Test loss 2.0180909240722658\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 25%|██▌       | 25/100 [03:28<10:34,  8.45s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 24 | Train loss 2.0059189948654175 | Test loss 2.0125832761764526\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 26%|██▌       | 26/100 [03:36<10:28,  8.49s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 25 | Train loss 2.0001256843948365 | Test loss 1.9907795387268066\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 27%|██▋       | 27/100 [03:45<10:22,  8.53s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 26 | Train loss 1.9879284728240967 | Test loss 1.9861130767822266\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 28%|██▊       | 28/100 [03:53<10:07,  8.44s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 27 | Train loss 1.9892514736175537 | Test loss 1.9901926273345947\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 29%|██▉       | 29/100 [04:01<09:51,  8.34s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 28 | Train loss 1.9764408477783204 | Test loss 1.9791935054779053\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 30%|███       | 30/100 [04:10<09:46,  8.38s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 29 | Train loss 1.9718123357772828 | Test loss 1.9787754484176636\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 31%|███       | 31/100 [04:18<09:35,  8.35s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 30 | Train loss 1.9683549154281617 | Test loss 1.9769414289474487\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 32%|███▏      | 32/100 [04:27<09:37,  8.50s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 31 | Train loss 1.9622158302307129 | Test loss 1.9744243564605712\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|███▎      | 33/100 [04:35<09:23,  8.42s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 32 | Train loss 1.954531245803833 | Test loss 1.9583241687774657\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 34%|███▍      | 34/100 [04:44<09:18,  8.47s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 33 | Train loss 1.9516938358306886 | Test loss 1.9661750730514527\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 35%|███▌      | 35/100 [04:52<09:06,  8.41s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 34 | Train loss 1.9429111333084106 | Test loss 1.9547528093338014\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 36%|███▌      | 36/100 [05:00<08:57,  8.39s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 35 | Train loss 1.936868459701538 | Test loss 1.9480136234283447\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 37%|███▋      | 37/100 [05:09<08:46,  8.36s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 36 | Train loss 1.938102710762024 | Test loss 1.9412438957214355\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 38%|███▊      | 38/100 [05:17<08:32,  8.27s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 37 | Train loss 1.925363808364868 | Test loss 1.9333798864364624\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 39%|███▉      | 39/100 [05:25<08:25,  8.29s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 38 | Train loss 1.9321440334701538 | Test loss 1.9427977809906005\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 40%|████      | 40/100 [05:33<08:11,  8.20s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 39 | Train loss 1.917102647972107 | Test loss 1.9338168577194215\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 41%|████      | 41/100 [05:41<08:03,  8.20s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 40 | Train loss 1.9102508689117432 | Test loss 1.9251356187820434\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 42%|████▏     | 42/100 [05:49<07:54,  8.18s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 41 | Train loss 1.9105314895629883 | Test loss 1.9212084093093873\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 43%|████▎     | 43/100 [05:58<07:50,  8.26s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 42 | Train loss 1.903428951072693 | Test loss 1.9258697372436524\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 44%|████▍     | 44/100 [06:06<07:47,  8.34s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 43 | Train loss 1.9021081894683838 | Test loss 1.9251466661453247\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 45%|████▌     | 45/100 [06:15<07:36,  8.31s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 44 | Train loss 1.8985106341171265 | Test loss 1.906894130706787\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 46%|████▌     | 46/100 [06:23<07:28,  8.30s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 45 | Train loss 1.8922450658798218 | Test loss 1.916402152633667\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 47%|████▋     | 47/100 [06:31<07:17,  8.26s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 46 | Train loss 1.8939871980667113 | Test loss 1.915008723449707\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 48%|████▊     | 48/100 [06:39<07:10,  8.27s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 47 | Train loss 1.8810727848434448 | Test loss 1.9002143690109252\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 49%|████▉     | 49/100 [06:48<07:01,  8.26s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 48 | Train loss 1.8759324806594848 | Test loss 1.9012603561401367\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|█████     | 50/100 [06:56<06:55,  8.31s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 49 | Train loss 1.8732922787857056 | Test loss 1.899664295578003\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 51%|█████     | 51/100 [07:05<06:52,  8.42s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 50 | Train loss 1.8714064330673217 | Test loss 1.8997163331985474\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 52%|█████▏    | 52/100 [07:13<06:42,  8.38s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 51 | Train loss 1.8728002242279054 | Test loss 1.9053330442428589\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 53%|█████▎    | 53/100 [07:21<06:32,  8.35s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 52 | Train loss 1.8684964336013794 | Test loss 1.892404904937744\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 54%|█████▍    | 54/100 [07:30<06:24,  8.35s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 53 | Train loss 1.8654935026550292 | Test loss 1.893747460746765\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 55%|█████▌    | 55/100 [07:38<06:15,  8.35s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 54 | Train loss 1.8592012964248656 | Test loss 1.8875270051956177\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 56%|█████▌    | 56/100 [07:46<06:07,  8.34s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 55 | Train loss 1.8532498527908325 | Test loss 1.8867394088745117\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 57%|█████▋    | 57/100 [07:54<05:55,  8.27s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 56 | Train loss 1.8472758079910279 | Test loss 1.8907104658126832\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 58%|█████▊    | 58/100 [08:03<05:45,  8.23s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 57 | Train loss 1.8457461935806274 | Test loss 1.8850287984848022\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 59%|█████▉    | 59/100 [08:11<05:37,  8.24s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 58 | Train loss 1.8447082510757447 | Test loss 1.8873567949295045\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 60%|██████    | 60/100 [08:19<05:28,  8.22s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 59 | Train loss 1.8413205624771118 | Test loss 1.8831854526519776\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 61%|██████    | 61/100 [08:27<05:19,  8.19s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 60 | Train loss 1.8339418282699584 | Test loss 1.886530140686035\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 62%|██████▏   | 62/100 [08:35<05:12,  8.23s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 61 | Train loss 1.8321611987686157 | Test loss 1.878689730644226\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 63%|██████▎   | 63/100 [08:44<05:05,  8.25s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 62 | Train loss 1.8345385149765014 | Test loss 1.8811444431304931\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 64%|██████▍   | 64/100 [08:52<04:59,  8.31s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 63 | Train loss 1.8246712198638917 | Test loss 1.8782608352661132\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 65%|██████▌   | 65/100 [09:01<04:52,  8.35s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 64 | Train loss 1.8312368952560425 | Test loss 1.877709781074524\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 66%|██████▌   | 66/100 [09:09<04:44,  8.37s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 65 | Train loss 1.8164022611618043 | Test loss 1.879829407119751\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|██████▋   | 67/100 [09:17<04:34,  8.33s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 66 | Train loss 1.817010679397583 | Test loss 1.8736696975708007\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 68%|██████▊   | 68/100 [09:26<04:28,  8.39s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 67 | Train loss 1.8146648346328735 | Test loss 1.8697213823318481\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 69%|██████▉   | 69/100 [09:34<04:19,  8.37s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 68 | Train loss 1.8114710320663452 | Test loss 1.8738105110168457\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 70%|███████   | 70/100 [09:42<04:11,  8.38s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 69 | Train loss 1.81198038356781 | Test loss 1.865073572921753\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 71%|███████   | 71/100 [09:51<04:02,  8.36s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 70 | Train loss 1.8071761646270752 | Test loss 1.8750470136642456\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 72%|███████▏  | 72/100 [09:59<03:54,  8.37s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 71 | Train loss 1.8021555598831176 | Test loss 1.8678240127563477\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 73%|███████▎  | 73/100 [10:07<03:44,  8.31s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 72 | Train loss 1.806926650238037 | Test loss 1.8728877086639404\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 74%|███████▍  | 74/100 [10:15<03:34,  8.26s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 73 | Train loss 1.7979348818969727 | Test loss 1.8719818490982056\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 75%|███████▌  | 75/100 [10:24<03:28,  8.33s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 74 | Train loss 1.7927236309432983 | Test loss 1.8631770498275757\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 76%|███████▌  | 76/100 [10:32<03:20,  8.37s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 75 | Train loss 1.7920584590911866 | Test loss 1.8685806547164916\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 77%|███████▋  | 77/100 [10:40<03:09,  8.25s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 76 | Train loss 1.7928072746658326 | Test loss 1.8644443758010865\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 78%|███████▊  | 78/100 [10:49<03:01,  8.25s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 77 | Train loss 1.788929377708435 | Test loss 1.857013551902771\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 79%|███████▉  | 79/100 [10:57<02:53,  8.26s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 78 | Train loss 1.7806573893737794 | Test loss 1.8541622436523437\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 80%|████████  | 80/100 [11:05<02:44,  8.23s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 79 | Train loss 1.7917965169525147 | Test loss 1.8544714990615845\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 81%|████████  | 81/100 [11:14<02:37,  8.32s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 80 | Train loss 1.7796577955245971 | Test loss 1.8559788200378418\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 82%|████████▏ | 82/100 [11:22<02:31,  8.44s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 81 | Train loss 1.7735248118972777 | Test loss 1.860383553123474\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 83%|████████▎ | 83/100 [11:30<02:21,  8.35s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 82 | Train loss 1.7758769264221193 | Test loss 1.853589101791382\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 84%|████████▍ | 84/100 [11:39<02:13,  8.32s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 83 | Train loss 1.7707223918914794 | Test loss 1.8598631496429443\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 85%|████████▌ | 85/100 [11:47<02:03,  8.22s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 84 | Train loss 1.766134020614624 | Test loss 1.8531146398544311\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 86%|████████▌ | 86/100 [11:55<01:55,  8.26s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 85 | Train loss 1.763005269432068 | Test loss 1.857615331840515\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 87%|████████▋ | 87/100 [12:04<01:48,  8.35s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 86 | Train loss 1.765016409072876 | Test loss 1.852997560119629\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 88%|████████▊ | 88/100 [12:12<01:39,  8.29s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 87 | Train loss 1.7616707609176636 | Test loss 1.8523096965789796\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 89%|████████▉ | 89/100 [12:20<01:30,  8.25s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 88 | Train loss 1.7631068569564818 | Test loss 1.8545490509033202\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 90%|█████████ | 90/100 [12:28<01:22,  8.30s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 89 | Train loss 1.7572699490356445 | Test loss 1.8534082683563233\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 91%|█████████ | 91/100 [12:36<01:14,  8.22s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 90 | Train loss 1.7691221405410766 | Test loss 1.851688709449768\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 92%|█████████▏| 92/100 [12:45<01:05,  8.19s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 91 | Train loss 1.7575333493041991 | Test loss 1.8522770378112794\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 93%|█████████▎| 93/100 [12:53<00:57,  8.24s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 92 | Train loss 1.755941989402771 | Test loss 1.8586383785247802\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 94%|█████████▍| 94/100 [13:01<00:49,  8.17s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 93 | Train loss 1.756261735305786 | Test loss 1.8594229356765748\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 95%|█████████▌| 95/100 [13:09<00:40,  8.16s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 94 | Train loss 1.7516488500976561 | Test loss 1.8534573366165161\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 96%|█████████▌| 96/100 [13:17<00:32,  8.17s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 95 | Train loss 1.7490053300857544 | Test loss 1.8469494594573974\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 97%|█████████▋| 97/100 [13:26<00:24,  8.25s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 96 | Train loss 1.7365227814865112 | Test loss 1.8486788982391358\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 98%|█████████▊| 98/100 [13:34<00:16,  8.25s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 97 | Train loss 1.7356075606155394 | Test loss 1.8470183088302612\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 99%|█████████▉| 99/100 [13:42<00:08,  8.19s/it]\u001B[A\u001B[A"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 98 | Train loss 1.73528732837677 | Test loss 1.8580757360458373\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 100/100 [13:50<00:00,  8.31s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch 99 | Train loss 1.7431965669631957 | Test loss 1.850503893661499\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJNAuHjNQuvn"
   },
   "source": [
    "Построим график функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F6OEGqriQuvo",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1610651350382,
     "user_tz": -180,
     "elapsed": 1135,
     "user": {
      "displayName": "Iaroslav Sokolov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHt_w9-U1o8Sy_kutVo5pzmBrbbw0dWtxW9hnv=s64",
      "userId": "04885912710307977692"
     }
    },
    "outputId": "2c24f88b-d180-4aec-fc10-6d6d0e9b2178"
   },
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(train_l)), train_l, label=\"train\")\n",
    "plt.plot(range(len(test_l)), test_l, label=\"test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": [],
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miUxg0bDQuvs"
   },
   "source": [
    "И, наконец, посчитаем метрики"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UXSOJFI8Quvt",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1610651359918,
     "user_tz": -180,
     "elapsed": 10668,
     "user": {
      "displayName": "Iaroslav Sokolov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHt_w9-U1o8Sy_kutVo5pzmBrbbw0dWtxW9hnv=s64",
      "userId": "04885912710307977692"
     }
    },
    "outputId": "c4dc8f51-54e3-4c45-9ed4-36fe172c6cf4"
   },
   "source": [
    "true_positive = np.zeros(10)\n",
    "true_negative = np.zeros(10)\n",
    "false_positive = np.zeros(10)\n",
    "false_negative = np.zeros(10)\n",
    "accuracy = 0\n",
    "ctn = 0\n",
    "for X, y in iter(test_loader):\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X).max(dim=1)[1]\n",
    "    for i in range(10):\n",
    "        for pred, real in zip(y_pred, y):\n",
    "            if real == i:\n",
    "                if pred == real:\n",
    "                    true_positive[i] += 1\n",
    "                else:\n",
    "                    false_negative[i] += 1\n",
    "            else:\n",
    "                if pred == i:\n",
    "                    false_positive[i] += 1\n",
    "                else:\n",
    "                    true_negative[i] += 1\n",
    "            \n",
    "    accuracy += torch.sum(y_pred == y).item()\n",
    "    ctn += len(y)\n",
    "print(\"Overall accuracy\", accuracy / ctn)\n",
    "print(\"Precision\", true_positive / (true_positive + false_positive))\n",
    "print(\"Recall\", true_positive / (true_positive + false_negative))\n",
    "print(\"Mean Precision\", np.mean(true_positive / (true_positive + false_positive)))\n",
    "print(\"Mean Recall\", np.mean(true_positive / (true_positive + false_negative)))"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Overall accuracy 0.6082\n",
      "Precision [0.66225839 0.74317492 0.471843   0.47052023 0.49956934 0.49369369\n",
      " 0.73556582 0.62068966 0.8015873  0.68562565]\n",
      "Recall [0.651 0.735 0.553 0.407 0.58  0.548 0.637 0.702 0.606 0.663]\n",
      "Mean Precision 0.6184528004890482\n",
      "Mean Recall 0.6082000000000001\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EKA-j4rIQuvv"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}